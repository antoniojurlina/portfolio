{
  
    
        "post0": {
            "title": "How does traffic congestion impact life satisfaction?",
            "content": ". Summary . In an attempt to venture solo, while an undergraduate student in economics, I worked on a probit model designed to study the effect traffic congestion has on satisfaction with work and place of residence. The question seemed straightforward, and the dependent variables were already either presented as binary or Likert scales that could be easily turned into one. Data came from the city of Cardiff, UK, in the form of an online questionnaire. I find that an increase in the perceived level of congestion makes individuals less happy with working and living in Cardiff. After adjusting for all the potential issues (like hetersokedasticity) this data might have, the results seem quite robust across two probit models. Included, also, is a link to the GitHub repository containing the data I worked with and the Stata do-file. . Part I . 1. Data . Council authority of Cardiff has, in cooperation with Cardiff Business Partnership, conducted an online survey, yielding 2,094 responses. After data clean-up process, 2,045 observations remained. While most variables were dropped, 17 were deemed significant enough to include (see Table 1). Missing values were filled with unconditional means, except in the case of the variables describing the primary method of transportation and proximity of a train station, where 19 and 32 observations with missing values were dropped, respectively. Variables regarding age, income and employment duration were acquired via drop-down answer menus on the survey, with each answer constituting a specific bracket. For the sake of continuity and interpretability, these answers were encoded as the midpoint value of each bracket. Since each set of brackets concluded with an open ended one (e.g. more than 10, 65+, etc.), and no middle point could be determined, each of those categories was encoded as the sum of the size of the largest bracket (for the relevant variable) and the bracket’s lower bound. . . Table 1 - Data Summary . Average Std. Dev. Min Max Explanation . Quality of public transportation | 3.26 | 1.09 | 1 | 5 | 1 is very bad and 5 is very good | . Being able to get from place to place with little traffic (i.e. congestion) | 2.72 | 1.04 | 1 | 5 | 1 is very bad and 5 is very good | . Work proximity | 2.98 | 1.28 | 1 | 5 | 1 you like the most and 5 you dislike | . Travel within the city | 3.86 | 0.96 | 1 | 5 | 1 is not important and 5 is very important | . Ease of getting to work | 3.65 | 1.02 | 1 | 5 | 1 is not important and 5 is very important | . Overall satisfaction with life | 2.95 | 0.98 | 1 | 5 | 1 being highly satisfied and 5 being unsatisfied | . Number of years employed | 9.02 | 5.46 | 0.5 | 15 | How long have you worked in your present employment | . Income | 25,748.18 | 10,318.80 | 5,720 | 55,241 | How much do you get paid, before taxes | . Age | 41.50 | 11.05 | 21 | 74 | How old are you | . Sex | 0.42 | 0.49 | 0 | 1 | 1 for male, 0 for female | . Relationship | 0.69 | 0.46 | 0 | 1 | 1 for in a relationship, 0 for single | . Children | 0.56 | 0.49 | 0 | 1 | 1 for having any number of children, 0 for none | . Overall satisfaction with place of residence | 0.66 | 0.47 | 0 | 1 | 1 for satisfied, 0 for unsatisfied | . Train station proximity | 2.72 | 0.50 | 1 | 3 | Do you have a train station within 2 miles of your residence? 1 - Don’t Know, 2 - No, 3 - Yes | . Education | 2.53 | 1.10 | 1 | 6 | 1 - High School, 2 - Associates degree, 3 – Bachelor’s Degree BA, BSc, 4 – Master’s Degree, 5 - Professional degree, 6 - PhD | . Primary mode of transportation | 1.36 | 0.83 | 0 | 2 | 0 - Hippie, 1 - Public, 2 - Drive | . Job satisfaction | 0.52 | 0.50 | 0 | 1 | 1 for satisfied, 0 for unsatisfied | . . 2. Methodology . Literature review, focusing on measuring different sorts of satisfaction1 2 3, exemplifies various probit models and their appropriate usage. Binary nature of the dependent variables, together with the literature, was the main reason for choosing a probit-model approach. Given that the report attempts to clarify the effect congestion might have on satisfaction with place of residence (R) and working (J) in Cardiff, following two models represent its cornerstone: . J=β0+β1congestion+xδ+ϵ1J = beta_0 + beta_1 congestion + x delta + epsilon_1J=β0​+β1​congestion+xδ+ϵ1​ . R=β0+β1congestion+xδ+ϵ2R = beta_0 + beta_1 congestion + x delta + epsilon_2R=β0​+β1​congestion+xδ+ϵ2​ . Model 2 dependent variable was binary in the form the data was presented. However, Model 1 dependent variable had to be created from a set of Likert scale variables focusing on job satisfaction. Mean value for the set was created across individuals and compared to “3”, which is the neutral option on the answer sheet. Individuals above “3” were categorized as satisfied and the rest were categorized as unsatisfied. This way, a binary variable was generated, appropriate for use in a probit model. Cronbach’s alpha was used to determine the scale reliability coefficient (0.8471), indicating that the set of Likert-scale variables used are closely related as a group and representative of the shared concept. . Furthermore, for both Models 1 and 2, β betaβ and δ deltaδ represent coefficients to be estimated (see Table 1). Control variables were chosen based on two criteria: 1) those causally linked with the dependent variable through congestion and 2) those with a direct, causal link to satisfaction that were deemed as likely sources of severe endogeneity. Explanatory variables of interest consist of demographic characteristics4 5, congestion6 7 8 and those capturing satisfaction spillover9 10. Stochastic terms are noted as ϵ1 epsilon_1ϵ1​ and ϵ2 epsilon_2ϵ2​. Also, since the probit models are being used, robust standard errors are reported and corrected for underlying (inherent) heteroskedasticity. . 3. Hypotheses . . H01H_{01}H01​: Congestion has no significant impact on job satisfaction for residents of Cardiff. . HA1H_{A1}HA1​: Congestion has a negative impact on job satisfaction for residents of Cardiff. . . H02H_{02}H02​: Congestion has no significant impact on satisfaction with place of residence. . HA2H_{A2}HA2​: Congestion has a negative impact on satisfaction with place of residence. . . Models are demonstrating the conditional probability of a specific outcome occurring (Yi=1Y_i = 1Yi​=1 meaning satisfied with job for Model 1 and satisfied with place of residence for Model 2), for 3 which the marginal effects show how a unit change for congestion increases (or decreases) the probability of the given outcome occurring (et ceteris paribus). . . Part II . 1. Satisfaction with working and living in Cardiff . . I am quite confident that an increase in perceived level of congestion makes individuals 4% less likely to be satisfied with working in Cardiff. . . I am quite confident that an increase in perceived level of congestion makes individuals 5% less likely to be satisfied with living in Cardiff. . . Tables 2 and 3 present more detailed results for these claims and state the possibility of a mistake being made. Numbers showing the impact of congestion on satisfaction with working and living in Cardiff are positive. This is a consequence of the way the survey was designed (1 is very bad and 5 is very good) and should be interpreted with an opposite sign. Literature review supports this finding. High levels of traffic congestion are associated with mental and physical stress11 12, which are further aggravated with the inability to complete daily routines 13. Furthermore, long work commutes cause residual stress in the workplace 7 14 15. Kahneman et al.8 found that work commutes were most frequently associated with negative feelings, out of all daily habits. . . Table 2 - Model 1 marginal effects . Marginal effect Standard error . Quality of public transportation | | 0.049*** | 0.011 | . Being able to get from place to place with little traffic (i.e. congestion) | | 0.041*** | 0.011 | . Work proximity | | 0.023*** | 0.008 | . Travel within the city | | 0.007 | 0.014 | . Ease of getting to work | | 0.015 | 0.013 | . Overall satisfaction with life | | -0.027** | 0.011 | . Number of years employed | | -0.001 | 0.002 | . Income | | 0.000*** | 0.000 | . Age | | -0.008 | 0.007 | . Sex | | -0.072*** | 0.022 | . Relationship | | -0.015 | 0.025 | . Children | | 0.039 | 0.026 | . Overall satisfaction with place of residence | | 0.094*** | 0.023 | . Train station proximity | Yes No | -0.204*** -0.219*** | 0.058 0.027 | . Education | | 0.024** | 0.010 | . Primary mode of transportation | Public Drive | 0.010 0.076*** | 0.033 0.027 | . * - 90 % significance / ** - 95 % significance / *** - 99 % significance . . Public transportation plays a major role in mitigating the effects of congestion on satisfaction16. However, while it is the most accepted solution to congestion17, it only works if it is appropriately scaled with the needs of the public18. In Cardiff, an increase in perceived quality of public transportation makes individuals 4% (on average) more likely to feel satisfied with their work and 6% (on average) more likely to feel satisfied with where they live, holding everything else the same. Moreover, utilizing public transportation systems decreases the likelihood of being satisfied with place of residence by 6% (on average), holding everything else the same. Those most likely to utilize public transportation live inside the urban area while those living in the suburbs are more likely to drive to work. Therefore, this decrease in likelihood of being satisfied with the place of residence, based on public transport utilization, indicates a preference for living outside of the city. Furthermore, driving decreases the likelihood of being satisfied with working in Cardiff by 8% (on average), holding everything else the same (see Tables 2 and 3 for more detailed results and the likelihood of mistakes being reported). Finally, living close to work increases the likelihood of being satisfied with working in Cardiff by 2% (on average), holding everything else the same. . . Table 3 - Model 2 marginal effects . Marginal effect Standard error . Quality of public transportation | | 0.062*** | 0.010 | . Being able to get from place to place with little traffic (i.e. congestion) | | 0.052*** | 0.010 | . Work proximity | | 0.003 | 0.008 | . Travel within the city | | 0.016 | 0.013 | . Ease of getting to work | | -0.021* | 0.012 | . Overall satisfaction with workplace | | 0.086*** | 0.020 | . Number of years employed | | 0.002 | 0.002 | . Income | | 0.000*** | 0.000 | . Age | | -0.006 | 0.006 | . Sex | | -0.035 | 0.020 | . Relationship | | 0.106*** | 0.023 | . Children | | 0.034 | 0.024 | . Overall satisfaction with life | | 0.002 | 0.010 | . Train station proximity | Yes No | 0.212*** 0.121 | 0.075 0.077 | . Education | | 0.019*** | 0.009 | . Primary mode of transportation | Public Drive | -0.055* -0.030 | 0.032 0.026 | . * - 90 % significance / ** - 95 % significance / *** - 99 % significance . . While there are negative effects of congestion on satisfaction with working and living in Cardiff, those effects are smaller than anticipated. This could be because working during a period of recession or post-recession19 recovery causes enough satisfaction that lessens the burdens of daily commute13. Additionally, as noted by Stokols et al. 11, congestion is only relevant when it significantly differs from the expected traffic levels. If people are exposed to similar congestion levels every day, negative effects are diminished. . 2. Policy relevance . Those that live within 2 miles of a train station and those that live further away, are about 20% (on average) less likely to feel satisfied with working in Cardiff, holding everything else the same. Moreover, those that live within 2 miles of a train station are about 20% (on average) more likely to be satisfied with their place of residence, holding everything else the same (see Tables 2 and 3). Since most of survey respondents (74%) live within 2 miles of a train station and more than half (59%) drive to work (not necessarily overlapping groups), it could be assumed that there is a negative perception of public transportation and a strong tendency to avoid it. This, in addition to reported congestion and public transportation effects, indicates that policy under consideration by the council authority of Cardiff needs to address both public transportation and congestion. Building a new metro system as well as introducing a congestion charge for those driving into the center of Cardiff should be packaged together. Literature review supports this claim, as well20. . 3. Limitations . Variability of answers provided was reduced when most missing values were filled with average values for the category. Further adding to this issue was the design of the survey itself. Answers that were selected from a drop-down menu were presented as brackets, chosen by the surveyor. This eliminated most of the effect that individuals with answers far away from the average would have. Moreover, for privacy reasons, answers stating the specific place of residence were excluded, introducing additional lack of clarity. . Due to model choice, differences between observed and predicted and expected and predicted values for variables of interest were not uniform across the data. This means that I was more likely to falsely perceive an observation as insignificant and reject its validity and explanatory power. Finally, I wish to point out the possibility that any variable not considered stands a chance of being correlated with variables I did consider (e.g. place of residence might be linked with public transport utilization, commute satisfaction and satisfaction with living and working in Cardiff). By omitting any such variable, I introduced the likelihood of overstating the effects considered variables have on overall satisfaction. . 4. GitHub repository . For data, code, and similar projects, visit https://github.com/antoniojurlina/econometrics. . . Stata code . cls clear //clear previous data use CBP_survey.dta //choose data // following commands represent my data clean-up process //////////////////////////////////////////////////////////////////// encode Q3, generate(yrs_employed) replace yrs_employed = 0.5 if Q3 == &quot;Less than 1 year&quot; replace yrs_employed = 2 if Q3 == &quot;1 to 3 years&quot; replace yrs_employed = 4 if Q3 == &quot;3 to 5 years&quot; replace yrs_employed = 7.5 if Q3 == &quot;5 to 10 years&quot; replace yrs_employed = 15 if Q3 == &quot;More than 10 years&quot; encode Q4, generate(income) replace income = 5720 if Q4 == &quot;£0 - £11,440 per year&quot; replace income = 12480.5 if Q4 == &quot;£11,441 - £13,520 per year&quot; replace income = 14820.5 if Q4 == &quot;£13,521 - £16,120 per year&quot; replace income = 17420.5 if Q4 == &quot;£16,121 - £18,720 per year&quot; replace income = 20540.5 if Q4 == &quot;£18,721 - £22,360 per year&quot; replace income = 25220.5 if Q4 == &quot;£22,361 - £28,080 per year&quot; replace income = 31720.5 if Q4 == &quot;£28,081 - £35,360 per year&quot; replace income = 40300.5 if Q4 == &quot;£35,361 - £45,240 per year&quot; replace income = 55241 if Q4 == &quot;£45,241 or more per year&quot; drop if Q5 == &quot;25- 40&quot; | Q5 == &quot;25- 41&quot; | Q5 == &quot;25- 42&quot; | Q5 == &quot;25- 43&quot; drop if Q5 == &quot;25- 44&quot; | Q5 == &quot;25- 45&quot; | Q5 == &quot;25- 46&quot; | Q5 == &quot;25- 47&quot; | Q5 == &quot;25- 48&quot; | Q5 == &quot;25- 49&quot; encode Q5, generate(age) replace age = 21 if Q5 == &quot;18- 24&quot; replace age = 32 if Q5 == &quot;25- 39&quot; replace age = 47 if Q5 == &quot;40- 54&quot; replace age = 59.5 if Q5 == &quot;55- 64&quot; replace age = 74 if Q5 == &quot;65&quot; generate age_sq = age * age label define sex 1 &quot;Male&quot; 0 &quot;Female&quot; encode Q6, generate(sex) generate relationshipy = Q7 replace relationshipy = &quot;Yes&quot; if Q7 == &quot;Co-habiting&quot; replace relationshipy = &quot;Yes&quot; if Q7 == &quot;Married&quot; replace relationshipy = &quot;No&quot; if Q7 == &quot;Single&quot; label define relationship 1 &quot;Yes&quot; 0 &quot;No&quot; encode relationshipy, generate(relationship) drop relationshipy Q7 label define children 1 &quot;Yes&quot; 0 &quot;No&quot; encode Q8, generate(children) rename Q14h pub_trans_quality rename Q14i congestion generate satisfactiony = Q15 label define satisfaction 1 &quot;Satisfied&quot; 0 &quot;Unsatisfied&quot; encode satisfactiony, generate(satisfaction) drop satisfactiony Q15 encode Q19, generate(train) label define education 1 &quot;High School&quot; 2 &quot;Associates degree&quot; 3 &quot;Bachelors Degree BA, BSc&quot; 4 &quot;Masters Degree&quot; 5 &quot;Professional degree&quot; 6 &quot;PhD&quot; encode Q20, generate(education) rename Q30c travel_importance rename Q30f work_travel_ease rename Q31 life_satisfaction summarize pub_trans_quality replace pub_trans_quality = r(mean) if pub_trans_quality == . summarize congestion replace congestion = r(mean) if congestion == . summarize yrs_employed replace yrs_employed = r(mean) if yrs_employed == . summarize income replace income = r(mean) if income == . summarize age replace age = r(mean) if age == . summarize sex replace sex = r(mean) if sex == . summarize relationship replace relationship = r(mean) if relationship == . summarize satisfaction replace satisfaction = r(mean) if satisfaction == . summarize train drop if train == . summarize education replace education = r(mean) if education == . summarize life_satisfaction replace life_satisfaction = r(mean) if life_satisfaction == . summarize children replace children = r(mean) if children == . summarize work_travel_ease replace work_travel_ease = r(mean) if work_travel_ease == . summarize travel_importance replace travel_importance = r(mean) if travel_importance == . generate transport1 = Q16 replace transport1 = &quot;Hippie&quot; if Q16 == &quot;Walk&quot; | Q16 == &quot;Cycle&quot; replace transport1 = &quot;Public&quot; if Q16 == &quot;Train&quot; | Q16 == &quot;Bus&quot; replace transport1 = &quot;Drive&quot; if Q16 == &quot;Car /Motorcycle&quot; label define trans1 0 &quot;Hippie&quot; 1 &quot;Public&quot; 2 &quot;Drive&quot; encode transport1, generate(trans1) drop if trans1 == . summarize Q28a replace Q28a = r(mean) if Q28a == . summarize Q28b replace Q28b = r(mean) if Q28b == . summarize Q28c replace Q28c = r(mean) if Q28c == . summarize Q28d replace Q28d = r(mean) if Q28d == . summarize Q28e replace Q28e = r(mean) if Q28e == . summarize Q28f replace Q28f = r(mean) if Q28f == . summarize Q28g replace Q28g = r(mean) if Q28g == . summarize Q29a replace Q29a = r(mean) if Q29a == . summarize Q29b replace Q29b = r(mean) if Q29b == . summarize Q29c replace Q29c = r(mean) if Q29c == . summarize Q29d replace Q29d = r(mean) if Q29d == . summarize Q29e replace Q29e = r(mean) if Q29e == . summarize Q29f replace Q29f = r(mean) if Q29f == . summarize Q29g replace Q29g = r(mean) if Q29g == . summarize Q29h replace Q29h = r(mean) if Q29h == . rename Q29c work_proximity generate job = (Q28a + Q28b + Q28c + Q28d + Q28e + Q28f + Q28g)/7 generate job_satisfaction = job summarize job replace job_satisfaction = 1 if job &gt; 3 replace job_satisfaction = 0 if job &lt;= 3 alpha Q28a-Q28g drop Q3 Q4 Q5 Q6 Q8 Q19 Q20 drop Q1 Q2 Q11 Q13 Q12 Q14a Q14b Q14c Q14d Q14e Q14f Q14g Q14j Q14k Q14l Q14m Q14n Q14o drop Q17 Q21 Q22 Q23a Q23b Q24 Q25 Q26 Q27 Q28a Q28b Q28c Q28d Q28e Q28f Q28g drop Q29a Q29b Q29d Q29e Q29f Q29g Q29h Q30a Q30b Q30d Q30e Q30g Q30h Q30i Q30j Q30k drop Q32 Q33 drop job Q16 Q18 transport1 //////////////////////////////////////////////////////////////////// // end of data clean-up process summarize probit job_satisfaction satisfaction pub_trans_quality children congestion life_satisfaction yrs_employed travel_importance work_travel_ease work_proximity income age age_sq sex relationship i.train education i.trans1,robust margins, dydx(*) probit satisfaction job_satisfaction pub_trans_quality children congestion life_satisfaction yrs_employed travel_importance work_travel_ease work_proximity income age age_sq sex relationship i.train education i.trans1,robust margins, dydx(*) save CBP_survey_clean.dta, replace . . Works Cited . Blanchflower, D. G., &amp; Oswald, A. J. (2004). Well-being over time in Britain and the USA. Journal of Public Economics, 88, 1359-1386. doi:10.1016/S0047-2727(02)00168-8 &#8617; . | Fetai, B., Abduli, S., &amp; Qirici, S. (2015). An Ordered Probit Model of Job Satisfaction in the Former Yugoslav Republic of Macedonia. Procedia Economics and Finance, 33, 350- 357. doi:10.1016/S2212-5671(15)01719-0 &#8617; . | Rayton, B. A. (2006). Examining the interconnection of job satisfaction and organizational commitment: An application of the bivariate probit model. Int. J. of Human Resource Management, 17(1), 139-154. doi:10.1080/09585190500366649 &#8617; . | Richey, S. (2012). Determinants of Community Satisfaction and its Relative Importance for Life Satisfaction. &#8617; . | Auh, S., &amp; Cook, C. (2009). Quality of Community Life Among Rural Residents: An Integrated Model. Social Indicators Research, 94(3), 377-389. &#8617; . | Lipsetz, D. A. (2000). Residential Satisfaction: Identifying the differences between suburban-ites and urbanites. Columbus: Ohio State University. &#8617; . | Novaco, R. W., Stokols, D., &amp; Milanesi, L. (1990). Objective and subjective dimensions of travel impedance as determinants of commuting stress. American Journal of Community Psychology, 18, 231–257. &#8617; &#8617;2 . | Kahneman, D., Krueger, A. B., Schkade, D., Schwarz, N., &amp; Stone, A. (2004). A survey method for characterizing daily life experience: The day reconstruction method (DRM). Science, 306, 1776–1780. &#8617; &#8617;2 . | Shimon L. Dolan &amp; Eric Gosselin, 2000. “Job satisfaction and life satisfaction: Analysis of a reciprocal model with social demographic moderators,” Economics Working Papers 484, Department of Economics and Business, Universitat Pompeu Fabra. &#8617; . | Ilies, R., Wilson, K. S., &amp; Wagner, D. T. (2009). The Spillover of Daily Job Satisfaction Onto Employees’ Family Lives: The Facilitating Role of Work-Family Integration. Academy of Management Journal, 52(1), 87-102. doi:10.5465/AMJ.2009.36461938 &#8617; . | Stokols, D., Novaco, R. W., Stokols, J., &amp; Campbell, J. (1978). Traffic Congestion, Type A Behavior, and Stress. Journal of Applied Psychology, 63(4), 467-480. doi:10.1037/0021- 9010.63.4.467 &#8617; &#8617;2 . | Novaco, R. W., &amp; Gonzales, O. I. (2009). Commuting and well-being. In Y. Amichai- Hamburger (Ed.), Technology and well-being (pp. 174–205). New York: Cambridge University Press. &#8617; . | Olsson, L. E., Garling, T., Ettema, D., Friman, M., &amp; Fujii, S. (2013). Happiness and Satisfaction with Work Commute. Soc Indic Res, 111, 255-263. doi:10.1007/s11205-012- 0003-2 &#8617; &#8617;2 . | Glass, D. C., Singer, J., &amp; Pennebaker, J. Behavioral and physiological effects of uncontrollable environmental events. In D. Stokols (Ed.), Perspectives on environment and behavior. New York: Plenum, 1977. &#8617; . | Sherrod, D. R. Crowding, perceived control, and behavioral aftereffects. Journal of Applied Social Psychology, 1974, 4, 171-186. &#8617; . | Kottenhoff, K., &amp; Freij, K. B. (2009). The role of public transport for feasibility and acceptability of congestion charging – The case of Stockholm. Transportation Research Part A, 43, 297-305. doi:10.1016/j.tra.2008.09.004 &#8617; . | Schlag, B., Teubel, U., 1997. Public acceptability of transport pricing. IATSS Research, 21(2). &#8617; . | Transport for London (TfL), 2005. Central London Congestion Charging. Impacts monitoring, Third Annual Report, April 2005 &#8617; . | Gross Domestic Product Preliminary Estimate: Q4 2014 (pp. 1-20, Rep.). (2015). Office for National Statistics. &#8617; . | Jaensirisak, S., Wardman, M., May, A.D., 2005. Explaining variations in public acceptability of road pricing schemes. Journal of Transport Economics and Policy 39, 127–154. &#8617; . |",
            "url": "https://antoniojurlina.github.io/portfolio/2021/06/26/congestion-and-satisfaction.html",
            "relUrl": "/2021/06/26/congestion-and-satisfaction.html",
            "date": " • Jun 26, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Title",
            "content": "Agora - A Dark Web Marketplace . Data analysis and visualization . Antonio Jurlina . import pandas as pd import numpy as np import matplotlib.pyplot as plt import os import warnings warnings.filterwarnings(&quot;ignore&quot;) os.chdir(&#39;/Users/antoniojurlina/Projects/learning_python/data/&#39;) . A few years ago, a reddit user called usheep scraped a DarkWeb marketplace called the Agora for a list of vendors, their identities and products, while threatening to expose them all as blackmail. Eventually, this data set made it to the general public, and while usheep disappeared, we have thousands of rows of vendors, items, categories, descriptions and prices in an organized data set that contains drugs, weapons, books, services, and more. There are over 100,000 unique observations spanning year 2014. My goal is to analyze this data set and explore some common items being sold, as well as the usual prices across categories. . Although there is over 1.6TB of uncompressed, scraped data of dark web sales available out there, the goal of this project will be to look at the Kaggle published subset which is around 30MB and contains ~100,000 observations. The data set is available as a direct .csv download with a Kaggle account. Additonally, since all the prices listed in this data set are in Bitcoin, I went to Coindesk and downloaded the daily closing price of Bitcoin in US Dollar amounts, for that whole period. This data set will be used to make the costs more comprehensible by converting them to current USD prices. . dw_data = pd.read_csv(&quot;Agora.csv&quot;, usecols=[&#39;Vendor&#39;, &#39; Category&#39;, &#39; Item&#39;, &#39; Item Description&#39;, &#39; Price&#39;, &#39; Rating&#39;]) .rename(columns={&#39;Vendor&#39;:&#39;vendor&#39;, &#39; Category&#39;:&#39;category&#39;, &#39; Item&#39;:&#39;item&#39;, &#39; Item Description&#39;:&#39;description&#39;, &#39; Price&#39;:&#39;price&#39;, &#39; Rating&#39;:&#39;rating&#39;}) # Importing Bitcoin data and selecting out the relevant column bitcoin = pd.read_csv(&quot;BTC_USD_2014-01-02_2015-01-01-CoinDesk.csv&quot;) inflation = 0.1186 # the inflation rate between 2014 and 2021 bitcoin = bitcoin.iloc[:,2]*(1+inflation) . The Agora data set sample, as it is currently in the memory, is shown below. The variables that need to be cleaned up further are category, price, and rating. . Category: There are categories and subcategories within this column that need to be split into separate columns. | Price: This a string because the word BTC is present in each and should be converted to numeric for further analysis. | Rating: This shows the rating and the possible highest score, which should also be separated out. | . dw_data.head() . vendor category item description price rating . 0 CheapPayTV | Services/Hacking | 12 Month HuluPlus gift Code | 12-Month HuluPlus Codes for $25. They are wort... | 0.05027025666666667 BTC | 4.96/5 | . 1 CheapPayTV | Services/Hacking | Pay TV Sky UK Sky Germany HD TV and much mor... | Hi we offer a World Wide CCcam Service for En... | 0.152419585 BTC | 4.96/5 | . 2 KryptykOG | Services/Hacking | OFFICIAL Account Creator Extreme 4.2 | Tagged Submission Fix Bebo Submission Fix Adju... | 0.007000000000000005 BTC | 4.93/5 | . 3 cyberzen | Services/Hacking | VPN &gt; TOR &gt; SOCK TUTORIAL | How to setup a VPN &gt; TOR &gt; SOCK super safe enc... | 0.019016783532494728 BTC | 4.89/5 | . 4 businessdude | Services/Hacking | Facebook hacking guide | . This guide will teach you how to hack Faceb... | 0.062018073963963936 BTC | 4.88/5 | . From the bitcoin data set, the only relevant variable is column 2, which has been adjusted from 2014 to 2021 USD values and extracted as a single Series. From here, I can obtain the average bitcoin value for 2014 and use it to convert all the listed prices in the Agora data set. This is not a flawless approach as the sale listings do not have specific dates associated with them and Bitcoin varies daily. Therefore, I will also use the lowest and highest Bitcoin values in USD across 2014 to show the possible price ranges. . bitcoin.head() . 0 860.313571 1 899.384815 2 909.776933 3 974.430167 4 1085.770018 Name: Closing Price (USD), dtype: float64 . Cleaning Data . Below is the code that performs the data cleaning discussed above. In addition to all the steps previously outlined, I find price outliers within each category and create a new Boolean column which indicates whether each value is an outlier within its respective category. . In this case, outliers are defined as prices that are above the threshold: . $$price &gt; 1.5 * IQR + 3^{rd} Quartile$$ . where IQR stands for interquartile range. . dw_data = dw_data[dw_data[&#39;price&#39;].str.contains(&#39;BTC&#39;, na=False)] dw_data[&#39;price&#39;] = dw_data[&#39;price&#39;].str.replace(&quot; BTC&quot;, &quot;&quot;) dw_data[&#39;price&#39;] = pd.to_numeric(dw_data[&#39;price&#39;]) # Separating the rating column into rating and total possible score columns dw_data = dw_data[~dw_data[&#39;rating&#39;].str.contains(&#39;[0 deals]&#39;, na=False)] dw_data[&#39;rating&#39;] = dw_data[&#39;rating&#39;].str.replace(&quot;~&quot;, &quot;&quot;) ratings = dw_data[&#39;rating&#39;].str.split(&quot;/&quot;, n=3,expand=True) dw_data[&#39;rating&#39;] = pd.to_numeric(ratings[0]) dw_data[&#39;rating_total&#39;] = pd.to_numeric(ratings[1]) # separating out the categories hierarchy categories = dw_data[&#39;category&#39;].str.split(&quot;/&quot;, n=3,expand=True) categories[0] = categories[0].str.replace(&quot;^Info$&quot;, &quot;Information&quot;) categories_to_use = [&#39;Services&#39;, &#39;Drugs&#39;, &#39;Forgeries&#39;, &#39;Tobacco&#39;, &#39;Counterfeits&#39;, &#39;Data&#39;, &#39;Information&#39;, &#39;Electronics&#39;, &#39;Drug paraphernalia&#39;, &#39;Other&#39;, &#39;Jewelry&#39;, &#39;Weapons&#39;, &#39;Chemicals&#39;] dw_data[&#39;category&#39;] = categories[0] dw_data[&#39;category1&#39;] = categories[1] dw_data[&#39;category2&#39;] = categories[2] dw_data[&#39;category3&#39;] = categories[3] dw_data = dw_data[dw_data[&#39;category&#39;].isin(categories_to_use)].reset_index(drop=True) # converting BTC to 2021 $USD values dw_data[&#39;usd_low&#39;] = np.round(np.min(bitcoin) * dw_data[&#39;price&#39;], 2) dw_data[&#39;usd&#39;] = np.round(np.mean(bitcoin) * dw_data[&#39;price&#39;], 2) dw_data[&#39;usd_high&#39;] = np.round(np.max(bitcoin) * dw_data[&#39;price&#39;], 2) # adding a column that signifies which value is an outlier within top level category def limit(column): iqr = column.quantile(0.75) - column.quantile(0.25) top = column.quantile(0.75) + 1.5*iqr return top limits = dw_data.groupby(&#39;category&#39;)[&#39;usd&#39;].agg(limit) .reset_index().rename(columns={&#39;usd&#39;:&#39;limit&#39;}) dw_data = dw_data.merge(limits, on=&#39;category&#39;, how=&#39;left&#39;) dw_data[&#39;outlier&#39;] = dw_data[&#39;usd&#39;] &gt; dw_data[&#39;limit&#39;] dw_data = dw_data.drop(&#39;limit&#39;, axis=1) # deleting all intermediate data frames del [categories, categories_to_use, ratings, limits] . The resulting data frame is presented below. . print(dw_data.shape) dw_data.head() . (73314, 14) . vendor category item description price rating rating_total category1 category2 category3 usd_low usd usd_high outlier . 0 CheapPayTV | Services | 12 Month HuluPlus gift Code | 12-Month HuluPlus Codes for $25. They are wort... | 0.050270 | 4.96 | 5.0 | Hacking | None | None | 17.23 | 29.60 | 54.58 | False | . 1 CheapPayTV | Services | Pay TV Sky UK Sky Germany HD TV and much mor... | Hi we offer a World Wide CCcam Service for En... | 0.152420 | 4.96 | 5.0 | Hacking | None | None | 52.25 | 89.74 | 165.49 | False | . 2 KryptykOG | Services | OFFICIAL Account Creator Extreme 4.2 | Tagged Submission Fix Bebo Submission Fix Adju... | 0.007000 | 4.93 | 5.0 | Hacking | None | None | 2.40 | 4.12 | 7.60 | False | . 3 cyberzen | Services | VPN &gt; TOR &gt; SOCK TUTORIAL | How to setup a VPN &gt; TOR &gt; SOCK super safe enc... | 0.019017 | 4.89 | 5.0 | Hacking | None | None | 6.52 | 11.20 | 20.65 | False | . 4 businessdude | Services | Facebook hacking guide | . This guide will teach you how to hack Faceb... | 0.062018 | 4.88 | 5.0 | Hacking | None | None | 21.26 | 36.51 | 67.34 | False | . Market Size . Here, I present all the distinct categories that span the hierarchy of all listings on the Agora. For each distinct category, the size represents the total number of items being sold while the two values listed represent the total value of the entire marketplace (category) in 2021 US Dollars. In each graph, there is a top and a bottom Value plot. The top one is simply all the values added up while the bottom one is all the values added up but with the outliers removed. . According to the number of listings and to the entire value of all items listed combined, the marketplace for Drugs on the Agora is the largest one, by far. With over 60,000 listings, and a combined value of over $900 million, it dwarves other categories. . markets = dw_data.groupby(&#39;category&#39;)[&#39;usd&#39;].agg([sum, np.size]).reset_index() .merge(dw_data[~dw_data[&#39;outlier&#39;]] .groupby(&#39;category&#39;)[&#39;usd&#39;] .sum().reset_index(), on=&#39;category&#39;, how=&#39;left&#39;) .rename(columns={&#39;usd&#39;:&#39;sum_no&#39;}) .sort_values(&#39;size&#39;, ascending=False).set_index(&#39;category&#39;) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(3,1, sharex=True, figsize=(15,10)) ax[0].bar(markets.index, markets[&#39;size&#39;], color = &quot;#BB5566&quot;) ax[0].set_xticklabels(markets.index, rotation=70, size = 15) ax[0].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[0].set_ylabel(&quot;Size&quot;, size = 15) ax[1].bar(markets.index, markets[&#39;sum&#39;], color = &quot;#004488&quot;) ax[1].set_xticklabels(markets.index, rotation=70, size = 15) ax[1].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[1].set_ylabel(&quot;Value (USD)&quot;, size = 15) ax[2].bar(markets.index, markets[&#39;sum_no&#39;], color = &quot;#DDAA33&quot;) ax[2].set_xticklabels(markets.index, rotation=70, size = 15) ax[2].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[2].set_ylabel(&quot;Value (USD)&quot;, size = 15) plt.show() . Given such a pronounced difference between the Drugs marketplace and all the other ones, I created a second version of the plot above, but with the Drugs bar removed. The remaining categories are sorted by the number of listings. The graph at the very bottom is cleared of all outlier effects and shows that Counterfeits and Weapons steadily outpace other categories in the total value of items being sold. . markets_no = markets[markets.index != &#39;Drugs&#39;].sort_values(&#39;size&#39;, ascending=False) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(3,1, sharex=True, figsize=(15,10)) ax[0].bar(markets_no.index, markets_no[&#39;size&#39;], color = &quot;#BB5566&quot;) ax[0].set_xticklabels(markets_no.index, rotation=70, size = 15) ax[0].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[0].set_ylabel(&quot;Size&quot;, size = 15) ax[1].bar(markets_no.index, markets_no[&#39;sum&#39;], color = &quot;#004488&quot;) ax[1].set_xticklabels(markets_no.index, rotation=70, size = 15) ax[1].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[1].set_ylabel(&quot;Value (USD)&quot;, size = 15) ax[2].bar(markets_no.index, markets_no[&#39;sum_no&#39;], color = &quot;#DDAA33&quot;) ax[2].set_xticklabels(markets_no.index, rotation=70, size = 15) ax[2].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[2].set_ylabel(&quot;Value (USD)&quot;, size = 15) plt.show() . Weapons . weapons_of_interest = [&#39;Glock&#39;, &#39;Ruger&#39;, &#39;Walther&#39;, &#39;Beretta&#39;, &#39;AK-47&#39;] weapons_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(5): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Lethal firearms&#39;] subset = subset[subset[&#39;item&#39;].str.contains(weapons_of_interest[i], na=False)] weapons_df = weapons_df.append(subset[weapons_df.columns], ignore_index=True) weapons_df[&#39;weapon&#39;] = &#39;weapon&#39; for i in range(5): weapons_df.loc[:, &#39;weapon&#39;][weapons_df[&#39;item&#39;] .str.contains(weapons_of_interest[i], na=False)] = weapons_of_interest[i] weapons_df = weapons_df.reset_index(drop=True) del [subset, i, weapons_of_interest] print(weapons_df.shape) weapons_df.head() . (67, 4) . item price usd weapon . 0 Glock 19 Gen 3 9mm &amp; 2 Mags | 4.862971 | 2863.12 | Glock | . 1 Glock 17 3rd Gen FULL ESCROW | 3.656912 | 2153.04 | Glock | . 2 Glock 21SF 45 ACP FULL ESCROW | 3.656912 | 2153.04 | Glock | . 3 Glock 20 10MM FULL ESCROW | 3.656912 | 2153.04 | Glock | . 4 Glock 26 gen 3 9mm &amp; 2 mags | 4.598510 | 2707.41 | Glock | . Here, I create a data frame listing all the popular weapons brands, most of which can be found in the US legally. The names were extracted from the item descriptions and median prices were obtained for each brand. . weapons = weapons_df.groupby(&#39;weapon&#39;)[&#39;usd&#39;].median() .reset_index().sort_values(&#39;usd&#39;) .set_index(&#39;weapon&#39;) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(figsize=(13,10)) ax.bar(weapons.index, weapons[&#39;usd&#39;], color = &#39;#004488&#39;) ax.set_xticklabels(weapons.index, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) ax.set_ylabel(&quot;Median Cost (USD)&quot;, size = 25) plt.show() . Buying weapons illegaly, compared to the usual US prices, is significantly more expensive. The median price for each brand listen on the Agora is anywhere between two to five times more expensive than if purchased from an accredited weapons dealer. Further analysis would be interesting here to determine the specific difference between legal prices and the ones from the Dark Web. Currently, that is beyond the scope of this project. . Drugs . drugs = dw_data[dw_data[&#39;category&#39;]==&#39;Drugs&#39;].groupby(&#39;category1&#39;)[&#39;usd&#39;] .agg([sum, np.size]).reset_index().sort_values(&#39;size&#39;, ascending=False) .set_index(&#39;category1&#39;) print(drugs.shape) drugs.head() . (13, 2) . sum size . category1 . Cannabis 5.200670e+08 | 20542.0 | . Ecstasy 1.700251e+08 | 9654.0 | . Stimulants 1.852040e+08 | 8474.0 | . Psychedelics 9.215761e+06 | 5384.0 | . Opioids 7.974575e+06 | 4474.0 | . plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(2, 1, sharex=True, figsize=(15,10)) ax[0].bar(drugs.index, drugs[&#39;size&#39;], color = &#39;#BB5566&#39;) ax[0].set_xticklabels(drugs.index, rotation=70, size = 20) ax[0].tick_params(axis=&#39;y&#39;, labelsize= 15) ax[0].set_ylabel(&quot;Size&quot;, size = 20) ax[1].bar(drugs.index, drugs[&#39;sum&#39;], color = &#39;#004488&#39;) ax[1].set_ylabel(&quot;Value (USD)&quot;, size = 20) ax[1].set_xticklabels(drugs.index, rotation=70, size = 20) ax[1].tick_params(axis=&#39;y&#39;, labelsize= 15) plt.show() . As pictured above, cannabis sales are much higher than those from all the other types of drugs sold on the Agora. Both in total market value (over $500 million) and market size (over 20,000 listings), illegal cannabis sales (at least in 2014) were far more popular than those of any other type of drug. . YouTube . social_media_regex = [&#39;[Y|y][O|o][U|u][T|t][U|u][B|b][E|e]&#39;, &#39;[T|t][W|w][I|i][T|t][T|t][E|e][R|r]&#39;, &#39;[F|f][A|a][C|c][E|e][B|b][O|o][O|o][K|k]&#39;, &#39;[I|i][N|n][S|s][T|t][A|a][G|g][R|r][A|a][M|m]&#39;] social_media = [&#39;YouTube&#39;, &#39;Twitter&#39;, &#39;Facebook&#39;, &#39;Instagram&#39;] products_regex = [&#39;[L|l][I|i][K|k][E|e]&#39;, &#39;[S|s][U|u][B|b][S|s][C|c][R|r][I|i][B|b][E|e][R|r]&#39;, &#39;[V|v][I|i][E|e][W|w]&#39;, &#39;[C|c][O|o][M|m][M|m][E|e][N|n][T|t]&#39;] products = [&#39;likes&#39;, &#39;subscribers&#39;, &#39;views&#39;, &#39;comments&#39;] social_media_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(4): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Advertising&#39;] subset = subset[subset[&#39;item&#39;].str.contains(social_media_regex[i], na=False)] social_media_df = social_media_df.append(subset[social_media_df.columns], ignore_index=True) social_media_df[&#39;company&#39;] = &#39;company&#39; social_media_df[&#39;product&#39;] = &#39;product&#39; for i in range(4): social_media_df.loc[:, &#39;company&#39;][social_media_df[&#39;item&#39;] .str.contains(social_media_regex[i], na=False)] = social_media[i] social_media_df.loc[:, &#39;product&#39;][social_media_df[&#39;item&#39;] .str.contains(products_regex[i], na=False)] = products[i] social_media_df = social_media_df[social_media_df[&#39;product&#39;] != &#39;product&#39;] .reset_index(drop=True) quantity = social_media_df[&#39;item&#39;] .str.extract(&#39;( d[ s| d]? d? d? s? d? d? d? s? d? d? d?)&#39;, expand=False).str.replace(&#39; s&#39;, &quot;&quot;) social_media_df[&#39;quantity&#39;] = pd.to_numeric(quantity) social_media_df.loc[64, &#39;quantity&#39;] = 5000 social_media_df[&#39;unit_price&#39;] = social_media_df[&#39;price&#39;] / social_media_df[&#39;quantity&#39;] social_media_df[&#39;unit_usd&#39;] = social_media_df[&#39;usd&#39;] / social_media_df[&#39;quantity&#39;] del [social_media_regex, social_media, products_regex, products, subset, i, quantity] print(social_media_df.shape) social_media_df.head() . (74, 8) . item price usd company product quantity unit_price unit_usd . 0 100 Youtube likes just 8 USD! | 0.032760 | 19.29 | YouTube | likes | 100 | 0.000328 | 0.192900 | . 1 100 Youtube subscriber just 10 USD! | 0.040950 | 24.11 | YouTube | subscribers | 100 | 0.000409 | 0.241100 | . 2 100 Youtube unlike just 10 USD! | 0.040929 | 24.10 | YouTube | likes | 100 | 0.000409 | 0.241000 | . 3 1000 Youtube views just 10 USD | 0.040960 | 24.12 | YouTube | views | 1000 | 0.000041 | 0.024120 | . 4 500 000 Real High Retention Youtube views | 2.747742 | 1617.76 | YouTube | views | 500000 | 0.000005 | 0.003236 | . Gathering social media content into a coherent data frame was a little more work. The code above extracts the per unit cost in purchasing an additional like, subscriber, view or comment for a given social media account. Most listings are regarding Facebook and Youtube. The code above uses the item descriptions to extract the number and type of unit being sold, and creates several new columns that reflect this while facilitating further analysis. . youtube = social_media_df[social_media_df[&#39;company&#39;]==&#39;YouTube&#39;] .groupby(&#39;product&#39;)[&#39;unit_usd&#39;].median().reset_index() .sort_values(&#39;unit_usd&#39;, ascending=False).set_index(&#39;product&#39;) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(figsize=(15,10)) ax.bar(youtube.index, youtube[&#39;unit_usd&#39;], color = &#39;#228833&#39;) ax.set_xticklabels(youtube.index, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) ax.set_ylabel(&quot;Median Cost per Unit (USD)&quot;, size = 20) plt.show() . Pictured above, is the data for YouTube. According to all the postings, each additional comment was worth around 60 cents of a 2021 USD, putting it higher than subscribers, likes or views. Vast majority of these listings indicate that each product is of &#39;high quality&#39; meaning they are such that bot detecting software would not cleanse them. This usually indicates click farms (mostly from third world countries in which people are paid to constantly create new accounts and provide social media engagement) but such a claim cannot be ascertained from given data alone. . Below, I quickly calculate the expected per-unit-cost of a Facebook like, which seems to be a somewhat lower value than YouTube likes. . fb_like = social_media_df[(social_media_df[&#39;company&#39;]==&#39;Facebook&#39;) &amp; (social_media_df[&#39;product&#39;]==&#39;likes&#39;)] .loc[:, &#39;unit_usd&#39;].median() print(&quot;The expected per-unit cost of a Facebook like is&quot;, round(fb_like*100, 2), &quot;cents (2021 $USD).&quot;) . The expected per-unit cost of a Facebook like is 12.48 cents (2021 $USD). . Documents . doc_regex = [&#39;[P|p]assport&#39;, &#39;[D|d]river[ &#39;]?s[ s]?&#39;] doc = [&quot;Passport&quot;, &quot;Driver&#39;s License&quot;] docs_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(2): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Physical documents&#39;] subset = subset[subset[&#39;item&#39;].str.contains(doc_regex[i], na=False)] docs_df = docs_df.append(subset[docs_df.columns], ignore_index=True) docs_df[&#39;document&#39;] = &#39;document&#39; for i in range(2): docs_df.loc[:, &#39;document&#39;][docs_df[&#39;item&#39;] .str.contains(doc_regex[i], na=False)] = doc[i] docs_df.loc[:, &#39;document&#39;][docs_df[&#39;item&#39;] .str.contains(&#39;Passport[ s]?[+]&#39;)] = &quot;Passport+ID+Driver&#39;s License&quot; docs_df = docs_df[~docs_df[&#39;item&#39;].str.contains(&#39;emplate&#39;)].reset_index(drop=True) del [doc, doc_regex, i, subset] print(docs_df.shape) docs_df.head() . (114, 4) . item price usd document . 0 Fake Danish Passport | 2.808987 | 1653.82 | Passport | . 1 Fake Lithuanian Passport (old version) | 3.803150 | 2239.14 | Passport | . 2 Fake Lithuanian Passport | 2.475635 | 1457.55 | Passport | . 3 Netherlands Physical Passport + DL + ID CARD | 7.761003 | 4569.36 | Passport+ID+Driver&#39;s License | . 4 Pysical Fake Passports and IDs | 0.015551 | 9.16 | Passport | . This data frame was created in a way similar to the social media one. Item descriptions were used to separate out the type of &#39;product&#39; being sold. Three specific groups were identified: Passports, Driver&#39;s Licenses and a combination product of Passport, ID and Driver&#39;s License. These are worldwide so documents belong to numerous countries and/or states. For the sake of simplicity, the median cost is calculated on a worldwide basis. Counterfeit document listing also include a lot of templates being sold, which were filtered out, given that their values were significantly lower. . documents = docs_df.groupby(&#39;document&#39;)[&#39;usd&#39;].median().reset_index() .sort_values(&#39;usd&#39;, ascending=False).set_index(&#39;document&#39;) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(figsize=(15,10)) ax.bar(documents.index, documents[&#39;usd&#39;], color = &#39;#AA3377&#39;) ax.set_xticklabels(documents.index, size = 15) ax.set_ylabel(&quot;Median Cost (USD)&quot;, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) plt.show() . Median costs were calcualted given that there are numerous countries listed, with varying degrees of value associated with their documentation, as well as several prominent outliers. While this category provides listings for real documents, that is, documents that are actually harder to detect as illegaly obtained and that exist in some database, there are many from the Fake category that slip into this one. They usually cost much less but the wording in the item description is difficult to use in filtering them out. Hopefully, the median expectation protects against these outlier effects. . Cigarettes . cigs_regex = [&#39;[D|d][U|u][N|n][H|h][I|i][L|l][L|l]&#39;, &#39;[B|b][O|o][N|n][D|d]&#39;, &#39;[V|v][O|o][G|g][U|u][E|e]&#39;, &#39;[W|w][I|i][N|n][S|s][T|t][O|o][N|n]&#39;, &#39;[M|m][A|a][R|r][L|l][B|b][O|o][R|r][O|o]&#39;, &#39;[P|p][A|a][L|l][L|l][ s]?[M|m][A|a][L|l][L|l]&#39;, &#39;[L|l][ s]?[&amp;][ s]?[M|m]&#39;, &#39;[L|l][U|u][C|c][K|k][Y|y]&#39;, &#39;[C|c][A|a][M|m][E|e][L|l]&#39;, &#39;[C|c][H|h][E|e][S|s][T|t][E|e][R|r]&#39;, &#39;[D|d][A|a][V|v][I|i][D|d][O|o]&#39;, &#39;[P|p][A|a][R|r][L|l][I|i][A|a][M|m]&#39;, &#39;[V|v][I|i][R|r][G|g][I|i][N|n][I|i][A|a]&#39;, &#39;[R|r][E|e][D|d][ s]?[&amp;][ s]?[W|w][H|h][I|i][T|t][E|e]&#39;] cigs = [&#39;Dunhill&#39;, &#39;Bond&#39;, &#39;Vogue&#39;, &#39;Winston&#39;, &#39;Marlboro&#39;, &#39;Pall Mall&#39;, &#39;L&amp;M&#39;, &#39;Lucky Strike&#39;, &#39;Camel&#39;, &#39;Chesterfield&#39;, &#39;Davidoff&#39;, &#39;Parliament&#39;, &#39;Virginia&#39;, &#39;Red&amp;White&#39;] cigs_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(10): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Smoked&#39;] subset = subset[subset[&#39;item&#39;].str.contains(cigs_regex[i], na=False)] cigs_df = cigs_df.append(subset[cigs_df.columns], ignore_index=True) cigs_df[&#39;brand&#39;] = &#39;brand&#39; for i in range(14): cigs_df.loc[:, &#39;brand&#39;][cigs_df[&#39;item&#39;] .str.contains(cigs_regex[i], na=False)] = cigs[i] # almost all original prices are listed for 10 packs, so I divide by 10 to get pack price # but sometimes it&#39;s a single pack listed and that&#39;s why I did this division only when # the total price listed was above 10USD cigs_df[&#39;unit_cost&#39;] = cigs_df[&#39;usd&#39;] cigs_df.loc[:, &#39;unit_cost&#39;][cigs_df[&#39;unit_cost&#39;] &gt; 10] = cigs_df[&#39;unit_cost&#39;] / 10 cigs_df = cigs_df.reset_index(drop=True) del [cigs_regex, cigs, subset, i] print(cigs_df.shape) cigs_df.head() . (101, 5) . item price usd brand unit_cost . 0 Dunhill Fine Cut Dark Blue (10 packs x 20 ciga... | 0.160849 | 94.70 | Dunhill | 9.470 | . 1 DUNHILL Cheap Cigarettes to USA and EU | 0.156015 | 91.86 | Dunhill | 9.186 | . 2 Dunhill Fine Cut Black (10 packs x 20 cigarettes) | 0.160806 | 94.68 | Dunhill | 9.468 | . 3 Copy of Dunhill Fine Cut Black (10 packs x 20 ... | 0.091288 | 53.75 | Dunhill | 5.375 | . 4 Bond Cheap Cigarettes to USA and EU | 0.088071 | 51.85 | Bond | 5.185 | . Again, repeating the procedure used for the social media and documentation data frames, once again I go through the item descriptions and extract all the relevant cigarette brands to group by later. Furthermore, it is clear from the description that almost every single listed price is for a product consisting of 10 packs with 20 cigarettes each. Therefore, the Bitcoin price was converted to US Dollar values, and divided by 10, to get the cost of a single pack for each brand. . cigarettes = cigs_df.groupby(&#39;brand&#39;)[&#39;unit_cost&#39;].median().reset_index() .sort_values(&#39;unit_cost&#39;, ascending=False).set_index(&#39;brand&#39;) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(figsize=(15,10)) ax.bar(cigarettes.index, cigarettes[&#39;unit_cost&#39;], color = &#39;#CC3311&#39;) ax.set_xticklabels(cigarettes.index, rotation=80, size = 20) ax.set_ylabel(&quot;Median Cost per Pack (USD)&quot;, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) plt.show() . As can be seen in the plot above, the brands are sorted by cost. The prices are median costs, per pack, in 2021 $USD. Surprisingly, they don&#39;t differ significantly from prices in states with low to moderate taxes. However, they are much smaller than those in states like New York, which have costs of around $15 per pack, easily. . Conclusion . As an initial look into the illegal dealings on the Dark Web, this project is appropriate. It allows for a preliminary sense of how products are listed, what the scope of the markets is, how data is organized and what possible insights can be gleaned from it. Furthermore, it illuminates all the issues present in trying to make sense of it all. The prices are listed in Bitcoin and held online for unknown periods of time, even as Bitcoin fluctuates. Knowing the possible range for a year helps in narrowing down the expected cost values. However, error bars need to be created to show the amount of uncertainty in this approach. . With more computing power and detailed string parsing scripts, I would be willing to take a crack at the 1.6TB data set. Price differences between legal and illegal products can illuminate countless premiums customers are willing to pay on various products, which is a worthwhile endeavor for any economist to pursue. . For the data and other notebooks, see github.com/antoniojurlina/learning_python. .",
            "url": "https://antoniojurlina.github.io/portfolio/2021/05/07/Dark_Web.html",
            "relUrl": "/2021/05/07/Dark_Web.html",
            "date": " • May 7, 2021"
        }
        
    
  

  
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page9": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://antoniojurlina.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}