{
  
    
        "post0": {
            "title": "Agora - A Dark Web Marketplace",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import os import warnings . A few years ago, a reddit user called usheep scraped a DarkWeb marketplace called the Agora for a list of vendors, their identities and products, while threatening to expose them all as blackmail. Eventually, this data set made it to the general public, and while usheep disappeared, we have thousands of rows of vendors, items, categories, descriptions and prices in an organized data set that contains drugs, weapons, books, services, and more. There are over 100,000 unique observations spanning year 2014. My goal is to analyze this data set and explore some common items being sold, as well as the usual prices across categories. . Although there is over 1.6TB of uncompressed, scraped data of dark web sales available out there, the goal of this project will be to look at the Kaggle published subset which is around 30MB and contains ~100,000 observations. The data set is available as a direct .csv download with a Kaggle account. Additonally, since all the prices listed in this data set are in Bitcoin, I went to Coindesk and downloaded the daily closing price of Bitcoin in US Dollar amounts, for that whole period. This data set will be used to make the costs more comprehensible by converting them to current USD prices. . dw_data = pd.read_csv(&quot;Agora.csv&quot;, usecols=[&#39;Vendor&#39;, &#39; Category&#39;, &#39; Item&#39;, &#39; Item Description&#39;, &#39; Price&#39;, &#39; Rating&#39;]) .rename(columns={&#39;Vendor&#39;:&#39;vendor&#39;, &#39; Category&#39;:&#39;category&#39;, &#39; Item&#39;:&#39;item&#39;, &#39; Item Description&#39;:&#39;description&#39;, &#39; Price&#39;:&#39;price&#39;, &#39; Rating&#39;:&#39;rating&#39;}) # Importing Bitcoin data and selecting out the relevant column bitcoin = pd.read_csv(&quot;BTC_USD_2014-01-02_2015-01-01-CoinDesk.csv&quot;) inflation = 0.1186 # the inflation rate between 2014 and 2021 bitcoin = bitcoin.iloc[:,2]*(1+inflation) . The Agora data set sample, as it is currently in the memory, is shown below. The variables that need to be cleaned up further are category, price, and rating. . Category: There are categories and subcategories within this column that need to be split into separate columns. | Price: This a string because the word BTC is present in each and should be converted to numeric for further analysis. | Rating: This shows the rating and the possible highest score, which should also be separated out. | . dw_data.head() . vendor category item description price rating . 0 CheapPayTV | Services/Hacking | 12 Month HuluPlus gift Code | 12-Month HuluPlus Codes for $25. They are wort... | 0.05027025666666667 BTC | 4.96/5 | . 1 CheapPayTV | Services/Hacking | Pay TV Sky UK Sky Germany HD TV and much mor... | Hi we offer a World Wide CCcam Service for En... | 0.152419585 BTC | 4.96/5 | . 2 KryptykOG | Services/Hacking | OFFICIAL Account Creator Extreme 4.2 | Tagged Submission Fix Bebo Submission Fix Adju... | 0.007000000000000005 BTC | 4.93/5 | . 3 cyberzen | Services/Hacking | VPN &gt; TOR &gt; SOCK TUTORIAL | How to setup a VPN &gt; TOR &gt; SOCK super safe enc... | 0.019016783532494728 BTC | 4.89/5 | . 4 businessdude | Services/Hacking | Facebook hacking guide | . This guide will teach you how to hack Faceb... | 0.062018073963963936 BTC | 4.88/5 | . From the bitcoin data set, the only relevant variable is column 2, which has been adjusted from 2014 to 2021 USD values and extracted as a single Series. From here, I can obtain the average bitcoin value for 2014 and use it to convert all the listed prices in the Agora data set. This is not a flawless approach as the sale listings do not have specific dates associated with them and Bitcoin varies daily. Therefore, I will also use the lowest and highest Bitcoin values in USD across 2014 to show the possible price ranges. . bitcoin.head() . 0 860.313571 1 899.384815 2 909.776933 3 974.430167 4 1085.770018 Name: Closing Price (USD), dtype: float64 . Cleaning Data . Below is the code that performs the data cleaning discussed above. In addition to all the steps previously outlined, I find price outliers within each category and create a new Boolean column which indicates whether each value is an outlier within its respective category. . In this case, outliers are defined as prices that are above the threshold: . $$price &gt; 1.5 * IQR + 3^{rd} Quartile$$ . where IQR stands for interquartile range. . dw_data = dw_data[dw_data[&#39;price&#39;].str.contains(&#39;BTC&#39;, na=False)] dw_data[&#39;price&#39;] = dw_data[&#39;price&#39;].str.replace(&quot; BTC&quot;, &quot;&quot;) dw_data[&#39;price&#39;] = pd.to_numeric(dw_data[&#39;price&#39;]) # Separating the rating column into rating and total possible score columns dw_data = dw_data[~dw_data[&#39;rating&#39;].str.contains(&#39;[0 deals]&#39;, na=False)] dw_data[&#39;rating&#39;] = dw_data[&#39;rating&#39;].str.replace(&quot;~&quot;, &quot;&quot;) ratings = dw_data[&#39;rating&#39;].str.split(&quot;/&quot;, n=3,expand=True) dw_data[&#39;rating&#39;] = pd.to_numeric(ratings[0]) dw_data[&#39;rating_total&#39;] = pd.to_numeric(ratings[1]) # separating out the categories hierarchy categories = dw_data[&#39;category&#39;].str.split(&quot;/&quot;, n=3,expand=True) categories[0] = categories[0].str.replace(&quot;^Info$&quot;, &quot;Information&quot;) categories_to_use = [&#39;Services&#39;, &#39;Drugs&#39;, &#39;Forgeries&#39;, &#39;Tobacco&#39;, &#39;Counterfeits&#39;, &#39;Data&#39;, &#39;Information&#39;, &#39;Electronics&#39;, &#39;Drug paraphernalia&#39;, &#39;Other&#39;, &#39;Jewelry&#39;, &#39;Weapons&#39;, &#39;Chemicals&#39;] dw_data[&#39;category&#39;] = categories[0] dw_data[&#39;category1&#39;] = categories[1] dw_data[&#39;category2&#39;] = categories[2] dw_data[&#39;category3&#39;] = categories[3] dw_data = dw_data[dw_data[&#39;category&#39;].isin(categories_to_use)].reset_index(drop=True) # converting BTC to 2021 $USD values dw_data[&#39;usd_low&#39;] = np.round(np.min(bitcoin) * dw_data[&#39;price&#39;], 2) dw_data[&#39;usd&#39;] = np.round(np.mean(bitcoin) * dw_data[&#39;price&#39;], 2) dw_data[&#39;usd_high&#39;] = np.round(np.max(bitcoin) * dw_data[&#39;price&#39;], 2) # adding a column that signifies which value is an outlier within top level category def limit(column): iqr = column.quantile(0.75) - column.quantile(0.25) top = column.quantile(0.75) + 1.5*iqr return top limits = dw_data.groupby(&#39;category&#39;)[&#39;usd&#39;].agg(limit) .reset_index().rename(columns={&#39;usd&#39;:&#39;limit&#39;}) dw_data = dw_data.merge(limits, on=&#39;category&#39;, how=&#39;left&#39;) dw_data[&#39;outlier&#39;] = dw_data[&#39;usd&#39;] &gt; dw_data[&#39;limit&#39;] dw_data = dw_data.drop(&#39;limit&#39;, axis=1) # deleting all intermediate data frames del [categories, categories_to_use, ratings, limits] . The resulting data frame is presented below. . print(dw_data.shape) dw_data.head() . (73314, 14) . vendor category item description price rating rating_total category1 category2 category3 usd_low usd usd_high outlier . 0 CheapPayTV | Services | 12 Month HuluPlus gift Code | 12-Month HuluPlus Codes for $25. They are wort... | 0.050270 | 4.96 | 5.0 | Hacking | None | None | 17.23 | 29.60 | 54.58 | False | . 1 CheapPayTV | Services | Pay TV Sky UK Sky Germany HD TV and much mor... | Hi we offer a World Wide CCcam Service for En... | 0.152420 | 4.96 | 5.0 | Hacking | None | None | 52.25 | 89.74 | 165.49 | False | . 2 KryptykOG | Services | OFFICIAL Account Creator Extreme 4.2 | Tagged Submission Fix Bebo Submission Fix Adju... | 0.007000 | 4.93 | 5.0 | Hacking | None | None | 2.40 | 4.12 | 7.60 | False | . 3 cyberzen | Services | VPN &gt; TOR &gt; SOCK TUTORIAL | How to setup a VPN &gt; TOR &gt; SOCK super safe enc... | 0.019017 | 4.89 | 5.0 | Hacking | None | None | 6.52 | 11.20 | 20.65 | False | . 4 businessdude | Services | Facebook hacking guide | . This guide will teach you how to hack Faceb... | 0.062018 | 4.88 | 5.0 | Hacking | None | None | 21.26 | 36.51 | 67.34 | False | . Market Size . Here, I present all the distinct categories that span the hierarchy of all listings on the Agora. For each distinct category, the size represents the total number of items being sold while the two values listed represent the total value of the entire marketplace (category) in 2021 US Dollars. In each graph, there is a top and a bottom Value plot. The top one is simply all the values added up while the bottom one is all the values added up but with the outliers removed. . According to the number of listings and to the entire value of all items listed combined, the marketplace for Drugs on the Agora is the largest one, by far. With over 60,000 listings, and a combined value of over $900 million, it dwarves other categories. . markets = dw_data.groupby(&#39;category&#39;)[&#39;usd&#39;].agg([sum, np.size]).reset_index() .merge(dw_data[~dw_data[&#39;outlier&#39;]] .groupby(&#39;category&#39;)[&#39;usd&#39;] .sum().reset_index(), on=&#39;category&#39;, how=&#39;left&#39;) .rename(columns={&#39;usd&#39;:&#39;sum_no&#39;}) .sort_values(&#39;size&#39;, ascending=False).set_index(&#39;category&#39;) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(3,1, sharex=True, figsize=(15,10)) ax[0].bar(markets.index, markets[&#39;size&#39;], color = &quot;#BB5566&quot;) ax[0].set_xticklabels(markets.index, rotation=70, size = 15) ax[0].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[0].set_ylabel(&quot;Size&quot;, size = 15) ax[1].bar(markets.index, markets[&#39;sum&#39;], color = &quot;#004488&quot;) ax[1].set_xticklabels(markets.index, rotation=70, size = 15) ax[1].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[1].set_ylabel(&quot;Value (USD)&quot;, size = 15) ax[2].bar(markets.index, markets[&#39;sum_no&#39;], color = &quot;#DDAA33&quot;) ax[2].set_xticklabels(markets.index, rotation=70, size = 15) ax[2].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[2].set_ylabel(&quot;Value (USD)&quot;, size = 15) plt.show() . Given such a pronounced difference between the Drugs marketplace and all the other ones, I created a second version of the plot above, but with the Drugs bar removed. The remaining categories are sorted by the number of listings. The graph at the very bottom is cleared of all outlier effects and shows that Counterfeits and Weapons steadily outpace other categories in the total value of items being sold. . markets_no = markets[markets.index != &#39;Drugs&#39;].sort_values(&#39;size&#39;, ascending=False) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(3,1, sharex=True, figsize=(15,10)) ax[0].bar(markets_no.index, markets_no[&#39;size&#39;], color = &quot;#BB5566&quot;) ax[0].set_xticklabels(markets_no.index, rotation=70, size = 15) ax[0].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[0].set_ylabel(&quot;Size&quot;, size = 15) ax[1].bar(markets_no.index, markets_no[&#39;sum&#39;], color = &quot;#004488&quot;) ax[1].set_xticklabels(markets_no.index, rotation=70, size = 15) ax[1].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[1].set_ylabel(&quot;Value (USD)&quot;, size = 15) ax[2].bar(markets_no.index, markets_no[&#39;sum_no&#39;], color = &quot;#DDAA33&quot;) ax[2].set_xticklabels(markets_no.index, rotation=70, size = 15) ax[2].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[2].set_ylabel(&quot;Value (USD)&quot;, size = 15) plt.show() . Weapons . weapons_of_interest = [&#39;Glock&#39;, &#39;Ruger&#39;, &#39;Walther&#39;, &#39;Beretta&#39;, &#39;AK-47&#39;] weapons_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(5): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Lethal firearms&#39;] subset = subset[subset[&#39;item&#39;].str.contains(weapons_of_interest[i], na=False)] weapons_df = weapons_df.append(subset[weapons_df.columns], ignore_index=True) weapons_df[&#39;weapon&#39;] = &#39;weapon&#39; for i in range(5): weapons_df.loc[:, &#39;weapon&#39;][weapons_df[&#39;item&#39;] .str.contains(weapons_of_interest[i], na=False)] = weapons_of_interest[i] weapons_df = weapons_df.reset_index(drop=True) del [subset, i, weapons_of_interest] print(weapons_df.shape) weapons_df.head() . (67, 4) . item price usd weapon . 0 Glock 19 Gen 3 9mm &amp; 2 Mags | 4.862971 | 2863.12 | Glock | . 1 Glock 17 3rd Gen FULL ESCROW | 3.656912 | 2153.04 | Glock | . 2 Glock 21SF 45 ACP FULL ESCROW | 3.656912 | 2153.04 | Glock | . 3 Glock 20 10MM FULL ESCROW | 3.656912 | 2153.04 | Glock | . 4 Glock 26 gen 3 9mm &amp; 2 mags | 4.598510 | 2707.41 | Glock | . Here, I create a data frame listing all the popular weapons brands, most of which can be found in the US legally. The names were extracted from the item descriptions and median prices were obtained for each brand. . weapons = weapons_df.groupby(&#39;weapon&#39;)[&#39;usd&#39;].median() .reset_index().sort_values(&#39;usd&#39;) .set_index(&#39;weapon&#39;) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(figsize=(13,10)) ax.bar(weapons.index, weapons[&#39;usd&#39;], color = &#39;#004488&#39;) ax.set_xticklabels(weapons.index, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) ax.set_ylabel(&quot;Median Cost (USD)&quot;, size = 25) plt.show() . Buying weapons illegaly, compared to the usual US prices, is significantly more expensive. The median price for each brand listen on the Agora is anywhere between two to five times more expensive than if purchased from an accredited weapons dealer. Further analysis would be interesting here to determine the specific difference between legal prices and the ones from the Dark Web. Currently, that is beyond the scope of this project. . Drugs . drugs = dw_data[dw_data[&#39;category&#39;]==&#39;Drugs&#39;].groupby(&#39;category1&#39;)[&#39;usd&#39;] .agg([sum, np.size]).reset_index().sort_values(&#39;size&#39;, ascending=False) .set_index(&#39;category1&#39;) print(drugs.shape) drugs.head() . (13, 2) . sum size . category1 . Cannabis 5.200670e+08 | 20542.0 | . Ecstasy 1.700251e+08 | 9654.0 | . Stimulants 1.852040e+08 | 8474.0 | . Psychedelics 9.215761e+06 | 5384.0 | . Opioids 7.974575e+06 | 4474.0 | . plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(2, 1, sharex=True, figsize=(15,10)) ax[0].bar(drugs.index, drugs[&#39;size&#39;], color = &#39;#BB5566&#39;) ax[0].set_xticklabels(drugs.index, rotation=70, size = 20) ax[0].tick_params(axis=&#39;y&#39;, labelsize= 15) ax[0].set_ylabel(&quot;Size&quot;, size = 20) ax[1].bar(drugs.index, drugs[&#39;sum&#39;], color = &#39;#004488&#39;) ax[1].set_ylabel(&quot;Value (USD)&quot;, size = 20) ax[1].set_xticklabels(drugs.index, rotation=70, size = 20) ax[1].tick_params(axis=&#39;y&#39;, labelsize= 15) plt.show() . As pictured above, cannabis sales are much higher than those from all the other types of drugs sold on the Agora. Both in total market value (over $500 million) and market size (over 20,000 listings), illegal cannabis sales (at least in 2014) were far more popular than those of any other type of drug. . YouTube . social_media_regex = [&#39;[Y|y][O|o][U|u][T|t][U|u][B|b][E|e]&#39;, &#39;[T|t][W|w][I|i][T|t][T|t][E|e][R|r]&#39;, &#39;[F|f][A|a][C|c][E|e][B|b][O|o][O|o][K|k]&#39;, &#39;[I|i][N|n][S|s][T|t][A|a][G|g][R|r][A|a][M|m]&#39;] social_media = [&#39;YouTube&#39;, &#39;Twitter&#39;, &#39;Facebook&#39;, &#39;Instagram&#39;] products_regex = [&#39;[L|l][I|i][K|k][E|e]&#39;, &#39;[S|s][U|u][B|b][S|s][C|c][R|r][I|i][B|b][E|e][R|r]&#39;, &#39;[V|v][I|i][E|e][W|w]&#39;, &#39;[C|c][O|o][M|m][M|m][E|e][N|n][T|t]&#39;] products = [&#39;likes&#39;, &#39;subscribers&#39;, &#39;views&#39;, &#39;comments&#39;] social_media_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(4): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Advertising&#39;] subset = subset[subset[&#39;item&#39;].str.contains(social_media_regex[i], na=False)] social_media_df = social_media_df.append(subset[social_media_df.columns], ignore_index=True) social_media_df[&#39;company&#39;] = &#39;company&#39; social_media_df[&#39;product&#39;] = &#39;product&#39; for i in range(4): social_media_df.loc[:, &#39;company&#39;][social_media_df[&#39;item&#39;] .str.contains(social_media_regex[i], na=False)] = social_media[i] social_media_df.loc[:, &#39;product&#39;][social_media_df[&#39;item&#39;] .str.contains(products_regex[i], na=False)] = products[i] social_media_df = social_media_df[social_media_df[&#39;product&#39;] != &#39;product&#39;] .reset_index(drop=True) quantity = social_media_df[&#39;item&#39;] .str.extract(&#39;( d[ s| d]? d? d? s? d? d? d? s? d? d? d?)&#39;, expand=False).str.replace(&#39; s&#39;, &quot;&quot;) social_media_df[&#39;quantity&#39;] = pd.to_numeric(quantity) social_media_df.loc[64, &#39;quantity&#39;] = 5000 social_media_df[&#39;unit_price&#39;] = social_media_df[&#39;price&#39;] / social_media_df[&#39;quantity&#39;] social_media_df[&#39;unit_usd&#39;] = social_media_df[&#39;usd&#39;] / social_media_df[&#39;quantity&#39;] del [social_media_regex, social_media, products_regex, products, subset, i, quantity] print(social_media_df.shape) social_media_df.head() . (74, 8) . item price usd company product quantity unit_price unit_usd . 0 100 Youtube likes just 8 USD! | 0.032760 | 19.29 | YouTube | likes | 100 | 0.000328 | 0.192900 | . 1 100 Youtube subscriber just 10 USD! | 0.040950 | 24.11 | YouTube | subscribers | 100 | 0.000409 | 0.241100 | . 2 100 Youtube unlike just 10 USD! | 0.040929 | 24.10 | YouTube | likes | 100 | 0.000409 | 0.241000 | . 3 1000 Youtube views just 10 USD | 0.040960 | 24.12 | YouTube | views | 1000 | 0.000041 | 0.024120 | . 4 500 000 Real High Retention Youtube views | 2.747742 | 1617.76 | YouTube | views | 500000 | 0.000005 | 0.003236 | . Gathering social media content into a coherent data frame was a little more work. The code above extracts the per unit cost in purchasing an additional like, subscriber, view or comment for a given social media account. Most listings are regarding Facebook and Youtube. The code above uses the item descriptions to extract the number and type of unit being sold, and creates several new columns that reflect this while facilitating further analysis. . youtube = social_media_df[social_media_df[&#39;company&#39;]==&#39;YouTube&#39;] .groupby(&#39;product&#39;)[&#39;unit_usd&#39;].median().reset_index() .sort_values(&#39;unit_usd&#39;, ascending=False).set_index(&#39;product&#39;) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(figsize=(15,10)) ax.bar(youtube.index, youtube[&#39;unit_usd&#39;], color = &#39;#228833&#39;) ax.set_xticklabels(youtube.index, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) ax.set_ylabel(&quot;Median Cost per Unit (USD)&quot;, size = 20) plt.show() . Pictured above, is the data for YouTube. According to all the postings, each additional comment was worth around 60 cents of a 2021 USD, putting it higher than subscribers, likes or views. Vast majority of these listings indicate that each product is of &#39;high quality&#39; meaning they are such that bot detecting software would not cleanse them. This usually indicates click farms (mostly from third world countries in which people are paid to constantly create new accounts and provide social media engagement) but such a claim cannot be ascertained from given data alone. . Below, I quickly calculate the expected per-unit-cost of a Facebook like, which seems to be a somewhat lower value than YouTube likes. . fb_like = social_media_df[(social_media_df[&#39;company&#39;]==&#39;Facebook&#39;) &amp; (social_media_df[&#39;product&#39;]==&#39;likes&#39;)] .loc[:, &#39;unit_usd&#39;].median() print(&quot;The expected per-unit cost of a Facebook like is&quot;, round(fb_like*100, 2), &quot;cents (2021 $USD).&quot;) . The expected per-unit cost of a Facebook like is 12.48 cents (2021 $USD). . Documents . doc_regex = [&#39;[P|p]assport&#39;, &#39;[D|d]river[ &#39;]?s[ s]?&#39;] doc = [&quot;Passport&quot;, &quot;Driver&#39;s License&quot;] docs_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(2): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Physical documents&#39;] subset = subset[subset[&#39;item&#39;].str.contains(doc_regex[i], na=False)] docs_df = docs_df.append(subset[docs_df.columns], ignore_index=True) docs_df[&#39;document&#39;] = &#39;document&#39; for i in range(2): docs_df.loc[:, &#39;document&#39;][docs_df[&#39;item&#39;] .str.contains(doc_regex[i], na=False)] = doc[i] docs_df.loc[:, &#39;document&#39;][docs_df[&#39;item&#39;] .str.contains(&#39;Passport[ s]?[+]&#39;)] = &quot;Passport+ID+Driver&#39;s License&quot; docs_df = docs_df[~docs_df[&#39;item&#39;].str.contains(&#39;emplate&#39;)].reset_index(drop=True) del [doc, doc_regex, i, subset] print(docs_df.shape) docs_df.head() . (114, 4) . item price usd document . 0 Fake Danish Passport | 2.808987 | 1653.82 | Passport | . 1 Fake Lithuanian Passport (old version) | 3.803150 | 2239.14 | Passport | . 2 Fake Lithuanian Passport | 2.475635 | 1457.55 | Passport | . 3 Netherlands Physical Passport + DL + ID CARD | 7.761003 | 4569.36 | Passport+ID+Driver&#39;s License | . 4 Pysical Fake Passports and IDs | 0.015551 | 9.16 | Passport | . This data frame was created in a way similar to the social media one. Item descriptions were used to separate out the type of &#39;product&#39; being sold. Three specific groups were identified: Passports, Driver&#39;s Licenses and a combination product of Passport, ID and Driver&#39;s License. These are worldwide so documents belong to numerous countries and/or states. For the sake of simplicity, the median cost is calculated on a worldwide basis. Counterfeit document listing also include a lot of templates being sold, which were filtered out, given that their values were significantly lower. . documents = docs_df.groupby(&#39;document&#39;)[&#39;usd&#39;].median().reset_index() .sort_values(&#39;usd&#39;, ascending=False).set_index(&#39;document&#39;) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(figsize=(15,10)) ax.bar(documents.index, documents[&#39;usd&#39;], color = &#39;#AA3377&#39;) ax.set_xticklabels(documents.index, size = 15) ax.set_ylabel(&quot;Median Cost (USD)&quot;, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) plt.show() . Median costs were calcualted given that there are numerous countries listed, with varying degrees of value associated with their documentation, as well as several prominent outliers. While this category provides listings for real documents, that is, documents that are actually harder to detect as illegaly obtained and that exist in some database, there are many from the Fake category that slip into this one. They usually cost much less but the wording in the item description is difficult to use in filtering them out. Hopefully, the median expectation protects against these outlier effects. . Cigarettes . cigs_regex = [&#39;[D|d][U|u][N|n][H|h][I|i][L|l][L|l]&#39;, &#39;[B|b][O|o][N|n][D|d]&#39;, &#39;[V|v][O|o][G|g][U|u][E|e]&#39;, &#39;[W|w][I|i][N|n][S|s][T|t][O|o][N|n]&#39;, &#39;[M|m][A|a][R|r][L|l][B|b][O|o][R|r][O|o]&#39;, &#39;[P|p][A|a][L|l][L|l][ s]?[M|m][A|a][L|l][L|l]&#39;, &#39;[L|l][ s]?[&amp;][ s]?[M|m]&#39;, &#39;[L|l][U|u][C|c][K|k][Y|y]&#39;, &#39;[C|c][A|a][M|m][E|e][L|l]&#39;, &#39;[C|c][H|h][E|e][S|s][T|t][E|e][R|r]&#39;, &#39;[D|d][A|a][V|v][I|i][D|d][O|o]&#39;, &#39;[P|p][A|a][R|r][L|l][I|i][A|a][M|m]&#39;, &#39;[V|v][I|i][R|r][G|g][I|i][N|n][I|i][A|a]&#39;, &#39;[R|r][E|e][D|d][ s]?[&amp;][ s]?[W|w][H|h][I|i][T|t][E|e]&#39;] cigs = [&#39;Dunhill&#39;, &#39;Bond&#39;, &#39;Vogue&#39;, &#39;Winston&#39;, &#39;Marlboro&#39;, &#39;Pall Mall&#39;, &#39;L&amp;M&#39;, &#39;Lucky Strike&#39;, &#39;Camel&#39;, &#39;Chesterfield&#39;, &#39;Davidoff&#39;, &#39;Parliament&#39;, &#39;Virginia&#39;, &#39;Red&amp;White&#39;] cigs_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(10): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Smoked&#39;] subset = subset[subset[&#39;item&#39;].str.contains(cigs_regex[i], na=False)] cigs_df = cigs_df.append(subset[cigs_df.columns], ignore_index=True) cigs_df[&#39;brand&#39;] = &#39;brand&#39; for i in range(14): cigs_df.loc[:, &#39;brand&#39;][cigs_df[&#39;item&#39;] .str.contains(cigs_regex[i], na=False)] = cigs[i] # almost all original prices are listed for 10 packs, so I divide by 10 to get pack price # but sometimes it&#39;s a single pack listed and that&#39;s why I did this division only when # the total price listed was above 10USD cigs_df[&#39;unit_cost&#39;] = cigs_df[&#39;usd&#39;] cigs_df.loc[:, &#39;unit_cost&#39;][cigs_df[&#39;unit_cost&#39;] &gt; 10] = cigs_df[&#39;unit_cost&#39;] / 10 cigs_df = cigs_df.reset_index(drop=True) del [cigs_regex, cigs, subset, i] print(cigs_df.shape) cigs_df.head() . (101, 5) . item price usd brand unit_cost . 0 Dunhill Fine Cut Dark Blue (10 packs x 20 ciga... | 0.160849 | 94.70 | Dunhill | 9.470 | . 1 DUNHILL Cheap Cigarettes to USA and EU | 0.156015 | 91.86 | Dunhill | 9.186 | . 2 Dunhill Fine Cut Black (10 packs x 20 cigarettes) | 0.160806 | 94.68 | Dunhill | 9.468 | . 3 Copy of Dunhill Fine Cut Black (10 packs x 20 ... | 0.091288 | 53.75 | Dunhill | 5.375 | . 4 Bond Cheap Cigarettes to USA and EU | 0.088071 | 51.85 | Bond | 5.185 | . Again, repeating the procedure used for the social media and documentation data frames, once again I go through the item descriptions and extract all the relevant cigarette brands to group by later. Furthermore, it is clear from the description that almost every single listed price is for a product consisting of 10 packs with 20 cigarettes each. Therefore, the Bitcoin price was converted to US Dollar values, and divided by 10, to get the cost of a single pack for each brand. . cigarettes = cigs_df.groupby(&#39;brand&#39;)[&#39;unit_cost&#39;].median().reset_index() .sort_values(&#39;unit_cost&#39;, ascending=False).set_index(&#39;brand&#39;) plt.style.use(&quot;dark_background&quot;) fig, ax = plt.subplots(figsize=(15,10)) ax.bar(cigarettes.index, cigarettes[&#39;unit_cost&#39;], color = &#39;#CC3311&#39;) ax.set_xticklabels(cigarettes.index, rotation=80, size = 20) ax.set_ylabel(&quot;Median Cost per Pack (USD)&quot;, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) plt.show() . As can be seen in the plot above, the brands are sorted by cost. The prices are median costs, per pack, in 2021 $USD. Surprisingly, they don&#39;t differ significantly from prices in states with low to moderate taxes. However, they are much smaller than those in states like New York, which have costs of around $15 per pack, easily. . Conclusion . As an initial look into the illegal dealings on the Dark Web, this project is appropriate. It allows for a preliminary sense of how products are listed, what the scope of the markets is, how data is organized and what possible insights can be gleaned from it. Furthermore, it illuminates all the issues present in trying to make sense of it all. The prices are listed in Bitcoin and held online for unknown periods of time, even as Bitcoin fluctuates. Knowing the possible range for a year helps in narrowing down the expected cost values. However, error bars need to be created to show the amount of uncertainty in this approach. . With more computing power and detailed string parsing scripts, I would be willing to take a crack at the 1.6TB data set. Price differences between legal and illegal products can illuminate countless premiums customers are willing to pay on various products, which is a worthwhile endeavor for any economist to pursue. . For the data and other notebooks, see github.com/antoniojurlina/learning_python. .",
            "url": "https://antoniojurlina.github.io/portfolio/projects/python/2021/05/07/Dark_Web.html",
            "relUrl": "/projects/python/2021/05/07/Dark_Web.html",
            "date": " • May 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Economic Freedom and Growth",
            "content": ". Summary . In an attempt to venture solo, while an undergraduate student in economics, I worked on a probit model designed to study the effect traffic congestion has on satisfaction with work and place of residence. The question seemed straightforward, and the dependent variables were already either presented as binary or Likert scales that could be easily turned into one. Data came from the city of Cardiff, UK, in the form of an online questionnaire. I find that an increase in the perceived level of congestion makes individuals less happy with working and living in Cardiff. After adjusting for all the potential issues (like hetersokedasticity) this data might have, the results seem quite robust across two probit models. Included, also, is a link to the GitHub repository containing the data I worked with and the Stata do-file. . 1. Review . Adam Smith1, with the publication of the Wealth of the Nations, instigated a debate around the causes of economic growth. Although mercantilism had dominated the period of late Renaissance in Europe and powerful merchants had built routes based on the belief that trade would benefit them greatly2, it wasn’t until Smith’s work had been published that attention turned towards free trade, production advantage, economies of scale and institutions (or lack thereof) intended to orchestrate this in unison. This work was further expanded by Ricardo3, who ushered the realization that benefits can be acquired even by those countries that are not the most efficient suppliers around. Eventually, economic growth was taken up by the likes of Solow4, who had expanded the Harrod-Domar model5 6 so that growth is represented as function of capital, labor and technology (with more optimistic limitations). . Following Solow’s work, Kuznets7 argued that, while necessary, technology itself wouldn’t suffice in producing measurable growth. He claimed that growth would induce change along the way (something along the lines of Schumpeter8), and all the conflict would have to be resolved cost-effectively through institutions designed to do so. Only then, with conflict resolution costs smaller than benefits of growth, would long-term economic progress occur. Finally, there is Milton Friedman9 , crafting institutional approach along more libertarian lines, arguing for a government that is there to promote safety, monopolize violence and influence the economy through the money supply. He also argued for the removal of major trade barriers as the only way to introduce stable equilibria. . More recent work had found that property rights, monetary stability, and freedom to trade internationally all have visible impact on growth 10 11 12 13 14. Additionally, previously underdeveloped, closed-off, and/or countries with centralized economies had all undergone drastic economic changes (in the positive direction) upon loosening institutional grips, opening towards the world and openly stifling hierarchical corruption. This can be observed in the economies of Taiwan, Singapore and Hong Kong, with China following closely upon realizing its neighbors had adopted a slightly more laissez-faire approach and experienced significant growth15. . Market equilibria inside closed economies adjust themselves according to supply and demand interactions and institutional involvement. With constraints effectively placed to incorporate the costs of most inefficiencies, effective resource allocation is determined on aggregate, through interactions of all the individual participants. With current levels of globalization and economic interconnectedness, eliminating quotas and barriers results in a world- wide market place with freer price points. This, much like on a single-country level, is an amalgamation of countless interactions producing an inherent equilibrium. Depending on the institutional restrictions imposed by all the individual players (and their size), this equilibrium will inch towards comparative efficiency, fostering more growth. With the readjustment of the production possibility frontiers to accommodate the new demand and supply pressures, Mundell16 and Fleming17 identify certain factors affecting GDP levels of open-economies: fiscal policy, monetary policy, and foreign trade shifts. Even if Leonteif’s18 observations (the failure of H-O theorem) hold across countries, there is still an adjustment shift according to the world market. . Finally, Gwartney, Lawson, &amp; Block19, have created an index consisting of all these factors influencing growth. The index rates the economic freedom of countries on a scale of 1 to 10, with 10 indicating a country that is completely free economically. The Economic Freedom Index (from here on referred to as EFI), is comprised of separate indices for the size of the government, legal system and property rights, freedom to trade internationally, stability of the monetary policy, and the number of regulatory obstacles. This index, and similar ones (such as the one produced by the Heritage Foundation) have been used in several ways in order to determine a possible causal link between economic freedom and economic growth20 21 22 23 24. . 2. Econometric Model . The EFI is published yearly by the Fraser Institute. Latest edition 25 is comprised of five areas used to construct a scale of economic freedom, with each area rated on a scale of 1 to 10. Size of Government focuses on individual choice-making through market interactions, as opposed to relying on policy making. Countries with low levels of government spending, a smaller government enterprise sector, and lower tax rates earn the highest ratings in this area. Legal System and Property Rights focuses on unbiased judiciary systems, effective protection of private property and impartial enforcement of the law. Countries that satisfy these categories the best, score the highest in this area. Sound Money refers to money with a stable purchasing power over time. Countries that score high in this area, must follow policies and adopt institutions that lead to low rates of inflation and avoid regulations that limit the ability to use alternative currencies. Freedom to Trade Internationally focuses on the level and ease of interactions across the borders. To score high in this area, a country must have “low tariffs, easy clearance and efficient administration of customs, a freely convertible currency, and few controls on the movement of physical and human capital”25. Regulation measures the access into markets and restrictions around economic interactions. To score high in this area, countries need to relax regulatory constraints around labor, product and credit markets. For more detail on the construction of each of these areas, see Appendix 1. . The model under consideration uses these areas as explanatory variables. This should help in determining the causal link, or at least, the sign of the relationship, between economic growth and factors determining classical assumptions around economic freedom. The model is as follows . GROWTHi,(t−5,t)=β0+β1GOVi,t−5+β2LEGALi,t−5+β3MONEYi,t−5+β4TRADEi,t−5+β5REGULATIONi,t−5+β6log(GDP)i,t−5+εi,tGROWTH_{i,(t-5,t)} = beta_0 + beta_1GOV_{i,t-5} + beta_2LEGAL_{i,t-5} + beta_3MONEY_{i,t-5} + beta_4TRADE_{i,t-5} + beta_5REGULATION_{i,t-5} + beta_6log(GDP)_{i,t-5} + varepsilon_{i,t}GROWTHi,(t−5,t)​=β0​+β1​GOVi,t−5​+β2​LEGALi,t−5​+β3​MONEYi,t−5​+β4​TRADEi,t−5​+β5​REGULATIONi,t−5​+β6​log(GDP)i,t−5​+εi,t​ . where the dependent variable represents growth of log GDP per capita over a 5-year period, β1 beta_1β1​ through β5 beta_5β5​ represent the five areas of EFI at the beginning of each 5-year period, β6 beta_6β6​ represents the log of GDP per capita at the beginning of each 5-year period and ε varepsilonε represents the error term. Each of these variables is defined across countries (i) and time (t, t-5), making this a fixed effects model. Country-level heterogeneity carries a lot of unobservable variables, so with fixed effects, the remaining variation can be used to causally identify the relationships of interest. The null hypotheses are that there is no significant effect between the five areas of EFI and growth of GDP per capita. The alternative hypotheses are that the higher a country scores in all five areas of EFI, the higher the log growth of GDP per capita, in accordance with classical assumptions. . 3. Data . The data set used for this project was created using three different sources. The EFI was obtained from the Fraser Institute25, data on GDP per capita was obtained from the World Bank Group26, recorded in 2018 US dollars. GDP was chosen to be per capita specifically to avoid any issues with population size differences among countries. Finally, a dummy variable on whether nations are members of the OECD was created using the list of member nations from the OECD website27. These variables are organized as panel data with 52 countries, ranging from 1970 to 2015. Countries were selected based on data availability, to avoid missing values that would result in omitted observations during estimation. Summary statistics (minimum, maximum, mean, and standard deviation) for all variables can be seen in Appendix 2 and graphs representing these variables, faceted by country, can be seen in Appendix 3. . . Figure 1 - Economic Freedom of the World 2015 Report (The Fraser Institute) . . 4. Preliminary Estimates . The first step of the analysis revolved around estimating an OLS version of the model in Equation 1. Results were estimated in three different ways – by grouping all countries together, by using only OECD member countries and by using only non-member countries. This dummy variable was introduced due to the special economic relationships fostered by OECD member nations, to stabilize unobservable heterogeneity between members and non-members. The use of the dummy variable was also inspired by previous research, especially that of Mankiw, Romer and Weil28. Detailed results are presented in Figure 2. . . Figure 2 - Panel Least Squares Regressions Dependent variable: log growth of GDP per capita (1975 - 2015) . Pooled non-OECD OECD . Intercept | 0.441 (0.034) | 0.372 (0.054) | 0.526 (0.046) | . log GDP per capita | -0.071*** (0.005) | -0.072*** (0.009) | -0.072*** (0.007) | . Size of Government | 0.005 (0.004) | 0.006 (0.006) | 0.002 (0.005) | . Legal System &amp; Property Rights | -0.003 (0.003) | -0.005 (0.005) | -0.002 (0.004) | . Sound Money | -0.004 (0.002) | -0.006** (0.003) | -0.000 (0.004) | . Regulation | 0.024*** (0.005) | 0.024*** (0.008) | 0.025*** (0.007) | . Freedom to Trade Internationally | 0.013*** (0.003) | 0.017*** (0.004) | 0.008*** (0.004) | . Observations | 430 | 189 | 241 | . Cross-sections (periods) | 52 (9) | 24 (9) | 28 (9) | . R2R^2R2 | 0.44 | 0.41 | 0.46 | . F-statistic (p-value) | 5.03 (0.000) | 3.88 (0.000) | 5.4 (0.000) | . Durbin-Watson statistic | 2.67 | 2.41 | 2.92 | . * - 90 % significance / ** - 95 % significance / *** - 99 % significance . . As Figure 2 shows, Freedom to Trade Internationally and Regulation have a significant effect across three model runs, and Sound Money has a significant effect for non-OECD countries only. Any one-point increase in the Freedom to Trade Internationally index is correlated with a 1.3 % (pooled), 1.7 % (non-OECD) and 0.8% (OECD) increase in the growth rate of GDP per capita, on average. Any one-point increase in the Regulation index is correlated with a 2.4 % (pooled), 2.4 % (non-OECD) and 2.5% (OECD) increase in the growth rate of GDP per capita, on average. Finally, any one-point increase in the Sound Money index for non-OECD countries is correlated with a 0.6 % decrease in the growth rate of GDP per capita, on average. To reiterate, an increase in index score across all three of the variables mentioned indicates an increase in economic freedom, as per EFI design. . 5. Testing . Although the results seem significant at first glance, there are many causes for concern regarding the validity of parameter identification in a simple OLS approach to this data set. This section is dedicated to discovering possible violations of Gauss-Markovian assumptions and checking the validity of model design. . Multicollinearity . One of the primary issues with deconstructing an index is the causal relationships between some of the subcomponents. It seems reasonable to assume that size of the government, amount of regulation and property rights are correlated with one another. This can result in multicollinearity among explanatory variables, affecting the robustness of the estimates. The correlation matrix in Figure 3 indicates strong correlation (over 0.5) between the five areas of the EFI. This serves as a rough estimate of multicollinearity present in the model, indicating that estimates need to be interpreted conservatively. For future reference, multicollinearity should be further confirmed by estimating the model and changing the data slightly, many times over, seeing how the estimates react. Also, dependent variables should be dropped, and model estimated without some, to see the effect on estimates. Significant estimate changes in both these approaches would indicate a presence of multicollinearity. Finally, a variance inflation factor should be calculated, as it gives an exact numeric value for evaluation. Since multicollinearity doesn’t change the BLUE properties of the model, and due to time limitations, the model will be left as is. . . Figure 3 - Correlation Matrix . log growth of GDP per capita GDP per capita Size of Government Legal System &amp; Property Rights Sound Money Regulation Freedom to Trade Internationally . log growth of GDP per capita | 1.000 | | | | | | | . GDP per capita | -0.089 | 1.000 | | | | | | . Size of Government | 0.042 | -0.162 | 1.000 | | | | | . Legal System &amp; Property Rights | -0.007 | 0.686 | -0.185 | 1.000 | | | | . Sound Money | -0.029 | 0.533 | 0.043 | 0.583 | 1.000 | | | . Regulation | -0.035 | 0.590 | 0.239 | 0.626 | 0.649 | 1.000 | | . Freedom to Trade Internationally | 0.030 | 0.495 | 0.079 | 0.722 | 0.644 | 0.668 | 1.000 | . Autocorrelation . Since the primary statistical software used in estimation doesn’t allow for direct autocorrelation testing, several indirect ways shall be explored, to detect any potential autocorrelation. First, simply observing graphs of residuals from the three OLS approaches, can be very indicative of any potential autocorrelation. Indeed, by looking at the attached graphs (see Appendix 4), it seems highly possible that autocorrelation is present. The order is harder to discern. Furthermore, Durbin-Watson statistic can be used for identification of first-order autocorrelation. The autocorrelation coefficient, ρ rhoρ, (with stable errors) is located on the interval −1 &lt; ρ rhoρ &lt; 1, and the Durbin-Watson test statistic is approximately equal to 4, 2 and 0, for the ρ rhoρ values of -1, 0, and 1, respectively. Therefore, the d-statistic serves as a rough guide of first order autocorrelation. In the first (pooled) OLS estimate, d is 2.7. Being further from 2 (in the positive direction), indicates that autocorrelation is more likely. The same can be said for the d values of the second (non- OECD) and third (OECD) OLS estimates, for which d values are 2.4 and 2.9, respectively (see Figure 2). Finally, a feasible GLS model is estimated, with the addition of autocorrelation parameters. These are added individually, starting with a parameter for first order autocorrelation, up to the point where their p-values start to become insignificant at conventional confidence levels (see Figure 4). The results indicate a presence of first and second order autocorrelation, with any subsequent level added failing to pass as significant or reducing the number of observations past the optimal point. . Heteroskedasticity . After scouring EViews help pages and related forums and blogposts, I have concluded that the current version of the statistical software just doesn’t provide support when it comes to testing for heteroskedasticity in panel data through direct tests. With that in mind, there were two options left for attempting to detect possible heteroskedasticity in the data. First approach is a common-sense (backed by econometrics textbooks29 30) approach, that assumes there is high probability of heteroskedastic errors occurring in cross-sectional data. This seems intuitively reasonable as well – countries vary greatly in GDP per capita and economic freedom measures, indicating a strong possibility of errors having varying degrees of statistical dispersion. Furthermore, Figure 5 plots residuals against fitted values, for the three OLS estimates (pooled, non-OECD, and OECD). These graphs indicate that errors are indeed not uniformly dispersed and that heteroskedasticity is likely present between cross-sections (i.e. countries). Finally, plots showing within-country fitted values and residuals aren’t feasible since there are only 10 periods under consideration meaning that there aren’t enough points to visually estimate the shape of error dispersion. . Redundant Fixed Effects . Figure 6 shows the results of redundant fixed effects tests, performed on the three OLS models. With future revised estimation in mind, the tests were completed for fixed cross-sectional effects, fixed period effects, and both. In each case, across all the model version, tests confirm that all model specifications are supported, with significant p-values across multiple tests. . Hausman Test . Hausman Test null hypothesis states that there is no correlation between unique errors and regressors (meaning that the random effects model is preferred) against an alternative that there is correlation between unique errors and regressors (meaning that the fixed effects model is preferred). This test could only be performed on cross-sectional fixed effects and not two-way fixed effects since EViews doesn’t estimate two-way random effects tests on unbalanced data for the purposes of further testing. Hausman test results (see Figure 7) reject the null hypothesis and the alternative is accepted – fixed effects model is appropriate. . 6. Revised Estimates . Following all the tests performed, I have decided to reformulate the previous cross-section fixed effects OLS model. For more precise parameter identification and robust standard errors, cross-sectional heteroskedasticity and first (and possibly second) order autocorrelation need to be addressed. Therefore, the revised model is structured as a cross-section fixed effects GLS model, with two terms for autocorrelation and cross-section weights which assume the presence of heteroskedasticity in the relevant dimension. Estimates for this model are presented in Figure 8, split up between estimates for all countries, only non-OECD countries, and only OECD countries. . As Figure 8 shows, Freedom to Trade Internationally and Sound Money have a significant effect across three model runs, and Regulation has a significant effect for non-OECD countries only. Any one-point increase in the Freedom to Trade Internationally index is correlated with a 1.3 % (pooled), 1.7 % (non-OECD) and 1% (OECD) increase in the growth rate of GDP per capita, on average. Any one-point increase in the Sound Money index is correlated with a 0.7 % (pooled), 0.9 % (non-OECD) and 0.6% (OECD) increase in the growth rate of GDP per capita, on average. Any one-point increase in the Regulation index for non-OECD countries is correlated with a 1.3 % increase in the growth rate of GDP per capita, on average. Finally, any one-point increase in the Size of Government index for pooled countries is correlated with a 0.5 % increase in the growth rate of GDP per capita, on average. To reiterate, an increase in index scores across all three of the variables mentioned indicates an increase in economic freedom, as per EFI design. . Moreover, to confirm that the revised model significantly diminishes heteroskedasticity and autocorrelation detected previously, Figure 8 reports the Durbin-Watson statistic and Figure 9 shows the residual plots. The Durbin-Watson (d) statistic went from 2.7 to 1.8 (pooled), 2.4 to 2.1 (non-OECD), and 2.9 to 1.7 (OECD). Since a d value of 2 indicates that ρ rhoρ is 0, this indicates a reduction in residual trend correlation. Additionally, Appendix 5 reports the trend of residuals across periods (compared with the OLS estimated ones), indicating a smoothening. Finally, Figure 9 reports the error dispersion (compared with the OLS estimated ones), indicating more uniform dispersion (homoskedasticity). . 7. Conclusion . This project finds a significant relationship between freedom to trade internationally and economic growth. This freedom is reflected in lower tariffs, few regulations on movement of human and physical capital, easily convertible currency and simple customs clearance operations. This finding mirrors that of Gwartney 23 and Torstensson 14, and contradicts the findings of Ayal and Karras10, who find a negative relationship between freedom to trade and economic growth. This finding holds for all countries pooled together, only non-OECD countries, as well as OECD member nations. Furthermore, this research finds that pursuing low inflation and allowing free access to alternative currency use has a small negative impact on economic growth (between 0.6% and 0.9% for each EFI area unit increase). This result directly contradicts that of Ayal and Karras10 and Barro31. There is also significant evidence that less stringent regulation positively affects economic growth for non-OECD countries only. This reflects the findings of Barro31, Torstensson14 and Knack &amp; Keefer13. Finally, there is significant evidence that smaller governments, that intervene economically less often, are positively correlated with economic growth. This has only been observed for the pooled data set and the effect was only 0.05%. This finding is mirrored in a more robust way in other research11 23 13. . Across all model formulations, freedom to trade internationally remained very robust. These results support classical economic theory3 1, as well as more modern assumptions9. Monetary stability through low inflation and the freedom to use alternate currencies seemed likely to be correlated with economic growth. However, research did not support this hypothesis. There are a few reasons that might explain this finding. Subcomponents of this area of the index might be constructed out of elements with opposite effects. Also, most countries in the data set do not wield the economic power of the United States and are economically tied to the fluctuations of more influential currencies. Therefore, they are unable to produce sound monetary policy and often attempt to restrict the power of foreign currencies in the domestic marketplace. If these countries experience increased growth rates, this area of the index does not predict development as assumed. Free movement into markets, as pictured through the regulation variable, is only significant for non-OECD countries, most of which are underdeveloped. This could indicate that lax regulation fosters growth on the way to the status of a first-world country. However, countries that had already reached these levels of development are not experiencing such growth rates and often begin introducing new regulation when they become appropriately placed on the Kuznets curve to do so. This is often regulation that deals with various market inefficiencies (externalities, informational asymmetry, etc.). . Limitations and future research . This data set was somewhat unbalanced (missing periods for a few cross-sections), resulting in the omission of those observations. Along with balanced panel data, the set needs to extend over a longer time frame, given that in the process of correcting for autocorrelation, the number of observations got further reduced. It would also be useful to detect breakpoints in the time series, centered around significant economic events (like the Great Recession), and perform the estimation around them. Much like a longer time frame, more countries included in the set would be useful. The issue lies in procuring the necessary data, especially further into the past, given that some countries do not provide any data or provide data that is highly questionable. . Works Cited . Smith, A. (2003). An Inquiry into the Nature and Causes of the Wealth of Nations. Bantam Classics. &#8617; &#8617;2 . | McCusker, J., &amp; Morgan, K. (Eds.). (2001). The Early Modern Atlantic Economy. Cambridge: Cambridge University Press. doi:10.1017/CBO9780511523878 &#8617; . | Ricardo, D. (1817). On the Principles of Political Economy and Taxation. London: Dover Publications. &#8617; &#8617;2 . | Solow, R. M. (1956). A Contribution to the Theory of Economic Growth. The Quarterly Journal of Economics, 70(1), 65. https://doi.org/10.2307/1884513 &#8617; . | Domar, E. D. (1946). Capital Expansion, Rate of Growth, and Employment. Econometrica, 14(2), 137–147. https://doi.org/10.2307/1905364 &#8617; . | Harrod, R. F. (1939). An Essay in Dynamic Theory. The Economic Journal, 49(193), 14–33. https://doi.org/10.2307/2225181 &#8617; . | Kuznets, S. (1973). Modern Economic Growth: Findings and Reflections. The American Economic Review, 63(3), 247–258. &#8617; . | Schumpeter, J. (1942). Capitalism, Socialism and Democracy (Vol. 1). Routledge. Retrieved from https://www.goodreads.com/work/best_book/129884-capitalism-socialism-and-democracy &#8617; . | Friedman, M. (1962). Capitalism and Freedom (1st ed.). University of Chicago Press. Retrieved from https://www.goodreads.com/work/best_book/1534488-capitalism-and-freedom &#8617; &#8617;2 . | Ayal, E. B., &amp; Karras, G. (1998). Components of Economic Freedom and Growth: An Empirical Study. The Journal of Developing Areas, 32(3), 327–338. &#8617; &#8617;2 &#8617;3 . | Barro, R. J. (1991). Economic Growth in a Cross Section of Countries. The Quarterly Journal of Economics, 106(2), 407–443. https://doi.org/10.2307/2937943 &#8617; &#8617;2 . | Easterly, W. (1992). Marginal income tax rates and economic growth in developing countries (No. WPS1050) (p. 1). The World Bank. Retrieved from http://documents.worldbank.org/curated/en/432391468766196026/Marginal-income-tax-rates-and-economic-growth-in-developing-countries &#8617; . | Knack, S., &amp; Keefer, P. (1995). Institutions and Economic Performance: Cross-Country Tests Using Alternative Institutional Measures. Economics &amp; Politics, 7(3), 207–227. https://doi.org/10.1111/j.1468-0343.1995.tb00111.x &#8617; &#8617;2 &#8617;3 . | Torstensson, J. (1994). Property Rights and Economic Growth: An Empirical Study. Kyklos, 47(2), 231–247. &#8617; &#8617;2 &#8617;3 . | Naughton, B. (2007). The Chinese economy: transitions and growth. Cambridge, Mass: MIT Press. &#8617; . | Mundell, R. A. (1963). Capital Mobility and Stabilization Policy under Fixed and Flexible Exchange Rates. The Canadian Journal of Economics and Political Science / Revue Canadienne d’Economique et de Science Politique, 29(4), 475–485. https://doi.org/10.2307/139336 &#8617; . | J. M. Fleming, “Domestic Financial Policies under Fixed and Floating Exchange Rates,” IMF Staff Papers, Vol. 9, 1962, pp. 369-379. doi:10.2307/3866091 &#8617; . | Leontief, W. (1953). Domestic Production and Foreign Trade; The American Capital Position Re-Examined. Proceedings of the American Philosophical Society, 97(4), 332– 349. &#8617; . | Gwartney, J. D., Lawson, R. A., &amp; Block, W. (1996). Economic Freedom of the World:1975-1995 (p. 342). The Fraser Institute. Retrieved from http://bit.ly/2jUkBGR &#8617; . | Berggren, N. (2003). The Benefits of Economic Freedom: A Survey. The Independent Review, 8(2), 193–211. &#8617; . | Carlsson, F., &amp; Lundström, S. (2002). Economic freedom and growth: Decomposing the effects. Public Choice, 112, 335–344. https://doi.org/10.1023/a:1019968525415 &#8617; . | de Haan, J., &amp; Sturm, J.-E. (2000). On the relationship between economic freedom and economic growth. European Journal of Political Economy, 16(2), 215–241. https://doi.org/10.1016/S0176-2680(99)00065-8 &#8617; . | Gwartney, J. D., Lawson, R. A., &amp; Holcombe, R. G. (1999). Economic Freedom and the Environment for Economic Growth. Journal of Institutional and Theoretical Economics, 155(4), 643–663. &#8617; &#8617;2 &#8617;3 . | Nelson, M. A., &amp; Singh, R. D. (1998). Democracy, Economic Freedom, Fiscal Policy, and Growth in LDCs: A Fresh Look. Economic Development and Cultural Change, 46(4), 677– 696. https://doi.org/10.1086/452369 &#8617; . | Economic Freedom of the World. (2015, December 22). Retrieved October 9, 2018, from http://bit.ly/2h5xBVI &#8617; &#8617;2 &#8617;3 . | GDP per capita (current US$) Data. (2018). Retrieved October 9, 2018, from https://data.worldbank.org/indicator/NY.GDP.PCAP.CD &#8617; . | OECD - Members and partners. (2018, July). Retrieved December 12, 2018, from http://www.oecd.org/about/membersandpartners/ &#8617; . | Mankiw, N. G., Romer, D., &amp; Weil, D. N. (1992). A Contribution to the Empirics of Economic Growth. The Quarterly Journal of Economics, 407–437. https://doi.org/10.3386/w3541 &#8617; . | Guajarati, D. N. (1987). Basic Econometrics (4th ed.). &#8617; . | Wooldridge, J. M. (2016). Introductory Econometrics: A Modern Approach (6th ed.). Thomson South-Western. &#8617; . | Barro, R. J. (1996). Democracy and Growth. Journal of Economic Growth, 1(1), 1–27. &#8617; &#8617;2 . |",
            "url": "https://antoniojurlina.github.io/portfolio/2019/12/07/economic-freedom-and-growth.html",
            "relUrl": "/2019/12/07/economic-freedom-and-growth.html",
            "date": " • Dec 7, 2019"
        }
        
    
  
    
        ,"post2": {
            "title": "How does traffic congestion impact life satisfaction?",
            "content": ". Summary . In an attempt to venture solo, while an undergraduate student in economics, I worked on a probit model designed to study the effect traffic congestion has on satisfaction with work and place of residence. The question seemed straightforward, and the dependent variables were already either presented as binary or Likert scales that could be easily turned into one. Data came from the city of Cardiff, UK, in the form of an online questionnaire. I find that an increase in the perceived level of congestion makes individuals less happy with working and living in Cardiff. After adjusting for all the potential issues (like hetersokedasticity) this data might have, the results seem quite robust across two probit models. Included, also, is a link to the GitHub repository containing the data I worked with and the Stata do-file. . Part I . 1. Data . Council authority of Cardiff has, in cooperation with Cardiff Business Partnership, conducted an online survey, yielding 2,094 responses. After data clean-up process, 2,045 observations remained. While most variables were dropped, 17 were deemed significant enough to include (see Table 1). Missing values were filled with unconditional means, except in the case of the variables describing the primary method of transportation and proximity of a train station, where 19 and 32 observations with missing values were dropped, respectively. Variables regarding age, income and employment duration were acquired via drop-down answer menus on the survey, with each answer constituting a specific bracket. For the sake of continuity and interpretability, these answers were encoded as the midpoint value of each bracket. Since each set of brackets concluded with an open ended one (e.g. more than 10, 65+, etc.), and no middle point could be determined, each of those categories was encoded as the sum of the size of the largest bracket (for the relevant variable) and the bracket’s lower bound. . . Table 1 - Data Summary . Average Std. Dev. Min Max Explanation . Quality of public transportation | 3.26 | 1.09 | 1 | 5 | 1 is very bad and 5 is very good | . Being able to get from place to place with little traffic (i.e. congestion) | 2.72 | 1.04 | 1 | 5 | 1 is very bad and 5 is very good | . Work proximity | 2.98 | 1.28 | 1 | 5 | 1 you like the most and 5 you dislike | . Travel within the city | 3.86 | 0.96 | 1 | 5 | 1 is not important and 5 is very important | . Ease of getting to work | 3.65 | 1.02 | 1 | 5 | 1 is not important and 5 is very important | . Overall satisfaction with life | 2.95 | 0.98 | 1 | 5 | 1 being highly satisfied and 5 being unsatisfied | . Number of years employed | 9.02 | 5.46 | 0.5 | 15 | How long have you worked in your present employment | . Income | 25,748.18 | 10,318.80 | 5,720 | 55,241 | How much do you get paid, before taxes | . Age | 41.50 | 11.05 | 21 | 74 | How old are you | . Sex | 0.42 | 0.49 | 0 | 1 | 1 for male, 0 for female | . Relationship | 0.69 | 0.46 | 0 | 1 | 1 for in a relationship, 0 for single | . Children | 0.56 | 0.49 | 0 | 1 | 1 for having any number of children, 0 for none | . Overall satisfaction with place of residence | 0.66 | 0.47 | 0 | 1 | 1 for satisfied, 0 for unsatisfied | . Train station proximity | 2.72 | 0.50 | 1 | 3 | Do you have a train station within 2 miles of your residence? 1 - Don’t Know, 2 - No, 3 - Yes | . Education | 2.53 | 1.10 | 1 | 6 | 1 - High School, 2 - Associates degree, 3 – Bachelor’s Degree BA, BSc, 4 – Master’s Degree, 5 - Professional degree, 6 - PhD | . Primary mode of transportation | 1.36 | 0.83 | 0 | 2 | 0 - Hippie, 1 - Public, 2 - Drive | . Job satisfaction | 0.52 | 0.50 | 0 | 1 | 1 for satisfied, 0 for unsatisfied | . . 2. Methodology . Literature review, focusing on measuring different sorts of satisfaction1 2 3, exemplifies various probit models and their appropriate usage. Binary nature of the dependent variables, together with the literature, was the main reason for choosing a probit-model approach. Given that the report attempts to clarify the effect congestion might have on satisfaction with place of residence (R) and working (J) in Cardiff, following two models represent its cornerstone: . J=β0+β1congestion+xδ+ϵ1J = beta_0 + beta_1 congestion + x delta + epsilon_1J=β0​+β1​congestion+xδ+ϵ1​ . R=β0+β1congestion+xδ+ϵ2R = beta_0 + beta_1 congestion + x delta + epsilon_2R=β0​+β1​congestion+xδ+ϵ2​ . Model 2 dependent variable was binary in the form the data was presented. However, Model 1 dependent variable had to be created from a set of Likert scale variables focusing on job satisfaction. Mean value for the set was created across individuals and compared to “3”, which is the neutral option on the answer sheet. Individuals above “3” were categorized as satisfied and the rest were categorized as unsatisfied. This way, a binary variable was generated, appropriate for use in a probit model. Cronbach’s alpha was used to determine the scale reliability coefficient (0.8471), indicating that the set of Likert-scale variables used are closely related as a group and representative of the shared concept. . Furthermore, for both Models 1 and 2, β betaβ and δ deltaδ represent coefficients to be estimated (see Table 1). Control variables were chosen based on two criteria: 1) those causally linked with the dependent variable through congestion and 2) those with a direct, causal link to satisfaction that were deemed as likely sources of severe endogeneity. Explanatory variables of interest consist of demographic characteristics4 5, congestion6 7 8 and those capturing satisfaction spillover9 10. Stochastic terms are noted as ϵ1 epsilon_1ϵ1​ and ϵ2 epsilon_2ϵ2​. Also, since the probit models are being used, robust standard errors are reported and corrected for underlying (inherent) heteroskedasticity. . 3. Hypotheses . . H01H_{01}H01​: Congestion has no significant impact on job satisfaction for residents of Cardiff. . HA1H_{A1}HA1​: Congestion has a negative impact on job satisfaction for residents of Cardiff. . . H02H_{02}H02​: Congestion has no significant impact on satisfaction with place of residence. . HA2H_{A2}HA2​: Congestion has a negative impact on satisfaction with place of residence. . . Models are demonstrating the conditional probability of a specific outcome occurring (Yi=1Y_i = 1Yi​=1 meaning satisfied with job for Model 1 and satisfied with place of residence for Model 2), for 3 which the marginal effects show how a unit change for congestion increases (or decreases) the probability of the given outcome occurring (et ceteris paribus). . . Part II . 1. Satisfaction with working and living in Cardiff . . I am quite confident that an increase in perceived level of congestion makes individuals 4% less likely to be satisfied with working in Cardiff. . . I am quite confident that an increase in perceived level of congestion makes individuals 5% less likely to be satisfied with living in Cardiff. . . Tables 2 and 3 present more detailed results for these claims and state the possibility of a mistake being made. Numbers showing the impact of congestion on satisfaction with working and living in Cardiff are positive. This is a consequence of the way the survey was designed (1 is very bad and 5 is very good) and should be interpreted with an opposite sign. Literature review supports this finding. High levels of traffic congestion are associated with mental and physical stress11 12, which are further aggravated with the inability to complete daily routines 13. Furthermore, long work commutes cause residual stress in the workplace 7 14 15. Kahneman et al.8 found that work commutes were most frequently associated with negative feelings, out of all daily habits. . . Table 2 - Model 1 marginal effects . Marginal effect Standard error . Quality of public transportation | | 0.049*** | 0.011 | . Being able to get from place to place with little traffic (i.e. congestion) | | 0.041*** | 0.011 | . Work proximity | | 0.023*** | 0.008 | . Travel within the city | | 0.007 | 0.014 | . Ease of getting to work | | 0.015 | 0.013 | . Overall satisfaction with life | | -0.027** | 0.011 | . Number of years employed | | -0.001 | 0.002 | . Income | | 0.000*** | 0.000 | . Age | | -0.008 | 0.007 | . Sex | | -0.072*** | 0.022 | . Relationship | | -0.015 | 0.025 | . Children | | 0.039 | 0.026 | . Overall satisfaction with place of residence | | 0.094*** | 0.023 | . Train station proximity | Yes No | -0.204*** -0.219*** | 0.058 0.027 | . Education | | 0.024** | 0.010 | . Primary mode of transportation | Public Drive | 0.010 0.076*** | 0.033 0.027 | . * - 90 % significance / ** - 95 % significance / *** - 99 % significance . . Public transportation plays a major role in mitigating the effects of congestion on satisfaction16. However, while it is the most accepted solution to congestion17, it only works if it is appropriately scaled with the needs of the public18. In Cardiff, an increase in perceived quality of public transportation makes individuals 4% (on average) more likely to feel satisfied with their work and 6% (on average) more likely to feel satisfied with where they live, holding everything else the same. Moreover, utilizing public transportation systems decreases the likelihood of being satisfied with place of residence by 6% (on average), holding everything else the same. Those most likely to utilize public transportation live inside the urban area while those living in the suburbs are more likely to drive to work. Therefore, this decrease in likelihood of being satisfied with the place of residence, based on public transport utilization, indicates a preference for living outside of the city. Furthermore, driving decreases the likelihood of being satisfied with working in Cardiff by 8% (on average), holding everything else the same (see Tables 2 and 3 for more detailed results and the likelihood of mistakes being reported). Finally, living close to work increases the likelihood of being satisfied with working in Cardiff by 2% (on average), holding everything else the same. . . Table 3 - Model 2 marginal effects . Marginal effect Standard error . Quality of public transportation | | 0.062*** | 0.010 | . Being able to get from place to place with little traffic (i.e. congestion) | | 0.052*** | 0.010 | . Work proximity | | 0.003 | 0.008 | . Travel within the city | | 0.016 | 0.013 | . Ease of getting to work | | -0.021* | 0.012 | . Overall satisfaction with workplace | | 0.086*** | 0.020 | . Number of years employed | | 0.002 | 0.002 | . Income | | 0.000*** | 0.000 | . Age | | -0.006 | 0.006 | . Sex | | -0.035 | 0.020 | . Relationship | | 0.106*** | 0.023 | . Children | | 0.034 | 0.024 | . Overall satisfaction with life | | 0.002 | 0.010 | . Train station proximity | Yes No | 0.212*** 0.121 | 0.075 0.077 | . Education | | 0.019*** | 0.009 | . Primary mode of transportation | Public Drive | -0.055* -0.030 | 0.032 0.026 | . * - 90 % significance / ** - 95 % significance / *** - 99 % significance . . While there are negative effects of congestion on satisfaction with working and living in Cardiff, those effects are smaller than anticipated. This could be because working during a period of recession or post-recession19 recovery causes enough satisfaction that lessens the burdens of daily commute13. Additionally, as noted by Stokols et al. 11, congestion is only relevant when it significantly differs from the expected traffic levels. If people are exposed to similar congestion levels every day, negative effects are diminished. . 2. Policy relevance . Those that live within 2 miles of a train station and those that live further away, are about 20% (on average) less likely to feel satisfied with working in Cardiff, holding everything else the same. Moreover, those that live within 2 miles of a train station are about 20% (on average) more likely to be satisfied with their place of residence, holding everything else the same (see Tables 2 and 3). Since most of survey respondents (74%) live within 2 miles of a train station and more than half (59%) drive to work (not necessarily overlapping groups), it could be assumed that there is a negative perception of public transportation and a strong tendency to avoid it. This, in addition to reported congestion and public transportation effects, indicates that policy under consideration by the council authority of Cardiff needs to address both public transportation and congestion. Building a new metro system as well as introducing a congestion charge for those driving into the center of Cardiff should be packaged together. Literature review supports this claim, as well20. . 3. Limitations . Variability of answers provided was reduced when most missing values were filled with average values for the category. Further adding to this issue was the design of the survey itself. Answers that were selected from a drop-down menu were presented as brackets, chosen by the surveyor. This eliminated most of the effect that individuals with answers far away from the average would have. Moreover, for privacy reasons, answers stating the specific place of residence were excluded, introducing additional lack of clarity. . Due to model choice, differences between observed and predicted and expected and predicted values for variables of interest were not uniform across the data. This means that I was more likely to falsely perceive an observation as insignificant and reject its validity and explanatory power. Finally, I wish to point out the possibility that any variable not considered stands a chance of being correlated with variables I did consider (e.g. place of residence might be linked with public transport utilization, commute satisfaction and satisfaction with living and working in Cardiff). By omitting any such variable, I introduced the likelihood of overstating the effects considered variables have on overall satisfaction. . 4. GitHub repository . For data, code, and similar projects, visit https://github.com/antoniojurlina/econometrics. . . Stata code . cls clear //clear previous data use CBP_survey.dta //choose data // following commands represent my data clean-up process //////////////////////////////////////////////////////////////////// encode Q3, generate(yrs_employed) replace yrs_employed = 0.5 if Q3 == &quot;Less than 1 year&quot; replace yrs_employed = 2 if Q3 == &quot;1 to 3 years&quot; replace yrs_employed = 4 if Q3 == &quot;3 to 5 years&quot; replace yrs_employed = 7.5 if Q3 == &quot;5 to 10 years&quot; replace yrs_employed = 15 if Q3 == &quot;More than 10 years&quot; encode Q4, generate(income) replace income = 5720 if Q4 == &quot;£0 - £11,440 per year&quot; replace income = 12480.5 if Q4 == &quot;£11,441 - £13,520 per year&quot; replace income = 14820.5 if Q4 == &quot;£13,521 - £16,120 per year&quot; replace income = 17420.5 if Q4 == &quot;£16,121 - £18,720 per year&quot; replace income = 20540.5 if Q4 == &quot;£18,721 - £22,360 per year&quot; replace income = 25220.5 if Q4 == &quot;£22,361 - £28,080 per year&quot; replace income = 31720.5 if Q4 == &quot;£28,081 - £35,360 per year&quot; replace income = 40300.5 if Q4 == &quot;£35,361 - £45,240 per year&quot; replace income = 55241 if Q4 == &quot;£45,241 or more per year&quot; drop if Q5 == &quot;25- 40&quot; | Q5 == &quot;25- 41&quot; | Q5 == &quot;25- 42&quot; | Q5 == &quot;25- 43&quot; drop if Q5 == &quot;25- 44&quot; | Q5 == &quot;25- 45&quot; | Q5 == &quot;25- 46&quot; | Q5 == &quot;25- 47&quot; | Q5 == &quot;25- 48&quot; | Q5 == &quot;25- 49&quot; encode Q5, generate(age) replace age = 21 if Q5 == &quot;18- 24&quot; replace age = 32 if Q5 == &quot;25- 39&quot; replace age = 47 if Q5 == &quot;40- 54&quot; replace age = 59.5 if Q5 == &quot;55- 64&quot; replace age = 74 if Q5 == &quot;65&quot; generate age_sq = age * age label define sex 1 &quot;Male&quot; 0 &quot;Female&quot; encode Q6, generate(sex) generate relationshipy = Q7 replace relationshipy = &quot;Yes&quot; if Q7 == &quot;Co-habiting&quot; replace relationshipy = &quot;Yes&quot; if Q7 == &quot;Married&quot; replace relationshipy = &quot;No&quot; if Q7 == &quot;Single&quot; label define relationship 1 &quot;Yes&quot; 0 &quot;No&quot; encode relationshipy, generate(relationship) drop relationshipy Q7 label define children 1 &quot;Yes&quot; 0 &quot;No&quot; encode Q8, generate(children) rename Q14h pub_trans_quality rename Q14i congestion generate satisfactiony = Q15 label define satisfaction 1 &quot;Satisfied&quot; 0 &quot;Unsatisfied&quot; encode satisfactiony, generate(satisfaction) drop satisfactiony Q15 encode Q19, generate(train) label define education 1 &quot;High School&quot; 2 &quot;Associates degree&quot; 3 &quot;Bachelors Degree BA, BSc&quot; 4 &quot;Masters Degree&quot; 5 &quot;Professional degree&quot; 6 &quot;PhD&quot; encode Q20, generate(education) rename Q30c travel_importance rename Q30f work_travel_ease rename Q31 life_satisfaction summarize pub_trans_quality replace pub_trans_quality = r(mean) if pub_trans_quality == . summarize congestion replace congestion = r(mean) if congestion == . summarize yrs_employed replace yrs_employed = r(mean) if yrs_employed == . summarize income replace income = r(mean) if income == . summarize age replace age = r(mean) if age == . summarize sex replace sex = r(mean) if sex == . summarize relationship replace relationship = r(mean) if relationship == . summarize satisfaction replace satisfaction = r(mean) if satisfaction == . summarize train drop if train == . summarize education replace education = r(mean) if education == . summarize life_satisfaction replace life_satisfaction = r(mean) if life_satisfaction == . summarize children replace children = r(mean) if children == . summarize work_travel_ease replace work_travel_ease = r(mean) if work_travel_ease == . summarize travel_importance replace travel_importance = r(mean) if travel_importance == . generate transport1 = Q16 replace transport1 = &quot;Hippie&quot; if Q16 == &quot;Walk&quot; | Q16 == &quot;Cycle&quot; replace transport1 = &quot;Public&quot; if Q16 == &quot;Train&quot; | Q16 == &quot;Bus&quot; replace transport1 = &quot;Drive&quot; if Q16 == &quot;Car /Motorcycle&quot; label define trans1 0 &quot;Hippie&quot; 1 &quot;Public&quot; 2 &quot;Drive&quot; encode transport1, generate(trans1) drop if trans1 == . summarize Q28a replace Q28a = r(mean) if Q28a == . summarize Q28b replace Q28b = r(mean) if Q28b == . summarize Q28c replace Q28c = r(mean) if Q28c == . summarize Q28d replace Q28d = r(mean) if Q28d == . summarize Q28e replace Q28e = r(mean) if Q28e == . summarize Q28f replace Q28f = r(mean) if Q28f == . summarize Q28g replace Q28g = r(mean) if Q28g == . summarize Q29a replace Q29a = r(mean) if Q29a == . summarize Q29b replace Q29b = r(mean) if Q29b == . summarize Q29c replace Q29c = r(mean) if Q29c == . summarize Q29d replace Q29d = r(mean) if Q29d == . summarize Q29e replace Q29e = r(mean) if Q29e == . summarize Q29f replace Q29f = r(mean) if Q29f == . summarize Q29g replace Q29g = r(mean) if Q29g == . summarize Q29h replace Q29h = r(mean) if Q29h == . rename Q29c work_proximity generate job = (Q28a + Q28b + Q28c + Q28d + Q28e + Q28f + Q28g)/7 generate job_satisfaction = job summarize job replace job_satisfaction = 1 if job &gt; 3 replace job_satisfaction = 0 if job &lt;= 3 alpha Q28a-Q28g drop Q3 Q4 Q5 Q6 Q8 Q19 Q20 drop Q1 Q2 Q11 Q13 Q12 Q14a Q14b Q14c Q14d Q14e Q14f Q14g Q14j Q14k Q14l Q14m Q14n Q14o drop Q17 Q21 Q22 Q23a Q23b Q24 Q25 Q26 Q27 Q28a Q28b Q28c Q28d Q28e Q28f Q28g drop Q29a Q29b Q29d Q29e Q29f Q29g Q29h Q30a Q30b Q30d Q30e Q30g Q30h Q30i Q30j Q30k drop Q32 Q33 drop job Q16 Q18 transport1 //////////////////////////////////////////////////////////////////// // end of data clean-up process summarize probit job_satisfaction satisfaction pub_trans_quality children congestion life_satisfaction yrs_employed travel_importance work_travel_ease work_proximity income age age_sq sex relationship i.train education i.trans1,robust margins, dydx(*) probit satisfaction job_satisfaction pub_trans_quality children congestion life_satisfaction yrs_employed travel_importance work_travel_ease work_proximity income age age_sq sex relationship i.train education i.trans1,robust margins, dydx(*) save CBP_survey_clean.dta, replace . . Works Cited . Blanchflower, D. G., &amp; Oswald, A. J. (2004). Well-being over time in Britain and the USA. Journal of Public Economics, 88, 1359-1386. doi:10.1016/S0047-2727(02)00168-8 &#8617; . | Fetai, B., Abduli, S., &amp; Qirici, S. (2015). An Ordered Probit Model of Job Satisfaction in the Former Yugoslav Republic of Macedonia. Procedia Economics and Finance, 33, 350- 357. doi:10.1016/S2212-5671(15)01719-0 &#8617; . | Rayton, B. A. (2006). Examining the interconnection of job satisfaction and organizational commitment: An application of the bivariate probit model. Int. J. of Human Resource Management, 17(1), 139-154. doi:10.1080/09585190500366649 &#8617; . | Richey, S. (2012). Determinants of Community Satisfaction and its Relative Importance for Life Satisfaction. &#8617; . | Auh, S., &amp; Cook, C. (2009). Quality of Community Life Among Rural Residents: An Integrated Model. Social Indicators Research, 94(3), 377-389. &#8617; . | Lipsetz, D. A. (2000). Residential Satisfaction: Identifying the differences between suburban-ites and urbanites. Columbus: Ohio State University. &#8617; . | Novaco, R. W., Stokols, D., &amp; Milanesi, L. (1990). Objective and subjective dimensions of travel impedance as determinants of commuting stress. American Journal of Community Psychology, 18, 231–257. &#8617; &#8617;2 . | Kahneman, D., Krueger, A. B., Schkade, D., Schwarz, N., &amp; Stone, A. (2004). A survey method for characterizing daily life experience: The day reconstruction method (DRM). Science, 306, 1776–1780. &#8617; &#8617;2 . | Shimon L. Dolan &amp; Eric Gosselin, 2000. “Job satisfaction and life satisfaction: Analysis of a reciprocal model with social demographic moderators,” Economics Working Papers 484, Department of Economics and Business, Universitat Pompeu Fabra. &#8617; . | Ilies, R., Wilson, K. S., &amp; Wagner, D. T. (2009). The Spillover of Daily Job Satisfaction Onto Employees’ Family Lives: The Facilitating Role of Work-Family Integration. Academy of Management Journal, 52(1), 87-102. doi:10.5465/AMJ.2009.36461938 &#8617; . | Stokols, D., Novaco, R. W., Stokols, J., &amp; Campbell, J. (1978). Traffic Congestion, Type A Behavior, and Stress. Journal of Applied Psychology, 63(4), 467-480. doi:10.1037/0021- 9010.63.4.467 &#8617; &#8617;2 . | Novaco, R. W., &amp; Gonzales, O. I. (2009). Commuting and well-being. In Y. Amichai- Hamburger (Ed.), Technology and well-being (pp. 174–205). New York: Cambridge University Press. &#8617; . | Olsson, L. E., Garling, T., Ettema, D., Friman, M., &amp; Fujii, S. (2013). Happiness and Satisfaction with Work Commute. Soc Indic Res, 111, 255-263. doi:10.1007/s11205-012- 0003-2 &#8617; &#8617;2 . | Glass, D. C., Singer, J., &amp; Pennebaker, J. Behavioral and physiological effects of uncontrollable environmental events. In D. Stokols (Ed.), Perspectives on environment and behavior. New York: Plenum, 1977. &#8617; . | Sherrod, D. R. Crowding, perceived control, and behavioral aftereffects. Journal of Applied Social Psychology, 1974, 4, 171-186. &#8617; . | Kottenhoff, K., &amp; Freij, K. B. (2009). The role of public transport for feasibility and acceptability of congestion charging – The case of Stockholm. Transportation Research Part A, 43, 297-305. doi:10.1016/j.tra.2008.09.004 &#8617; . | Schlag, B., Teubel, U., 1997. Public acceptability of transport pricing. IATSS Research, 21(2). &#8617; . | Transport for London (TfL), 2005. Central London Congestion Charging. Impacts monitoring, Third Annual Report, April 2005 &#8617; . | Gross Domestic Product Preliminary Estimate: Q4 2014 (pp. 1-20, Rep.). (2015). Office for National Statistics. &#8617; . | Jaensirisak, S., Wardman, M., May, A.D., 2005. Explaining variations in public acceptability of road pricing schemes. Journal of Transport Economics and Policy 39, 127–154. &#8617; . |",
            "url": "https://antoniojurlina.github.io/portfolio/2019/04/30/congestion-and-satisfaction.html",
            "relUrl": "/2019/04/30/congestion-and-satisfaction.html",
            "date": " • Apr 30, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Home",
          "content": "antoniojurlina.github.io .",
          "url": "https://antoniojurlina.github.io/portfolio/return/",
          "relUrl": "/return/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://antoniojurlina.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}