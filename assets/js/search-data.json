{
  
    
        "post0": {
            "title": "Agora - A Dark Web Marketplace",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import os . A few years ago, a reddit user called usheep scraped a DarkWeb marketplace called the Agora for a list of vendors, their identities and products, while threatening to expose them all as blackmail. Eventually, this data set made it to the general public, and while usheep disappeared, we have thousands of rows of vendors, items, categories, descriptions and prices in an organized data set that contains drugs, weapons, books, services, and more. There are over 100,000 unique observations spanning year 2014. My goal is to analyze this data set and explore some common items being sold, as well as the usual prices across categories. . Although there is over 1.6TB of uncompressed, scraped data of dark web sales available out there, the goal of this project will be to look at the Kaggle published subset which is around 30MB and contains ~100,000 observations. The data set is available as a direct .csv download with a Kaggle account. Additonally, since all the prices listed in this data set are in Bitcoin, I went to Coindesk and downloaded the daily closing price of Bitcoin in US Dollar amounts, for that whole period. This data set will be used to make the costs more comprehensible by converting them to current USD prices. . dw_data = pd.read_csv(&quot;Agora.csv&quot;, usecols=[&#39;Vendor&#39;, &#39; Category&#39;, &#39; Item&#39;, &#39; Item Description&#39;, &#39; Price&#39;, &#39; Rating&#39;]) .rename(columns={&#39;Vendor&#39;:&#39;vendor&#39;, &#39; Category&#39;:&#39;category&#39;, &#39; Item&#39;:&#39;item&#39;, &#39; Item Description&#39;:&#39;description&#39;, &#39; Price&#39;:&#39;price&#39;, &#39; Rating&#39;:&#39;rating&#39;}) # Importing Bitcoin data and selecting out the relevant column bitcoin = pd.read_csv(&quot;BTC_USD_2014-01-02_2015-01-01-CoinDesk.csv&quot;) inflation = 0.1186 # the inflation rate between 2014 and 2021 bitcoin = bitcoin.iloc[:,2]*(1+inflation) . The Agora data set sample, as it is currently in the memory, is shown below. The variables that need to be cleaned up further are category, price, and rating. . Category: There are categories and subcategories within this column that need to be split into separate columns. | Price: This a string because the word BTC is present in each and should be converted to numeric for further analysis. | Rating: This shows the rating and the possible highest score, which should also be separated out. | . dw_data.head() . vendor category item description price rating . 0 CheapPayTV | Services/Hacking | 12 Month HuluPlus gift Code | 12-Month HuluPlus Codes for $25. They are wort... | 0.05027025666666667 BTC | 4.96/5 | . 1 CheapPayTV | Services/Hacking | Pay TV Sky UK Sky Germany HD TV and much mor... | Hi we offer a World Wide CCcam Service for En... | 0.152419585 BTC | 4.96/5 | . 2 KryptykOG | Services/Hacking | OFFICIAL Account Creator Extreme 4.2 | Tagged Submission Fix Bebo Submission Fix Adju... | 0.007000000000000005 BTC | 4.93/5 | . 3 cyberzen | Services/Hacking | VPN &gt; TOR &gt; SOCK TUTORIAL | How to setup a VPN &gt; TOR &gt; SOCK super safe enc... | 0.019016783532494728 BTC | 4.89/5 | . 4 businessdude | Services/Hacking | Facebook hacking guide | . This guide will teach you how to hack Faceb... | 0.062018073963963936 BTC | 4.88/5 | . From the bitcoin data set, the only relevant variable is column 2, which has been adjusted from 2014 to 2021 USD values and extracted as a single Series. From here, I can obtain the average bitcoin value for 2014 and use it to convert all the listed prices in the Agora data set. This is not a flawless approach as the sale listings do not have specific dates associated with them and Bitcoin varies daily. Therefore, I will also use the lowest and highest Bitcoin values in USD across 2014 to show the possible price ranges. . bitcoin.head() . 0 860.313571 1 899.384815 2 909.776933 3 974.430167 4 1085.770018 Name: Closing Price (USD), dtype: float64 . Cleaning Data . Below is the code that performs the data cleaning discussed above. In addition to all the steps previously outlined, I find price outliers within each category and create a new Boolean column which indicates whether each value is an outlier within its respective category. . In this case, outliers are defined as prices that are above the threshold: . $$price &gt; 1.5 * IQR + 3^{rd} Quartile$$ . where IQR stands for interquartile range. . dw_data = dw_data[dw_data[&#39;price&#39;].str.contains(&#39;BTC&#39;, na=False)] dw_data[&#39;price&#39;] = dw_data[&#39;price&#39;].str.replace(&quot; BTC&quot;, &quot;&quot;) dw_data[&#39;price&#39;] = pd.to_numeric(dw_data[&#39;price&#39;]) # Separating the rating column into rating and total possible score columns dw_data = dw_data[~dw_data[&#39;rating&#39;].str.contains(&#39;[0 deals]&#39;, na=False)] dw_data[&#39;rating&#39;] = dw_data[&#39;rating&#39;].str.replace(&quot;~&quot;, &quot;&quot;) ratings = dw_data[&#39;rating&#39;].str.split(&quot;/&quot;, n=3,expand=True) dw_data[&#39;rating&#39;] = pd.to_numeric(ratings[0]) dw_data[&#39;rating_total&#39;] = pd.to_numeric(ratings[1]) # separating out the categories hierarchy categories = dw_data[&#39;category&#39;].str.split(&quot;/&quot;, n=3,expand=True) categories[0] = categories[0].str.replace(&quot;^Info$&quot;, &quot;Information&quot;) categories_to_use = [&#39;Services&#39;, &#39;Drugs&#39;, &#39;Forgeries&#39;, &#39;Tobacco&#39;, &#39;Counterfeits&#39;, &#39;Data&#39;, &#39;Information&#39;, &#39;Electronics&#39;, &#39;Drug paraphernalia&#39;, &#39;Other&#39;, &#39;Jewelry&#39;, &#39;Weapons&#39;, &#39;Chemicals&#39;] dw_data[&#39;category&#39;] = categories[0] dw_data[&#39;category1&#39;] = categories[1] dw_data[&#39;category2&#39;] = categories[2] dw_data[&#39;category3&#39;] = categories[3] dw_data = dw_data[dw_data[&#39;category&#39;].isin(categories_to_use)].reset_index(drop=True) # converting BTC to 2021 $USD values dw_data[&#39;usd_low&#39;] = np.round(np.min(bitcoin) * dw_data[&#39;price&#39;], 2) dw_data[&#39;usd&#39;] = np.round(np.mean(bitcoin) * dw_data[&#39;price&#39;], 2) dw_data[&#39;usd_high&#39;] = np.round(np.max(bitcoin) * dw_data[&#39;price&#39;], 2) # adding a column that signifies which value is an outlier within top level category def limit(column): iqr = column.quantile(0.75) - column.quantile(0.25) top = column.quantile(0.75) + 1.5*iqr return top limits = dw_data.groupby(&#39;category&#39;)[&#39;usd&#39;].agg(limit) .reset_index().rename(columns={&#39;usd&#39;:&#39;limit&#39;}) dw_data = dw_data.merge(limits, on=&#39;category&#39;, how=&#39;left&#39;) dw_data[&#39;outlier&#39;] = dw_data[&#39;usd&#39;] &gt; dw_data[&#39;limit&#39;] dw_data = dw_data.drop(&#39;limit&#39;, axis=1) # deleting all intermediate data frames del [categories, categories_to_use, ratings, limits] . The resulting data frame is presented below. . print(dw_data.shape) dw_data.head() . (73314, 14) . vendor category item description price rating rating_total category1 category2 category3 usd_low usd usd_high outlier . 0 CheapPayTV | Services | 12 Month HuluPlus gift Code | 12-Month HuluPlus Codes for $25. They are wort... | 0.050270 | 4.96 | 5.0 | Hacking | None | None | 17.23 | 29.60 | 54.58 | False | . 1 CheapPayTV | Services | Pay TV Sky UK Sky Germany HD TV and much mor... | Hi we offer a World Wide CCcam Service for En... | 0.152420 | 4.96 | 5.0 | Hacking | None | None | 52.25 | 89.74 | 165.49 | False | . 2 KryptykOG | Services | OFFICIAL Account Creator Extreme 4.2 | Tagged Submission Fix Bebo Submission Fix Adju... | 0.007000 | 4.93 | 5.0 | Hacking | None | None | 2.40 | 4.12 | 7.60 | False | . 3 cyberzen | Services | VPN &gt; TOR &gt; SOCK TUTORIAL | How to setup a VPN &gt; TOR &gt; SOCK super safe enc... | 0.019017 | 4.89 | 5.0 | Hacking | None | None | 6.52 | 11.20 | 20.65 | False | . 4 businessdude | Services | Facebook hacking guide | . This guide will teach you how to hack Faceb... | 0.062018 | 4.88 | 5.0 | Hacking | None | None | 21.26 | 36.51 | 67.34 | False | . Market Size . Here, I present all the distinct categories that span the hierarchy of all listings on the Agora. For each distinct category, the size represents the total number of items being sold while the two values listed represent the total value of the entire marketplace (category) in 2021 US Dollars. In each graph, there is a top and a bottom Value plot. The top one is simply all the values added up while the bottom one is all the values added up but with the outliers removed. . According to the number of listings and to the entire value of all items listed combined, the marketplace for Drugs on the Agora is the largest one, by far. With over 60,000 listings, and a combined value of over $900 million, it dwarves other categories. . markets = dw_data.groupby(&#39;category&#39;)[&#39;usd&#39;].agg([sum, np.size]).reset_index() .merge(dw_data[~dw_data[&#39;outlier&#39;]] .groupby(&#39;category&#39;)[&#39;usd&#39;] .sum().reset_index(), on=&#39;category&#39;, how=&#39;left&#39;) .rename(columns={&#39;usd&#39;:&#39;sum_no&#39;}) .sort_values(&#39;size&#39;, ascending=False).set_index(&#39;category&#39;) plt.style.use(&#39;dark_background&#39;) fig, ax = plt.subplots(3,1, sharex=True, figsize=(15,10)) ax[0].bar(markets.index, markets[&#39;size&#39;], color = &quot;#BB5566&quot;) ax[0].set_xticklabels(markets.index, rotation=70, size = 15) ax[0].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[0].set_ylabel(&quot;Size&quot;, size = 15) ax[1].bar(markets.index, markets[&#39;sum&#39;], color = &quot;#004488&quot;) ax[1].set_xticklabels(markets.index, rotation=70, size = 15) ax[1].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[1].set_ylabel(&quot;Value (USD)&quot;, size = 15) ax[2].bar(markets.index, markets[&#39;sum_no&#39;], color = &quot;#DDAA33&quot;) ax[2].set_xticklabels(markets.index, rotation=70, size = 15) ax[2].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[2].set_ylabel(&quot;Value (USD)&quot;, size = 15) plt.show() . Given such a pronounced difference between the Drugs marketplace and all the other ones, I created a second version of the plot above, but with the Drugs bar removed. The remaining categories are sorted by the number of listings. The graph at the very bottom is cleared of all outlier effects and shows that Counterfeits and Weapons steadily outpace other categories in the total value of items being sold. . markets_no = markets[markets.index != &#39;Drugs&#39;].sort_values(&#39;size&#39;, ascending=False) plt.style.use(&#39;dark_background&#39;) fig, ax = plt.subplots(3,1, sharex=True, figsize=(15,10)) ax[0].bar(markets_no.index, markets_no[&#39;size&#39;], color = &quot;#BB5566&quot;) ax[0].set_xticklabels(markets_no.index, rotation=70, size = 15) ax[0].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[0].set_ylabel(&quot;Size&quot;, size = 15) ax[1].bar(markets_no.index, markets_no[&#39;sum&#39;], color = &quot;#004488&quot;) ax[1].set_xticklabels(markets_no.index, rotation=70, size = 15) ax[1].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[1].set_ylabel(&quot;Value (USD)&quot;, size = 15) ax[2].bar(markets_no.index, markets_no[&#39;sum_no&#39;], color = &quot;#DDAA33&quot;) ax[2].set_xticklabels(markets_no.index, rotation=70, size = 15) ax[2].tick_params(axis=&#39;y&#39;, labelsize= 13) ax[2].set_ylabel(&quot;Value (USD)&quot;, size = 15) plt.show() . Weapons . weapons_of_interest = [&#39;Glock&#39;, &#39;Ruger&#39;, &#39;Walther&#39;, &#39;Beretta&#39;, &#39;AK-47&#39;] weapons_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(5): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Lethal firearms&#39;] subset = subset[subset[&#39;item&#39;].str.contains(weapons_of_interest[i], na=False)] weapons_df = weapons_df.append(subset[weapons_df.columns], ignore_index=True) weapons_df[&#39;weapon&#39;] = &#39;weapon&#39; for i in range(5): weapons_df.loc[:, &#39;weapon&#39;][weapons_df[&#39;item&#39;] .str.contains(weapons_of_interest[i], na=False)] = weapons_of_interest[i] weapons_df = weapons_df.reset_index(drop=True) del [subset, i, weapons_of_interest] print(weapons_df.shape) weapons_df.head() . (67, 4) . item price usd weapon . 0 Glock 19 Gen 3 9mm &amp; 2 Mags | 4.862971 | 2863.12 | Glock | . 1 Glock 17 3rd Gen FULL ESCROW | 3.656912 | 2153.04 | Glock | . 2 Glock 21SF 45 ACP FULL ESCROW | 3.656912 | 2153.04 | Glock | . 3 Glock 20 10MM FULL ESCROW | 3.656912 | 2153.04 | Glock | . 4 Glock 26 gen 3 9mm &amp; 2 mags | 4.598510 | 2707.41 | Glock | . Here, I create a data frame listing all the popular weapons brands, most of which can be found in the US legally. The names were extracted from the item descriptions and median prices were obtained for each brand. . weapons = weapons_df.groupby(&#39;weapon&#39;)[&#39;usd&#39;].median() .reset_index().sort_values(&#39;usd&#39;) .set_index(&#39;weapon&#39;) plt.style.use(&#39;dark_background&#39;) fig, ax = plt.subplots(figsize=(13,10)) ax.bar(weapons.index, weapons[&#39;usd&#39;], color = &#39;#004488&#39;) ax.set_xticklabels(weapons.index, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) ax.set_ylabel(&quot;Median Cost (USD)&quot;, size = 25) plt.show() . Buying weapons illegaly, compared to the usual US prices, is significantly more expensive. The median price for each brand listen on the Agora is anywhere between two to five times more expensive than if purchased from an accredited weapons dealer. Further analysis would be interesting here to determine the specific difference between legal prices and the ones from the Dark Web. Currently, that is beyond the scope of this project. . Drugs . drugs = dw_data[dw_data[&#39;category&#39;]==&#39;Drugs&#39;].groupby(&#39;category1&#39;)[&#39;usd&#39;] .agg([sum, np.size]).reset_index().sort_values(&#39;size&#39;, ascending=False) .set_index(&#39;category1&#39;) print(drugs.shape) drugs.head() . (13, 2) . sum size . category1 . Cannabis 5.200670e+08 | 20542.0 | . Ecstasy 1.700251e+08 | 9654.0 | . Stimulants 1.852040e+08 | 8474.0 | . Psychedelics 9.215761e+06 | 5384.0 | . Opioids 7.974575e+06 | 4474.0 | . plt.style.use(&#39;dark_background&#39;) fig, ax = plt.subplots(2, 1, sharex=True, figsize=(15,10)) ax[0].bar(drugs.index, drugs[&#39;size&#39;], color = &#39;#BB5566&#39;) ax[0].set_xticklabels(drugs.index, rotation=70, size = 20) ax[0].tick_params(axis=&#39;y&#39;, labelsize= 15) ax[0].set_ylabel(&quot;Size&quot;, size = 20) ax[1].bar(drugs.index, drugs[&#39;sum&#39;], color = &#39;#004488&#39;) ax[1].set_ylabel(&quot;Value (USD)&quot;, size = 20) ax[1].set_xticklabels(drugs.index, rotation=70, size = 20) ax[1].tick_params(axis=&#39;y&#39;, labelsize= 15) plt.show() . As pictured above, cannabis sales are much higher than those from all the other types of drugs sold on the Agora. Both in total market value (over $500 million) and market size (over 20,000 listings), illegal cannabis sales (at least in 2014) were far more popular than those of any other type of drug. . YouTube . social_media_regex = [&#39;[Y|y][O|o][U|u][T|t][U|u][B|b][E|e]&#39;, &#39;[T|t][W|w][I|i][T|t][T|t][E|e][R|r]&#39;, &#39;[F|f][A|a][C|c][E|e][B|b][O|o][O|o][K|k]&#39;, &#39;[I|i][N|n][S|s][T|t][A|a][G|g][R|r][A|a][M|m]&#39;] social_media = [&#39;YouTube&#39;, &#39;Twitter&#39;, &#39;Facebook&#39;, &#39;Instagram&#39;] products_regex = [&#39;[L|l][I|i][K|k][E|e]&#39;, &#39;[S|s][U|u][B|b][S|s][C|c][R|r][I|i][B|b][E|e][R|r]&#39;, &#39;[V|v][I|i][E|e][W|w]&#39;, &#39;[C|c][O|o][M|m][M|m][E|e][N|n][T|t]&#39;] products = [&#39;likes&#39;, &#39;subscribers&#39;, &#39;views&#39;, &#39;comments&#39;] social_media_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(4): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Advertising&#39;] subset = subset[subset[&#39;item&#39;].str.contains(social_media_regex[i], na=False)] social_media_df = social_media_df.append(subset[social_media_df.columns], ignore_index=True) social_media_df[&#39;company&#39;] = &#39;company&#39; social_media_df[&#39;product&#39;] = &#39;product&#39; for i in range(4): social_media_df.loc[:, &#39;company&#39;][social_media_df[&#39;item&#39;] .str.contains(social_media_regex[i], na=False)] = social_media[i] social_media_df.loc[:, &#39;product&#39;][social_media_df[&#39;item&#39;] .str.contains(products_regex[i], na=False)] = products[i] social_media_df = social_media_df[social_media_df[&#39;product&#39;] != &#39;product&#39;] .reset_index(drop=True) quantity = social_media_df[&#39;item&#39;] .str.extract(&#39;( d[ s| d]? d? d? s? d? d? d? s? d? d? d?)&#39;, expand=False).str.replace(&#39; s&#39;, &quot;&quot;) social_media_df[&#39;quantity&#39;] = pd.to_numeric(quantity) social_media_df.loc[64, &#39;quantity&#39;] = 5000 social_media_df[&#39;unit_price&#39;] = social_media_df[&#39;price&#39;] / social_media_df[&#39;quantity&#39;] social_media_df[&#39;unit_usd&#39;] = social_media_df[&#39;usd&#39;] / social_media_df[&#39;quantity&#39;] del [social_media_regex, social_media, products_regex, products, subset, i, quantity] print(social_media_df.shape) social_media_df.head() . (74, 8) . item price usd company product quantity unit_price unit_usd . 0 100 Youtube likes just 8 USD! | 0.032760 | 19.29 | YouTube | likes | 100 | 0.000328 | 0.192900 | . 1 100 Youtube subscriber just 10 USD! | 0.040950 | 24.11 | YouTube | subscribers | 100 | 0.000409 | 0.241100 | . 2 100 Youtube unlike just 10 USD! | 0.040929 | 24.10 | YouTube | likes | 100 | 0.000409 | 0.241000 | . 3 1000 Youtube views just 10 USD | 0.040960 | 24.12 | YouTube | views | 1000 | 0.000041 | 0.024120 | . 4 500 000 Real High Retention Youtube views | 2.747742 | 1617.76 | YouTube | views | 500000 | 0.000005 | 0.003236 | . Gathering social media content into a coherent data frame was a little more work. The code above extracts the per unit cost in purchasing an additional like, subscriber, view or comment for a given social media account. Most listings are regarding Facebook and Youtube. The code above uses the item descriptions to extract the number and type of unit being sold, and creates several new columns that reflect this while facilitating further analysis. . youtube = social_media_df[social_media_df[&#39;company&#39;]==&#39;YouTube&#39;] .groupby(&#39;product&#39;)[&#39;unit_usd&#39;].median().reset_index() .sort_values(&#39;unit_usd&#39;, ascending=False).set_index(&#39;product&#39;) plt.style.use(&#39;dark_background&#39;) fig, ax = plt.subplots(figsize=(15,10)) ax.bar(youtube.index, youtube[&#39;unit_usd&#39;], color = &#39;#228833&#39;) ax.set_xticklabels(youtube.index, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) ax.set_ylabel(&quot;Median Cost per Unit (USD)&quot;, size = 20) plt.show() . Pictured above, is the data for YouTube. According to all the postings, each additional comment was worth around 60 cents of a 2021 USD, putting it higher than subscribers, likes or views. Vast majority of these listings indicate that each product is of &#39;high quality&#39; meaning they are such that bot detecting software would not cleanse them. This usually indicates click farms (mostly from third world countries in which people are paid to constantly create new accounts and provide social media engagement) but such a claim cannot be ascertained from given data alone. . Below, I quickly calculate the expected per-unit-cost of a Facebook like, which seems to be a somewhat lower value than YouTube likes. . fb_like = social_media_df[(social_media_df[&#39;company&#39;]==&#39;Facebook&#39;) &amp; (social_media_df[&#39;product&#39;]==&#39;likes&#39;)] .loc[:, &#39;unit_usd&#39;].median() print(&quot;The expected per-unit cost of a Facebook like is&quot;, round(fb_like*100, 2), &quot;cents (2021 $USD).&quot;) . The expected per-unit cost of a Facebook like is 12.48 cents (2021 $USD). . Documents . doc_regex = [&#39;[P|p]assport&#39;, &#39;[D|d]river[ &#39;]?s[ s]?&#39;] doc = [&quot;Passport&quot;, &quot;Driver&#39;s License&quot;] docs_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(2): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Physical documents&#39;] subset = subset[subset[&#39;item&#39;].str.contains(doc_regex[i], na=False)] docs_df = docs_df.append(subset[docs_df.columns], ignore_index=True) docs_df[&#39;document&#39;] = &#39;document&#39; for i in range(2): docs_df.loc[:, &#39;document&#39;][docs_df[&#39;item&#39;] .str.contains(doc_regex[i], na=False)] = doc[i] docs_df.loc[:, &#39;document&#39;][docs_df[&#39;item&#39;] .str.contains(&#39;Passport[ s]?[+]&#39;)] = &quot;Passport+ID+Driver&#39;s License&quot; docs_df = docs_df[~docs_df[&#39;item&#39;].str.contains(&#39;emplate&#39;)].reset_index(drop=True) del [doc, doc_regex, i, subset] print(docs_df.shape) docs_df.head() . (114, 4) . item price usd document . 0 Fake Danish Passport | 2.808987 | 1653.82 | Passport | . 1 Fake Lithuanian Passport (old version) | 3.803150 | 2239.14 | Passport | . 2 Fake Lithuanian Passport | 2.475635 | 1457.55 | Passport | . 3 Netherlands Physical Passport + DL + ID CARD | 7.761003 | 4569.36 | Passport+ID+Driver&#39;s License | . 4 Pysical Fake Passports and IDs | 0.015551 | 9.16 | Passport | . This data frame was created in a way similar to the social media one. Item descriptions were used to separate out the type of &#39;product&#39; being sold. Three specific groups were identified: Passports, Driver&#39;s Licenses and a combination product of Passport, ID and Driver&#39;s License. These are worldwide so documents belong to numerous countries and/or states. For the sake of simplicity, the median cost is calculated on a worldwide basis. Counterfeit document listing also include a lot of templates being sold, which were filtered out, given that their values were significantly lower. . documents = docs_df.groupby(&#39;document&#39;)[&#39;usd&#39;].median().reset_index() .sort_values(&#39;usd&#39;, ascending=False).set_index(&#39;document&#39;) plt.style.use(&#39;dark_background&#39;) fig, ax = plt.subplots(figsize=(15,10)) ax.bar(documents.index, documents[&#39;usd&#39;], color = &#39;#AA3377&#39;) ax.set_xticklabels(documents.index, size = 15) ax.set_ylabel(&quot;Median Cost (USD)&quot;, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) plt.show() . Median costs were calcualted given that there are numerous countries listed, with varying degrees of value associated with their documentation, as well as several prominent outliers. While this category provides listings for real documents, that is, documents that are actually harder to detect as illegaly obtained and that exist in some database, there are many from the Fake category that slip into this one. They usually cost much less but the wording in the item description is difficult to use in filtering them out. Hopefully, the median expectation protects against these outlier effects. . Cigarettes . cigs_regex = [&#39;[D|d][U|u][N|n][H|h][I|i][L|l][L|l]&#39;, &#39;[B|b][O|o][N|n][D|d]&#39;, &#39;[V|v][O|o][G|g][U|u][E|e]&#39;, &#39;[W|w][I|i][N|n][S|s][T|t][O|o][N|n]&#39;, &#39;[M|m][A|a][R|r][L|l][B|b][O|o][R|r][O|o]&#39;, &#39;[P|p][A|a][L|l][L|l][ s]?[M|m][A|a][L|l][L|l]&#39;, &#39;[L|l][ s]?[&amp;][ s]?[M|m]&#39;, &#39;[L|l][U|u][C|c][K|k][Y|y]&#39;, &#39;[C|c][A|a][M|m][E|e][L|l]&#39;, &#39;[C|c][H|h][E|e][S|s][T|t][E|e][R|r]&#39;, &#39;[D|d][A|a][V|v][I|i][D|d][O|o]&#39;, &#39;[P|p][A|a][R|r][L|l][I|i][A|a][M|m]&#39;, &#39;[V|v][I|i][R|r][G|g][I|i][N|n][I|i][A|a]&#39;, &#39;[R|r][E|e][D|d][ s]?[&amp;][ s]?[W|w][H|h][I|i][T|t][E|e]&#39;] cigs = [&#39;Dunhill&#39;, &#39;Bond&#39;, &#39;Vogue&#39;, &#39;Winston&#39;, &#39;Marlboro&#39;, &#39;Pall Mall&#39;, &#39;L&amp;M&#39;, &#39;Lucky Strike&#39;, &#39;Camel&#39;, &#39;Chesterfield&#39;, &#39;Davidoff&#39;, &#39;Parliament&#39;, &#39;Virginia&#39;, &#39;Red&amp;White&#39;] cigs_df = pd.DataFrame(columns=[&#39;item&#39;, &#39;price&#39;, &#39;usd&#39;]) for i in range(10): subset = dw_data[dw_data[&#39;category1&#39;] == &#39;Smoked&#39;] subset = subset[subset[&#39;item&#39;].str.contains(cigs_regex[i], na=False)] cigs_df = cigs_df.append(subset[cigs_df.columns], ignore_index=True) cigs_df[&#39;brand&#39;] = &#39;brand&#39; for i in range(14): cigs_df.loc[:, &#39;brand&#39;][cigs_df[&#39;item&#39;] .str.contains(cigs_regex[i], na=False)] = cigs[i] # almost all original prices are listed for 10 packs, so I divide by 10 to get pack price # but sometimes it&#39;s a single pack listed and that&#39;s why I did this division only when # the total price listed was above 10USD cigs_df[&#39;unit_cost&#39;] = cigs_df[&#39;usd&#39;] cigs_df.loc[:, &#39;unit_cost&#39;][cigs_df[&#39;unit_cost&#39;] &gt; 10] = cigs_df[&#39;unit_cost&#39;] / 10 cigs_df = cigs_df.reset_index(drop=True) del [cigs_regex, cigs, subset, i] print(cigs_df.shape) cigs_df.head() . (101, 5) . item price usd brand unit_cost . 0 Dunhill Fine Cut Dark Blue (10 packs x 20 ciga... | 0.160849 | 94.70 | Dunhill | 9.470 | . 1 DUNHILL Cheap Cigarettes to USA and EU | 0.156015 | 91.86 | Dunhill | 9.186 | . 2 Dunhill Fine Cut Black (10 packs x 20 cigarettes) | 0.160806 | 94.68 | Dunhill | 9.468 | . 3 Copy of Dunhill Fine Cut Black (10 packs x 20 ... | 0.091288 | 53.75 | Dunhill | 5.375 | . 4 Bond Cheap Cigarettes to USA and EU | 0.088071 | 51.85 | Bond | 5.185 | . Again, repeating the procedure used for the social media and documentation data frames, once again I go through the item descriptions and extract all the relevant cigarette brands to group by later. Furthermore, it is clear from the description that almost every single listed price is for a product consisting of 10 packs with 20 cigarettes each. Therefore, the Bitcoin price was converted to US Dollar values, and divided by 10, to get the cost of a single pack for each brand. . cigarettes = cigs_df.groupby(&#39;brand&#39;)[&#39;unit_cost&#39;].median().reset_index() .sort_values(&#39;unit_cost&#39;, ascending=False).set_index(&#39;brand&#39;) plt.style.use(&#39;dark_background&#39;) fig, ax = plt.subplots(figsize=(15,10)) ax.bar(cigarettes.index, cigarettes[&#39;unit_cost&#39;], color = &#39;#CC3311&#39;) ax.set_xticklabels(cigarettes.index, rotation=80, size = 20) ax.set_ylabel(&quot;Median Cost per Pack (USD)&quot;, size = 20) ax.tick_params(axis=&#39;y&#39;, labelsize= 15) plt.show() . As can be seen in the plot above, the brands are sorted by cost. The prices are median costs, per pack, in 2021 $USD. Surprisingly, they don&#39;t differ significantly from prices in states with low to moderate taxes. However, they are much smaller than those in states like New York, which have costs of around $15 per pack, easily. . Conclusion . As an initial look into the illegal dealings on the Dark Web, this project is appropriate. It allows for a preliminary sense of how products are listed, what the scope of the markets is, how data is organized and what possible insights can be gleaned from it. Furthermore, it illuminates all the issues present in trying to make sense of it all. The prices are listed in Bitcoin and held online for unknown periods of time, even as Bitcoin fluctuates. Knowing the possible range for a year helps in narrowing down the expected cost values. However, error bars need to be created to show the amount of uncertainty in this approach. . With more computing power and detailed string parsing scripts, I would be willing to take a crack at the 1.6TB data set. Price differences between legal and illegal products can illuminate countless premiums customers are willing to pay on various products, which is a worthwhile endeavor for any economist to pursue. . For the data and other notebooks, see github.com/antoniojurlina/learning_python. .",
            "url": "https://antoniojurlina.github.io/portfolio/projects/python/2021/05/07/Dark_Web.html",
            "relUrl": "/projects/python/2021/05/07/Dark_Web.html",
            "date": " • May 7, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Working with Dates and Times",
            "content": "import pandas as pd import numpy as np from datetime import datetime as dt from dateutil import tz import matplotlib.pyplot as plt import seaborn as sns import os os.chdir(&#39;/Users/antoniojurlina/Projects/learning_python/data/&#39;) . The objective of this lab is to examine crime data from Chicago and analyze it with respect to types of crimes and temporal patterns of occurrence. . The data file, Crimes2021.csv, covers the period from January 1st, 2021 to March 15th, 2021. . It includes the following attributes: ID, Case Number, Date, Block, IUCR, Primary Type, Description, Location Description, Arrest, Domestic, Beat, District, Ward, Community Area, FBI Code, X Coordinate, Y Coordinate, Year, Updated On, Latitude, Longitude, Location . For this lab I will restrict the analysis to just the Date and Primary Type (the type of crime reported). . crimes = pd.read_csv(&#39;Crimes2021.csv&#39;, usecols=[&#39;Date&#39;, &#39;Primary Type&#39;], parse_dates=[&#39;Date&#39;]) .rename(columns={&#39;Date&#39;:&#39;date&#39;, &#39;Primary Type&#39;:&#39;type&#39;}) .sort_values(&#39;date&#39;, ignore_index=True) crimes[&#39;date&#39;] = crimes[&#39;date&#39;].dt.tz_localize(&#39;US/Central&#39;) crimes[&#39;type&#39;] = crimes[&#39;type&#39;].str.title() crimes[&#39;month&#39;] = crimes[&#39;date&#39;].dt.month_name() crimes[&#39;weekday&#39;] = crimes[&#39;date&#39;].dt.day_name() crimes[&#39;hour&#39;] = crimes[&#39;date&#39;].dt.hour crimes[&#39;week&#39;] = crimes[&#39;date&#39;].dt.isocalendar().week print(crimes.shape) crimes.head() . (35254, 6) . date type month weekday hour week . 0 2021-01-01 00:00:00-06:00 | Battery | January | Friday | 0 | 53 | . 1 2021-01-01 00:00:00-06:00 | Other Offense | January | Friday | 0 | 53 | . 2 2021-01-01 00:00:00-06:00 | Assault | January | Friday | 0 | 53 | . 3 2021-01-01 00:00:00-06:00 | Sex Offense | January | Friday | 0 | 53 | . 4 2021-01-01 00:00:00-06:00 | Criminal Damage | January | Friday | 0 | 53 | . Monthly Crime Statistics . monthly = crimes.groupby([&#39;month&#39;, &#39;type&#39;]).size(). reset_index().pivot_table(values=0, index=&#39;type&#39;, columns=&#39;month&#39;, fill_value=0) monthly = monthly[[&#39;January&#39;, &#39;February&#39;, &#39;March&#39;]] %matplotlib inline plt.style.use(&#39;ggplot&#39;) fig, ax = plt.subplots(figsize=(8,10)) ax = sns.heatmap(monthly, linewidth=0.01, cmap=&#39;crest&#39;) ax.set_ylabel(&quot;&quot;) ax.set_xlabel(&quot;&quot;) plt.show() . Overall, total counts per crime have gone down, across all types of crime, in the period from January to March. Battery, assault, criminal damage, deceptive practice and theft seem to be the most common types of crime commited in Chicago, over this period. . Weekly Crime Statistics . weekly = crimes.groupby([&#39;week&#39;, &#39;type&#39;]).size(). reset_index().pivot_table(values=0, index=&#39;type&#39;, columns=&#39;week&#39;, fill_value=0) cols = weekly.columns.tolist() cols = cols[-1:] + cols[:-1] weekly = weekly[cols] fig, ax = plt.subplots(figsize=(13,10)) ax = sns.heatmap(weekly, linewidth=0.01, cmap=&#39;crest&#39;) ax.set_ylabel(&quot;&quot;) plt.show() . On a weekly basis, assault, battery, and theft seem to be fairly constant in the number of occurences, while the overall number of deceptive practices has gone down. . Daily Crime Statistics . daily = crimes.groupby([&#39;weekday&#39;, &#39;type&#39;]).size(). reset_index().pivot_table(values=0, index=&#39;type&#39;, columns=&#39;weekday&#39;, fill_value=0) daily = daily[[&#39;Monday&#39;, &#39;Tuesday&#39;, &#39;Wednesday&#39;, &#39;Thursday&#39;, &#39;Friday&#39;, &#39;Saturday&#39;, &#39;Sunday&#39;]] fig, ax = plt.subplots(figsize=(13,10)) ax = sns.heatmap(daily, linewidth=0.01, cmap=&#39;crest&#39;) ax.set_ylabel(&quot;&quot;) ax.set_xlabel(&quot;&quot;) plt.show() . On a daily basis, battery seems to be most common on the weekends. Assault and criminal damage are fairly equally present throughout the week. Finally, Deceptive practices seem to decrease over the weekends. . Hourly Crime Statistics . hourly = crimes.groupby([&#39;hour&#39;, &#39;type&#39;]).size(). reset_index().pivot_table(values=0, index=&#39;type&#39;, columns=&#39;hour&#39;, fill_value=0) fig, ax = plt.subplots(figsize=(14,10)) ax = sns.heatmap(hourly, linewidth=0.01, cmap=&#39;crest&#39;) ax.set_ylabel(&quot;&quot;) plt.show() . On an hourly basis, deceptive practices occur largely around midnight, 9am and noon. Most crimes do not seem to occur between 1 and 9 am, with theft, battery and assault occuring mostly in the afternoons. . For the data and other notebooks, see github.com/antoniojurlina/learning_python. .",
            "url": "https://antoniojurlina.github.io/portfolio/learning/python/2021/04/29/working-with-dates-and-times.html",
            "relUrl": "/learning/python/2021/04/29/working-with-dates-and-times.html",
            "date": " • Apr 29, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Aggregating Data with Pandas",
            "content": "import pandas as pd import numpy as np from datetime import datetime as dt import matplotlib.pyplot as plt import os . First, we start by importing the two data sets to be merged. The data DataFrame is the same one that was cleaned in the previous lab. The zipcodes DataFrame will be used to fill in all the missing values in the data DataFrame. . os.chdir(&#39;/Users/antoniojurlina/Projects/learning_python/data/&#39;) data=pd.read_csv(&#39;CapeCodCases2019_clean.csv&#39;) zipcodes = pd.read_csv(&#39;MassachusettsZipcodeData.csv&#39;) data[&#39;city&#39;] = data[&#39;city&#39;].str.replace(&quot;Yarmouthport&quot;, &quot;Yarmouth Port&quot;) zipcodes[&#39;city_town&#39;] = zipcodes[&#39;city_town&#39;].str.replace(&quot;, TOWN OF&quot;, &quot;&quot;).str.title() zipcodes[&#39;county&#39;] = zipcodes[&#39;county&#39;].str.title() zipcodes[&#39;pc_name&#39;] = zipcodes[&#39;pc_name&#39;].str.title() zipcodes[&#39;postcode&#39;] = zipcodes[&#39;postcode&#39;].astype(str) zipcodes_short = zipcodes[[&#39;postcode&#39;, &#39;city_town&#39;, &#39;county&#39;]]. rename(columns = {&#39;postcode&#39;:&#39;zip_code&#39;,&#39;city_town&#39;:&#39;city&#39;}) . The cleaned data: . data.head() . Unnamed: 0 address city common_name county date disposition keywords admission_id case_year zip_code . 0 0 | Long Beach | Hyannis | Common Eider | Barnstable | 2019-01-01 | Died +24hr | NaN | 1 | 2019 | 2601 | . 1 1 | 3260 Main Street | Brewster | Common Eider | Barnstable | 2019-01-02 | Euthanized +24hr | NaN | 2 | 2019 | 2671 | . 2 2 | Thumpertown Beach | Eastham | Razorbill | Barnstable | 2019-01-02 | Dead on arrival | NaN | 3 | 2019 | NaN | . 3 3 | 5575 State Highway | Eastham | Southern Flying Squirrel | Barnstable | 2019-01-03 | Released | NaN | 4 | 2019 | 2651 | . 4 4 | 5575 State Highway | Eastham | Southern Flying Squirrel | Barnstable | 2019-01-03 | Released | NaN | 5 | 2019 | 2651 | . The data to be used for replacing missing values: . zipcodes_short.head() . zip_code city county . 0 1331 | Athol | Worcester | . 1 2769 | Rehoboth | Bristol | . 2 1085 | Westfield | Hampden | . 3 1370 | Shelburne | Franklin | . 4 1235 | Hinsdale | Berkshire | . Here we merge the two data frames, fill the missing values and retrieve the relevant columns. . merged_data = data.merge(zipcodes_short, on=[&#39;city&#39;], how=&#39;left&#39;) county_na_before = merged_data[&#39;county_x&#39;].isna().sum() zipcode_na_before = merged_data[&#39;zip_code_x&#39;].isna().sum() merged_data[&#39;county_x&#39;] = merged_data[&#39;county_x&#39;].fillna(merged_data[&#39;county_y&#39;]) merged_data[&#39;zip_code_x&#39;] = merged_data[&#39;zip_code_x&#39;].fillna(merged_data[&#39;zip_code_y&#39;]) county_na_after = merged_data[&#39;county_x&#39;].isna().sum() zipcode_na_after = merged_data[&#39;zip_code_x&#39;].isna().sum() clean_data = merged_data[[&#39;address&#39;, &#39;city&#39;, &#39;common_name&#39;, &#39;county_x&#39;, &#39;date&#39;, &#39;disposition&#39;, &#39;zip_code_x&#39;]] clean_data = clean_data.drop_duplicates(subset=clean_data.columns.difference([&#39;zip_code_x&#39;])) .rename(columns = {&#39;county_x&#39;:&#39;county&#39;, &#39;zip_code_x&#39;:&#39;zipcode&#39;}) clean_data.head() . address city common_name county date disposition zipcode . 0 Long Beach | Hyannis | Common Eider | Barnstable | 2019-01-01 | Died +24hr | 2601 | . 1 3260 Main Street | Brewster | Common Eider | Barnstable | 2019-01-02 | Euthanized +24hr | 2671 | . 2 Thumpertown Beach | Eastham | Razorbill | Barnstable | 2019-01-02 | Dead on arrival | 2642 | . 3 5575 State Highway | Eastham | Southern Flying Squirrel | Barnstable | 2019-01-03 | Released | 2651 | . 8 Nickerson State Park | Brewster | Common Loon | Barnstable | 2019-01-03 | Died +24hr | 2631 | . print(&#39;The number of missing values in the county column decreased from&#39;, county_na_before, &#39;to&#39; , county_na_after) . The number of missing values in the county column decreased from 151 to 13 . print(&#39;The number of missing values in the zipcode column decreased from&#39;, zipcode_na_before, &#39;to&#39; , zipcode_na_after) . The number of missing values in the zipcode column decreased from 1415 to 133 . Using combinations of the common name field with city to group and summarize the 10 most common types of animal that have been rescued by city: . view = clean_data.groupby([&#39;city&#39;, &#39;common_name&#39;]).size().reset_index(name = &#39;count&#39;). sort_values([&#39;city&#39;, &#39;count&#39;], ascending=False).groupby(&#39;city&#39;).head(10).reset_index(drop=True) view . city common_name count . 0 Yarmouth Port | Chimney Swift | 2 | . 1 Yarmouth Port | Downy Woodpecker | 2 | . 2 Yarmouth Port | Black-Capped Chickadee | 1 | . 3 Yarmouth Port | Eastern Cottontail | 1 | . 4 Yarmouth Port | Eastern Gray Squirrel | 1 | . ... ... | ... | ... | . 377 Barnstable | American Robin | 1 | . 378 Barnstable | House Sparrow | 1 | . 379 Barnstable | Mourning Dove | 1 | . 380 Athol | American Robin | 1 | . 381 968 Route 6A | Eastern Gray Squirrel | 1 | . 382 rows × 3 columns . Reporting and creating bar charts of the top 10 animals (Common Name) by city that have been rescued. . Note: Faceting over all the cities in the data set renders any data visualization unusable. Furthermore, many cities had only 1 or 2 animals rescued over the entire recorded period. Therefore, bar charts were reported separately for each city, and only when 3 or more distinct species had been rescued. . filtered = view.groupby(&#39;city&#39;).size().reset_index(name=&#39;n_of_species&#39;) cities = filtered[filtered[&#39;n_of_species&#39;] &gt; 2][&#39;city&#39;].unique() for x in cities: view[view[&#39;city&#39;]==x].drop(&#39;city&#39;, axis=1).set_index(&#39;common_name&#39;).plot(kind=&#39;barh&#39;) plt.title(x) plt.ylabel(&quot;Common Name&quot;) plt.xlabel(&quot;Number Rescued&quot;) . /Users/antoniojurlina/opt/anaconda3/lib/python3.8/site-packages/pandas/plotting/_matplotlib/core.py:328: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`). fig = self.plt.figure(figsize=self.figsize) . For the data and other notebooks, see github.com/antoniojurlina/learning_python. .",
            "url": "https://antoniojurlina.github.io/portfolio/learning/python/2021/04/12/aggregating-data-with-pandas.html",
            "relUrl": "/learning/python/2021/04/12/aggregating-data-with-pandas.html",
            "date": " • Apr 12, 2021"
        }
        
    
  
    
        ,"post3": {
            "title": "Cleaning Data with Pandas",
            "content": "import pandas as pd import numpy as np import datetime as dt import os . os.chdir(&#39;/Users/antoniojurlina/Projects/learning_python/data/&#39;) data=pd.read_csv(&#39;CapeCodCases2019.csv&#39;) data.head() . patients.address_found patients.city_found patients.common_name patients.county_found patients.found_at patients.disposition patients.keywords admissions.id admissions.case_year people.postal_code . 0 Long Beach | Hyannis | Common Eider | Barnstable County | 1/1/2019 | Died +24hr | NaN | 1 | 2019 | 2601 | . 1 3260 Main Street | Brewster | Common Eider | Barnstable County | 1/2/2019 | Euthanized +24hr | NaN | 2 | 2019 | 2671 | . 2 Thumpertown Beach | Eastham | Razorbill | Barnstable County | 1/2/2019 | Dead on arrival | NaN | 3 | 2019 | NaN | . 3 5575 State Highway | Eastham | Southern Flying Squirrel | Barnstable County | 1/3/2019 | Released | NaN | 4 | 2019 | 2651 | . 4 5575 State Highway | Eastham | Southern Flying Squirrel | Barnstable County | 1/3/2019 | Released | NaN | 5 | 2019 | 2651 | . By reviewing unique values of variables city, county and address, the following strings were determined to be different instantations of a missing value. . missing_values = [&quot;Umknown&quot;, &quot;No Info Given&quot;, &quot;Unknown.&quot;, &quot;Unknown&quot;, &quot;No Address Given&quot;, &quot;No Address Given.&quot;, &quot;None&quot;, &quot;No Information Given.&quot;, &quot;Nan&quot;, &quot;nan&quot;, &quot;&quot;] . From this point, we proceed to clean the data, starting with renaming the variables. . data = data.rename(columns={&#39;patients.address_found&#39;: &#39;address&#39;, &#39;patients.city_found&#39;: &#39;city&#39;, &#39;patients.common_name&#39;: &#39;common_name&#39;, &#39;patients.county_found&#39;: &#39;county&#39;, &#39;patients.found_at&#39;: &#39;date&#39;, &#39;patients.disposition&#39;: &#39;disposition&#39;, &#39;patients.keywords&#39;: &quot;keywords&quot;, &#39;admissions.id&#39;: &quot;admission_id&quot;, &#39;admissions.case_year&#39;: &quot;case_year&quot;, &#39;people.postal_code&#39;: &#39;zip_code&#39;}) print(data.shape) data.head() . (1871, 10) . address city common_name county date disposition keywords admission_id case_year zip_code . 0 Long Beach | Hyannis | Common Eider | Barnstable County | 1/1/2019 | Died +24hr | NaN | 1 | 2019 | 2601 | . 1 3260 Main Street | Brewster | Common Eider | Barnstable County | 1/2/2019 | Euthanized +24hr | NaN | 2 | 2019 | 2671 | . 2 Thumpertown Beach | Eastham | Razorbill | Barnstable County | 1/2/2019 | Dead on arrival | NaN | 3 | 2019 | NaN | . 3 5575 State Highway | Eastham | Southern Flying Squirrel | Barnstable County | 1/3/2019 | Released | NaN | 4 | 2019 | 2651 | . 4 5575 State Highway | Eastham | Southern Flying Squirrel | Barnstable County | 1/3/2019 | Released | NaN | 5 | 2019 | 2651 | . Then, most variables are insured to be stored as strings, with several having each first letter capitalized. The date a rescued animal was found is stored in date format. . data[&#39;address&#39;] = data[&#39;address&#39;].astype(str).str.title() data[&#39;city&#39;] = data[&#39;city&#39;].astype(str).str.title() data[&#39;common_name&#39;] = data[&#39;common_name&#39;].astype(str).str.title() data[&#39;county&#39;] = data[&#39;county&#39;].astype(str).str.title() data[&#39;date&#39;] = pd.to_datetime(data[&#39;date&#39;].astype(str), format=&#39;%m/%d/%Y&#39;) data[&#39;disposition&#39;] = data[&#39;disposition&#39;].astype(str) data[&#39;keywords&#39;] = data[&#39;keywords&#39;].astype(str) data[&#39;zip_code&#39;] = data[&#39;zip_code&#39;].astype(str) . Further cleaning proceeds with replacement of all strings previously determined to count as &#39;missing&#39; with NaN&#39;s, and some text cleanup. . missing = data[&#39;address&#39;].isin(missing_values) data.loc[missing, &#39;address&#39;] = np.nan missing = data[&#39;county&#39;].isin(missing_values) data[&#39;county&#39;] = data[&#39;county&#39;].str.replace(&quot;County&quot;, &quot;&quot;).str.strip() data.loc[missing, &#39;county&#39;] = np.nan data.loc[data[&#39;county&#39;]==&#39;2563&#39;, &#39;zip_code&#39;] = &#39;2563&#39; data.loc[data[&#39;county&#39;]==&#39;2563&#39;, &#39;county&#39;] = &#39;2563&#39; missing = data[&#39;city&#39;].isin(missing_values) data.loc[missing, &#39;city&#39;] = np.nan data.loc[data[&#39;city&#39;]==&#39;Orleans, Ma&#39;, &#39;city&#39;] = &#39;Orleans&#39; data.loc[data[&#39;city&#39;]==&#39;Eastham, Ma&#39;, &#39;city&#39;] = &#39;Eastham&#39; data.loc[data[&#39;city&#39;]==&#39;Eastham,&#39;, &#39;city&#39;] = &#39;Eastham&#39; data.loc[data[&#39;city&#39;]==&#39;Eastham.&#39;, &#39;city&#39;] = &#39;Eastham&#39; data.loc[data[&#39;city&#39;]==&#39;N. Eastham&#39;, &#39;city&#39;] = &#39;North Eastham&#39; data.loc[data[&#39;city&#39;]==&#39;E. Orleans&#39;, &#39;city&#39;] = &#39;East Orleans&#39; . Finally, any row where the address, county and city are missing, are dropped, given there can be no analysis of interest on them. . to_drop = np.where(data[&#39;address&#39;].isnull() &amp; data[&#39;city&#39;].isnull() &amp; data[&#39;county&#39;].isnull()) data = data.drop(np.concatenate(to_drop)) data = data.reset_index(drop = True) print(data.shape) data.head() . (1799, 10) . address city common_name county date disposition keywords admission_id case_year zip_code . 0 Long Beach | Hyannis | Common Eider | Barnstable | 2019-01-01 | Died +24hr | nan | 1 | 2019 | 2601 | . 1 3260 Main Street | Brewster | Common Eider | Barnstable | 2019-01-02 | Euthanized +24hr | nan | 2 | 2019 | 2671 | . 2 Thumpertown Beach | Eastham | Razorbill | Barnstable | 2019-01-02 | Dead on arrival | nan | 3 | 2019 | nan | . 3 5575 State Highway | Eastham | Southern Flying Squirrel | Barnstable | 2019-01-03 | Released | nan | 4 | 2019 | 2651 | . 4 5575 State Highway | Eastham | Southern Flying Squirrel | Barnstable | 2019-01-03 | Released | nan | 5 | 2019 | 2651 | . For the data and other notebooks, see github.com/antoniojurlina/learning_python. .",
            "url": "https://antoniojurlina.github.io/portfolio/learning/python/2021/03/27/cleaning-data-with-pandas.html",
            "relUrl": "/learning/python/2021/03/27/cleaning-data-with-pandas.html",
            "date": " • Mar 27, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Working with Pandas",
            "content": "import pandas as pd import numpy as np import datetime as dt import os os.chdir(&#39;/Users/antoniojurlina/Projects/learning_python/data/&#39;) data=pd.read_csv(&#39;RDC_Inventory_Core_Metrics_State_History.csv&#39;) data.head() . month_date_yyyymm state state_id median_listing_price median_listing_price_mm median_listing_price_yy active_listing_count active_listing_count_mm active_listing_count_yy median_days_on_market ... median_square_feet_yy average_listing_price average_listing_price_mm average_listing_price_yy total_listing_count total_listing_count_mm total_listing_count_yy pending_ratio pending_ratio_mm pending_ratio_yy . 0 202101 | delaware | de | 358050.0 | 0.0258 | 0.1002 | 1582 | -0.1558 | -0.5609 | 91.0 | ... | -0.0133 | 488354 | 0.0395 | 0.1432 | 4059 | -0.1368 | 0.0305 | 1.5657 | 0.0567 | 1.4725 | . 1 202101 | idaho | id | 465040.0 | 0.0816 | 0.2433 | 1817 | -0.1881 | -0.6908 | 66.0 | ... | -0.0427 | 977816 | 0.1570 | 0.6033 | 5554 | -0.1156 | -0.1471 | 2.0567 | 0.2506 | 1.9486 | . 2 202101 | arizona | az | 419050.0 | 0.0771 | 0.1403 | 8212 | -0.1487 | -0.5687 | 46.0 | ... | -0.0405 | 758396 | 0.1329 | 0.2679 | 22191 | -0.1257 | -0.2780 | 1.7023 | 0.0710 | 1.0878 | . 3 202101 | north dakota | nd | 235050.0 | -0.0053 | 0.0444 | 1992 | -0.1487 | -0.2949 | 92.0 | ... | -0.0105 | 281329 | 0.0055 | 0.0382 | 2732 | -0.1319 | -0.1658 | 0.3715 | 0.0266 | 0.2122 | . 4 202101 | maryland | md | 349050.0 | 0.0205 | 0.0579 | 6885 | -0.1406 | -0.5358 | 59.0 | ... | -0.0927 | 521490 | 0.0423 | 0.1250 | 17332 | -0.1330 | 0.0193 | 1.5174 | 0.0219 | 1.3708 | . 5 rows × 39 columns . columns = data.columns[0:12] data = data[columns] data[&#39;month_date_yyyymm&#39;] = pd.to_datetime(data[&#39;month_date_yyyymm&#39;], format=&#39;%Y%m&#39;) data = data.rename(columns={&#39;month_date_yyyymm&#39;: &#39;date&#39;, &#39;state&#39;: &#39;state&#39;, &#39;state_id&#39;: &#39;id&#39;, &#39;median_listing_price&#39;: &#39;mlp&#39;, &#39;median_listing_price_mm&#39;: &#39;mlp_mm&#39;, &#39;median_listing_price_yy&#39;: &#39;mlp_yy&#39;, &#39;active_listing_count&#39;: &quot;alc&quot;, &#39;active_listing_count_mm&#39;: &quot;alc_mm&quot;, &#39;active_listing_count_yy&#39;: &quot;alc_yy&quot;, &#39;median_days_on_market&#39;: &#39;mdom&#39;, &#39;median_days_on_market_mm&#39;: &#39;mdom_mm&#39;, &#39;median_days_on_market_yy&#39;: &#39;mdom_yy&#39;,}) data[&#39;year&#39;] = data[&#39;date&#39;].dt.strftime(&#39;%Y&#39;) data[&#39;month&#39;] = data[&#39;date&#39;].dt.strftime(&#39;%m&#39;) data = data[(data[&#39;year&#39;] != &#39;2016&#39;) &amp; (data[&#39;year&#39;] != &#39;2021&#39;)] data.head() . date state id mlp mlp_mm mlp_yy alc alc_mm alc_yy mdom mdom_mm mdom_yy year month . 51 2020-12-01 | west virginia | wv | 165050.0 | -0.0251 | 0.0036 | 3975 | -0.0693 | -0.4563 | 85.0 | 0.0897 | -0.2308 | 2020 | 12 | . 52 2020-12-01 | nevada | nv | 350049.0 | 0.0000 | 0.0449 | 8290 | -0.1076 | -0.2799 | 51.5 | 0.1705 | -0.2137 | 2020 | 12 | . 53 2020-12-01 | north dakota | nd | 236300.0 | -0.0154 | 0.0477 | 2340 | -0.0707 | -0.2769 | 87.5 | 0.1513 | -0.1250 | 2020 | 12 | . 54 2020-12-01 | iowa | ia | 214950.0 | 0.0000 | 0.1008 | 8690 | -0.0990 | -0.3749 | 77.0 | 0.1000 | -0.0833 | 2020 | 12 | . 55 2020-12-01 | arizona | az | 389050.0 | -0.0019 | 0.1116 | 9646 | -0.1391 | -0.5272 | 46.0 | 0.0952 | -0.2640 | 2020 | 12 | . (1) Computing the differences for the 3 variables (median listing price, active listing count, days on the market) between 2020 and (2019, 2018, and 2017). . columns = [&#39;month&#39;, &#39;state&#39;, &#39;mlp&#39;, &#39;alc&#39;, &#39;mdom&#39;] data_2017 = data[data[&#39;year&#39;] == &#39;2017&#39;][columns].reset_index(drop = True) data_2018 = data[data[&#39;year&#39;] == &#39;2018&#39;][columns].reset_index(drop = True) data_2019 = data[data[&#39;year&#39;] == &#39;2019&#39;][columns].reset_index(drop = True) data_2020 = data[data[&#39;year&#39;] == &#39;2020&#39;][columns].reset_index(drop = True) d1 = data_2020.set_index([&#39;state&#39;, &#39;month&#39;]) - data_2019.set_index([&#39;state&#39;, &#39;month&#39;]) d2 = data_2020.set_index([&#39;state&#39;, &#39;month&#39;]) - data_2018.set_index([&#39;state&#39;, &#39;month&#39;]) d3 = data_2020.set_index([&#39;state&#39;, &#39;month&#39;]) - data_2017.set_index([&#39;state&#39;, &#39;month&#39;]) d1[&#39;year&#39;] = &#39;2020-2019&#39; d2[&#39;year&#39;] = &#39;2020-2018&#39; d3[&#39;year&#39;] = &#39;2020-2017&#39; output = pd.concat([d1, d2, d3], axis = 0) del(d1, d2, d3) output.reset_index().set_index([&#39;year&#39;, &#39;state&#39;, &#39;month&#39;]) . mlp alc mdom . year state month . 2020-2019 alabama 01 20925.0 | -3646 | -10.0 | . 02 19550.0 | -3783 | -11.0 | . 03 17400.0 | -3988 | -16.0 | . 04 14550.0 | -3877 | -2.0 | . 05 13662.0 | -4751 | 4.5 | . ... ... ... ... | ... | ... | . 2020-2017 wyoming 08 42500.0 | -2191 | -16.5 | . 09 33500.0 | -2374 | -29.0 | . 10 30000.0 | -2304 | -33.5 | . 11 23550.0 | -2053 | -37.0 | . 12 20000.0 | -1929 | -42.0 | . 1836 rows × 3 columns . (2) Computing the difference between the means of these variables for the previous three years and 2020. . columns = [&#39;mlp&#39;, &#39;alc&#39;, &#39;mdom&#39;] d1 = data_2020[columns].mean() - data_2019[columns].mean() d2 = data_2020[columns].mean() - data_2018[columns].mean() d3 = data_2020[columns].mean() - data_2017[columns].mean() pd.concat([pd.DataFrame(d1.round(2), columns = [&#39;2020-2019&#39;]), pd.DataFrame(d2.round(2), columns = [&#39;2020-2018&#39;]), pd.DataFrame(d3.round(2), columns = [&#39;2020-2017&#39;])], axis = 1) . 2020-2019 2020-2018 2020-2017 . mlp 21696.86 | 36497.36 | 53650.75 | . alc -6949.50 | -7292.57 | -8271.04 | . mdom -4.93 | -6.44 | -11.06 | . For the data and other notebooks, see github.com/antoniojurlina/learning_python. .",
            "url": "https://antoniojurlina.github.io/portfolio/learning/python/2021/03/03/working-with-pandas.html",
            "relUrl": "/learning/python/2021/03/03/working-with-pandas.html",
            "date": " • Mar 3, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Working with NumPy",
            "content": "import pandas as pd import numpy as np import matplotlib.pyplot as plt import os . os.chdir(&#39;/Users/antoniojurlina/Projects/learning_python/data/&#39;) csv = &quot;BangorPrecip.csv&quot; bangorprecip = pd.read_csv(csv, index_col=0) months = bangorprecip.index.to_numpy() years = bangorprecip.columns.to_numpy() bangorprecip = bangorprecip.to_numpy() print(bangorprecip.shape) bangorprecip . (12, 10) . array([[3.47, 1.66, 1.95, 0.87, 3.18, 2.69, 2.38, 3.63, 5.53, 5.15], [2.19, 2.19, 1.55, 2.04, 1.99, 2.2 , 3.89, 2.26, 2.99, 1.83], [4.68, 4.5 , 1.4 , 2.05, 3.65, 1.45, 3.27, 2.07, 2.18, 1.94], [2.24, 5.26, 3.99, 1.77, 2.28, 2.39, 2.58, 3.98, 4.77, 5.53], [1.73, 3.51, 5.12, 4.56, 3.31, 2.32, 2.09, 6.36, 2.37, 4.43], [4.38, 2.79, 7.05, 5.46, 3.59, 4.9 , 2.85, 4.32, 5.42, 5.33], [2.15, 2.1 , 0.58, 3.74, 6.78, 1.16, 2.69, 1.91, 2.48, 4.46], [1.89, 8.32, 2.6 , 4.69, 2.96, 2.63, 2.24, 1.64, 2.73, 7.54], [5.73, 2.79, 6.33, 7.17, 0.89, 6.89, 1.23, 2.69, 2.63, 2.81], [5.83, 5.16, 6.96, 1.02, 6.85, 2.58, 3.2 , 5.59, 4.66, 5.8 ], [4.75, 2.19, 1.13, 3.76, 3.21, 2.26, 4.16, 3.05, 7. , 3.97], [5.47, 3.85, 3.59, 2.93, 5.23, 4.06, 3.77, 3.83, 4.23, 3.21]]) . 1. What was the total cumulative precipitation over the ten years? . total_precip = np.sum(bangorprecip) print(&quot;Total cumulative precipitation over the ten years was&quot;, total_precip, &quot;inches.&quot;) . Total cumulative precipitation over the ten years was 425.26 inches. . 2. What was the driest year? . yearly_totals = bangorprecip.sum(0) precip = float(yearly_totals[yearly_totals == yearly_totals.min()]) year = int(years[yearly_totals == yearly_totals.min()]) print(&quot;The driest year was&quot;, year, &quot;with a total of&quot;, precip, &quot;inches of precipitation.&quot;) . The driest year was 2016 with a total of 34.35 inches of precipitation. . 3. What are the yearly precipitation means? . averages = bangorprecip.mean(0) %matplotlib inline plt.style.use(&#39;ggplot&#39;) plt.bar(years, averages) plt.title(&quot;Average yearly precipitation&quot;) plt.ylabel(&quot;Inches&quot;) plt.show() . 4. What are the monthly min, mean, and max values over the ten years? . mins = bangorprecip.min(1) means = bangorprecip.mean(1) maxs = bangorprecip.max(1) %matplotlib inline plt.style.use(&#39;ggplot&#39;) plt.bar(months, mins, alpha = 0.8) plt.bar(months, means, alpha = 0.6) plt.bar(months, maxs, alpha = 0.4) plt.title(&quot;Monthly precipitation&quot;) plt.ylabel(&quot;Inches&quot;) plt.legend([&quot;min&quot;, &quot;mean&quot;, &quot;max&quot;]) plt.show() . 5. What was the smallest monthly precipitation value and in which month and year did this occur? . yearly_mins = bangorprecip.min(0) monthly_mins = bangorprecip.min(1) year = int(years[yearly_mins == yearly_mins.min()]) month = int(months[monthly_mins == monthly_mins.min()]) min_precip = bangorprecip.min(1).min() print(&quot;The smallest monthly precipitation was &quot;, min_precip, &quot; inches and it occured during &quot;, month,&quot;/&quot;,year, &quot;.&quot;, sep = &quot;&quot;) . The smallest monthly precipitation was 0.58 inches and it occured during 7/2012. . 6. How many months had precipitation amounts greater than 5 inches? . answer = np.sum(bangorprecip &gt; 5) print(answer, &quot;months had precitipation amounts greater than 5 inches.&quot;) . 26 months had precitipation amounts greater than 5 inches. . 7. How many months had precipitation greater than zero and less than 1.5 inches? What were these values and in what months and years did they occur? . answer = np.logical_and([bangorprecip &gt; 0], [bangorprecip &lt; 1.5]) print(np.sum(answer), &quot;months had precipitation greater than 0 and less than 1.5 inches.&quot;) print(&quot;&quot;) for count,val in enumerate(years): month = months[bangorprecip[:,count] &lt; 1.5] values = bangorprecip[:,2][bangorprecip[:,count] &lt; 1.5] if sum(values) != 0: print(&quot;In&quot;, years[count], &quot;, month(s)&quot;, month, &quot;had rainfalls of&quot;, values, &quot;, respectively.&quot;); . 9 months had precipitation greater than 0 and less than 1.5 inches. In 2012 , month(s) [ 3 7 11] had rainfalls of [1.4 0.58 1.13] , respectively. In 2013 , month(s) [ 1 10] had rainfalls of [1.95 6.96] , respectively. In 2014 , month(s) [9] had rainfalls of [6.33] , respectively. In 2015 , month(s) [3 7] had rainfalls of [1.4 0.58] , respectively. In 2016 , month(s) [9] had rainfalls of [6.33] , respectively. . 8. How different were monthly precipitation values in 2019 from 2018? . nineteen = np.concatenate(bangorprecip[:,years == &#39;2019&#39;]) eighteen = np.concatenate(bangorprecip[:,years == &#39;2018&#39;]) %matplotlib inline plt.style.use(&#39;ggplot&#39;) plt.bar(months, nineteen, alpha = 0.7) plt.bar(months, eighteen, alpha = 0.7) plt.title(&quot;Monthly precipitation (2018 vs. 2019)&quot;) plt.ylabel(&quot;Inches&quot;) plt.legend([&quot;2019&quot;, &quot;2018&quot;]) plt.show() . 9. Create a heatmap of the 12 x 10 array . %matplotlib inline plt.style.use(&#39;ggplot&#39;) imgplot = plt.imshow(bangorprecip, extent=[2010,2019,12,1], aspect=&#39;auto&#39;, cmap=&#39;viridis&#39;) plt.colorbar(); . For the data and other notebooks, see github.com/antoniojurlina/learning_python. .",
            "url": "https://antoniojurlina.github.io/portfolio/learning/python/2021/02/07/working-with-numpy.html",
            "relUrl": "/learning/python/2021/02/07/working-with-numpy.html",
            "date": " • Feb 7, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Working with Data Structures",
            "content": "import csv import os os.chdir(&#39;/Users/antoniojurlina/Projects/learning_python/data/&#39;) file_location = &quot;MaineCovidData.txt&quot; countycases = [] with open(file_location) as co: co_reader = csv.reader(co, delimiter=&#39; t&#39;) header=next(co_reader) for line in co_reader: countycases.append(line) county = [countycases[x] for x in range(0, 80, 5)] county = [item for l in county for item in l] county = [i.replace(&#39; County&#39;, &#39;&#39;) for i in county] cases = [countycases[x] for x in range(1, 80, 5)] cases = [item for l in cases for item in l] cases = [int(i.replace(&#39;,&#39;, &#39;&#39;)) for i in cases] cases_new = [countycases[x] for x in range(2, 80, 5)] cases_new = [item for l in cases_new for item in l] cases_new = [int(i.replace(&#39;+&#39;, &#39;&#39;)) for i in cases_new] deaths = [countycases[x] for x in range(3, 80, 5)] deaths = [item for l in deaths for item in l] deaths = [int(i.replace(&#39;,&#39;, &#39;&#39;)) for i in deaths] deaths_new = [countycases[x] for x in range(4, 80, 5)] deaths_new = [item for l in deaths_new for item in l] deaths_new = [int(i.replace(&#39;+&#39;, &#39;&#39;)) for i in deaths_new] cases_updated = [a + b for a, b in zip(cases, cases_new)] deaths_updated = [a + b for a, b in zip(deaths, deaths_new)] cases_by_county = dict(zip(county, cases)) cases_by_county_updated = dict(zip(county, cases_updated)) deaths_by_county_updated = dict(zip(county, deaths_updated)) cases_deaths_by_county = dict(zip(county, (zip(cases_updated, deaths_updated)))) . Bar chart of the second dictionary showing the cases updated with the increment: . import matplotlib.pyplot as plt res = dict(reversed(list(cases_by_county_updated.items()))) %matplotlib inline plt.style.use(&#39;ggplot&#39;) x_pos = [i for i, _ in enumerate(res.keys())] plt.bar(x_pos, res.values(), color=&#39;orange&#39;) plt.xlabel(&quot;County&quot;) plt.ylabel(&quot;Cases&quot;) plt.title(&quot;Maine Covid Cases by County&quot;) plt.xticks(x_pos, res.keys(), rotation =90) plt.show() . Bar chart that shows the deaths updated with the daily increment: . res = dict(reversed(list(deaths_by_county_updated.items()))) %matplotlib inline plt.style.use(&#39;ggplot&#39;) x_pos = [i for i, _ in enumerate(res.keys())] plt.bar(x_pos, res.values(), color=&#39;red&#39;) plt.xlabel(&quot;County&quot;) plt.ylabel(&quot;Deaths&quot;) plt.title(&quot;Maine Covid Deaths by County&quot;) plt.xticks(x_pos, res.keys(), rotation =90) plt.show() . For the data and other notebooks, see github.com/antoniojurlina/learning_python. .",
            "url": "https://antoniojurlina.github.io/portfolio/learning/python/2021/01/29/working-with-data-structures.html",
            "relUrl": "/learning/python/2021/01/29/working-with-data-structures.html",
            "date": " • Jan 29, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Predictive Power - Kriging and Machine Learning",
            "content": ". 1. Objective . The purpose of this project is to assess the predictive power of kriging (along with a generalized additive model) relative to that of a simple machine learning method like Random Forests, with ordinary least squares serving as the simplest (and most biased) approach. Furthermore, this project is specifically interested in the effect geographic location has on expected earnings for cab drivers in Manhattan. If drivers pick passengers up in downtown Manhattan, they will earn a higher wage on average, everything else equal, implying that geographic latitude is the most relevant factor when determining where to start. This is not a project that seeks to determine which specific variable contributes the most (on average with everything else held equal) to the final fares earned by cab drivers. Rather, it seeks to discover relevant relationship between latitude, longitude and fares collected. All other (known) effects are going to be accounted for and stripped away. . . Figure 1 - Spatial distribution of data . Table 1. First 6 (out of 50,000) rows of the data . log(total earnings) longitude latitude passenger count trip distance wday hour month geometry . 1 | 3.23 | -74 | 40.7 | 1 | 5.8 | Wed | 0 | May | (-73.98457 40.71474) | . 2 | 1.7 | -74 | 40.7 | 1 | 0.75 | Thu | 12 | Aug | (-73.99466 40.74018) | . 3 | 2.2 | -74 | 40.8 | 1 | 0.6 | Thu | 9 | Feb | (-73.96581 40.75857) | . 4 | 2.01 | -74 | 40.8 | 2 | 0.9 | Sat | 20 | Feb | (-73.9655 40.79076) | . 5 | 2.56 | -74 | 40.7 | 6 | 2.8 | Sun | 2 | Oct | (-73.9968 40.74702) | . 6 | 2.31 | -74 | 40.8 | 1 | 1.83 | Mon | 1 | Feb | (-73.9706 40.76213) | . . 2. Data . The data set used for this project comes from the “FOILing NYC’s Taxi Trip Data” project1. Chris Whong published 20 gigabytes of NYC Taxi data which includes fares, tips, pickup and drop-off latitudes and longitudes, medallion IDs, passenger numbers, trip durations, and more (Table 1). The sheer size of the data set (more than a 160 million observations over 2013 and 2014) meant that it would be computationally too expensive trying to process it locally. Therefore, a subset of 50,000 observations was randomly sampled, with each month of each year providing a roughly equal share of data points. This hopefully preserved any temporal effects that the data might have. Relevant visualization statistics are included in Figures 1 and 2. There was no missing data and trips with zero total fares earned were excluded (as they represent extremely unlikely and illegal scenarios usually). . . Table 2. Summary statistics of non-categorical variables . log(total earnings) passenger count trip distance . Min | 1.099 | 1 | 0 | . 1st Quartile | 2.079 | 1 | 1 | . Median | 2.398 | 1 | 1.735 | . Mean | 2.447 | 1.717 | 2.545 | . 3rd Quartile | 2.755 | 2 | 2.978 | . Max | 5.08 | 6 | 53 | . Figure 2 - Temporal breakdown of the dependent variable . . 3. Methods . A simple OLS approach, in which the variable of interest is the log of total earnings (sum of the fare, tip, surcharge and tax), is the baseline approach of this project. Independent variables are pickup latitude, pickup longitude, trip distance, and number of passengers. Next, a GAM is estimated over the same basic formula, but with smoothing applied to latitude, longitude and trip distance (with an assumption that the exact shape of the effect these variables have on final earnings is unknown). The use of the GAM (to be followed with kriging of residuals) comes from a very similar idea Gámez et al.2 had in their work on estimating housing prices in Albacete by incorporating the neighborhood effects through spatial autocorrelation estimation and subsequent kriging of GAM residuals. In Manhattan, the assumption is that downtown pickups result in higher earnings. This indicates a certain weight neighboring points will have on each other as an isotropic spatial autocorrelation process. Therefore, the residuals of the GAM are expected to be biased, and kriging will be performed to account for and fix this. Kriging itself has a random component to it, as the underlying Gaussian process is used to interpolate predictions3 4 5, which is what inspired the final aspect of this project, Random Forests. . . log(Y)=α∗Xlog(Y)= alpha * Xlog(Y)=α∗X . Note: α alphaα is a single-column matrix of parameter values to be estimated, X is a n:1+m matrix of independent variables where n is the number of observations in data and m is the number of variables . . Much like kriging, Random Forest models (as the name implies) rely on an underlying random process and are used in estimating spatial models and making predictions6 7. In this project, a Random Forest model is used to estimate Equation 1, by growing 500 random decision trees across the data space and averaging them out into a prediction set. For the purposes of training the three models and subsequently testing them, the data is split into two subsets: first one contains a random subset of 75% of the data for training the models, while the second contains the remaining 25% for testing the predictions and estimating root mean square errors (Figure 3). These two datasets are used for the OLS, the GAM and kriging combination (with a spherical effect variogram model of the spatial dependence), and the Random Forest. . . Figure 3 - Test vs. train data set comparison . . 4. Results and Discussion . Residuals of the OLS and GAM processes were tested for residual normality, with both rejecting the null hypotheses (Table 3). After kriging of the residuals from the GAM was performed, to account for spatial dependence, initial estimates were adjusted with kriging results and after performing the same tests, residuals testing failed to reject the normality hypothesis (Table 3). Final estimates are shown in Table 4. Additionally, the variogram model for the residuals of the GAM is shown in Figure 4. Reduction of the data set down to a smaller subsample, as well as the narrow nature of Manhattan preventing higher horizontal variation in the data, is a possible explanation for the pronounced nugget effect with the cyclical trailing-off pattern in the variogram. . . Table 3. Tests for residual normality . model statistic p.value test . OLS | 80898 | 0.000 | Jarque Berra | . GAM | 7195.7 | 0.013 | Jarque Berra | . GAM+Krige | 1889.1 | 0.135 | Jarque Berra | . OLS | 0.88984 | 0.000 | Shapiro-Wilk | . GAM | 0.91051 | 0.014 | Shapiro-Wilk | . GAM+Krige | 1.0134 | 0.091 | Shapiro-Wilk | . Figure 4 - Spherical variogram fit on GAM residuals . . Looking at Table 4, having completed all the necessary tests, it can be noted that between two coordinate points, latitude is the relevant one in predicting total wage earned from taxi rides. By no means are these the most important factors, but the original hypothesis remains unrejected with these results. There is a strong case to be made that being uptown versus downtown is what matters more than being on the west or east side of the island. The exact effect of the relationship is quite small in the end, and not worth elaborating on more, as the goal of this project has been accomplished. . . Table 4. Model estimates . OLS OLS OLS OLS GAM GAM GAM GAM . term | estimate | std.error | statistic | p.value | estimate | std.error | statistic | p.value | . intercept | -17 | 66.4 | -0.255 | 0.798 | 2.340 | 0.014 | 169.866 | 0.000 | . pickup longitude | 1.01 | 0.678 | -0.921 | 0.357 | 1.38 | 1.673 | 2.744 | 0.134 | . pickup latitude | -0.669 | 0.519 | -1.29 | 0.185 | -0.564 | 0.321 | 0.086 | 0.002 | . passenger count | 0.010 | 0.006 | 1.68 | 0.093 | -0.001 | 0.004 | -0.193 | 0.847 | . trip distance | 0.148 | 0.003 | 51.5 | 0.000 | 7.664 | 8.511 | 933.442 | 0.000 | . Table 5. Predictive and explanatory power . Model RMSE % Variance Explained . OLS | 0.318 | 71.60 | . GAM+Krige | 0.225 | 85.1 | . RandomForest | 0.252 | 81.31 | . . When it comes to predictions, using the testing subset of the model data, we can see that moving from the OLS, over the GAM, to the Random Forest, there is a decreasing error in being able to predict where highest total wages will be earned (Figure 4; Table 5). It is important to note that the GAM/Krige combination explains most of the variance in the data and has best prediction rates, albeit very close to the Random Forest (see Figure 5 and Appendix for visualizations). The expectation for the project was that the Random Forest model would contain significantly better prediction rates. Kriging is a powerful (and computation heavy) tool for interpolating a random space realization into a usable set of predictions while maintaining the ability to give plausible reasoning behind starting assumptions. While it did outperform the simple machine learning technique, it should be noted that it is computationally much heavier. Were I to use the entire data set with over a 100 million observations, Random Forest prediction rate is still high enough that I would chose it over the computationally demanding kriging process. . . Figure 5 - Maps From left to right: Actual data, GAM+Krige predictions, RandomForest predictions . 5. GitHub repository . For data, code, and similar projects, visit https://github.com/antoniojurlina/spatial_analysis. . Appendix 1: Distributions . GAM+Krige prediction distribution (red) vs. actual data (blue) . Appendix 2: R Code . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 . | #-- packages -- library(tidyverse) library(ggthemes) library(spdep) library(ggmap) library(viridis) library(lubridate) library(gstat) library(sp) library(sf) library(lmtest) library(tseries) library(broom) library(mgcv) library(randomForest) #-- data and directory -- paste0(here::here(), &quot;/final project/data&quot;) %&gt;% setwd() fares &lt;- read_csv(&quot;fares.csv&quot;) trips &lt;- read_csv(&quot;trips.csv&quot;) taxi_data &lt;- trips %&gt;% left_join(fares, by = c(&quot;medallion&quot;, &quot;hack_license&quot;, &quot;vendor_id&quot;, &quot;pickup_datetime&quot;)) %&gt;% select(-hack_license, -vendor_id, -rate_code, -store_and_fwd_flag) %&gt;% filter(total_amount &gt; 0) %&gt;% mutate(total_amount = log(total_amount)) %&gt;% filter(pickup_latitude &gt;= 40.70, pickup_latitude &lt;= 40.83, pickup_longitude &gt;= -74.025, pickup_longitude &lt;= -73.93) %&gt;% mutate(hour = hour(pickup_datetime), wday = wday(pickup_datetime, label = TRUE), month = month(pickup_datetime, label = TRUE)) %&gt;% select(total_amount, pickup_longitude, pickup_latitude, passenger_count, trip_distance, wday, hour, month) sample_rows &lt;- sample(nrow(taxi_data), 0.75 * nrow(taxi_data)) train &lt;- taxi_data[sample_rows, ] test &lt;- taxi_data[-sample_rows, ] train_sf &lt;- SpatialPointsDataFrame(coords = train[, 2:3], data = train, proj4string = CRS(&quot;+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0&quot;)) test_sf &lt;- SpatialPointsDataFrame(coords = test[, 2:3], data = test, proj4string = CRS(&quot;+proj=longlat +datum=WGS84 +ellps=WGS84 +towgs84=0,0,0&quot;)) train_sf &lt;- st_as_sf(train_sf) test_sf &lt;- st_as_sf(test_sf) manhattan &lt;- readRDS(&quot;manhattan.rds&quot;) #-- visualization and summary-- ggmap(manhattan, darken = 0.5) + scale_fill_viridis(option = &#39;plasma&#39;) + geom_bin2d(data = taxi_data, aes(pickup_longitude, pickup_latitude), bins = 50, alpha = 0.6) + labs(x = &quot;longitude&quot;, y = &quot;latitude&quot;, fill = &quot;count&quot;) taxi_data %&gt;% ggplot(aes(hour, total_amount, color = wday)) + geom_boxplot() + facet_wrap(~month) + theme_linedraw() bind_rows(test %&gt;% mutate(type = &quot;test&quot;), train %&gt;% mutate(type = &quot;train&quot;)) %&gt;% ggplot(aes(type, total_amount)) + geom_boxplot(fill = &quot;gold&quot;, color = &quot;black&quot;) + stat_summary(fun=mean, geom=&quot;point&quot;, shape = 21, size = 3, fill = &quot;white&quot;) + theme_par() + theme(axis.title.x = element_blank()) #-- OLS -- formula &lt;- as.formula(total_amount ~ pickup_longitude + pickup_latitude + passenger_count + trip_distance + wday + hour + month) model &lt;- glm(formula, family = &quot;gaussian&quot;, train) test$pred &lt;- predict(model, newdata = test) test %&gt;% mutate(resid = total_amount - pred) %&gt;% summarize(rmse = sqrt(mean(resid^2))) -&gt; rmse_ols #-- GAM -- gam &lt;- gam(total_amount ~ s(pickup_longitude) + s(pickup_latitude) + passenger_count + s(trip_distance) + wday + hour + month, family = &quot;gaussian&quot;, data = train) cbind(fitted(gam), residuals(gam)) %&gt;% as_tibble() %&gt;% ggplot(aes(V1, V2)) + geom_point() # data with residuals train_sf$resid &lt;- residuals(gam) test_sf$pred &lt;- predict(gam, newdata = test) test_sf %&gt;% mutate(resid = total_amount - pred) %&gt;% summarize(rmse = sqrt(mean(resid^2))) -&gt; rmse_gam #-- Krige -- set.seed(1234) # variograms taxi_variogram &lt;- variogram(resid ~ 1, data = select(train_sf, -total_amount), cutoff = 10) plot(taxi_variogram) spherical_fit &lt;- fit.variogram(taxi_variogram, vgm(0.05, &quot;Sph&quot;, 3, 0.04)) plot(taxi_variogram, spherical_fit) # ordinary kriging train_sp &lt;- as_Spatial(select(train_sf, -total_amount)) #prediction_grid &lt;- spsample(train_sp, nrow(train_sf), type = &quot;regular&quot;) test_sp &lt;- as_Spatial(select(test_sf, -pred)) spherical_krige &lt;- krige(formula = resid ~ 1, locations = train_sp, model = spherical_fit, newdata = test_sp, nmax=15) #-- GAM+Krige -- final_data &lt;- bind_cols(test_sf, spherical_krige$var1.pred, spherical_krige$var1.var) %&gt;% rename(&quot;variance&quot; = &quot;...12&quot;) %&gt;% mutate(pred = pred + ...11) %&gt;% select(- ...11) final_data %&gt;% mutate(resid = pred - total_amount) %&gt;% summarize(rmse = sqrt(mean(resid^2))) -&gt; rmse_gam_krige #-- randomForests -- rf_model &lt;- randomForest(total_amount ~ ., data = train, ntree = 500) test$pred &lt;- predict(rf_model, newdata = test, type = &quot;class&quot;) ggplot(test, aes(pickup_longitude, pickup_latitude, color = exp(pred), alpha = pred))+ geom_point(size = 2, show.legend = T) + scale_color_gradient_tableau() + coord_equal() + theme_par() + labs(fill = &quot;total earnings&quot;, alpha = &quot;log(total earnings)&quot;, subtitle = &quot;Manhattan&quot;, x = &quot;longitude&quot;, y = &quot;latitude&quot;) test %&gt;% mutate(residual = pred - total_amount) %&gt;% summarize(rmse = sqrt(mean(residual^2))) -&gt; rmse_rf #-- plots + tests -- jarque.bera.test(residuals(model)) jarque.bera.test(residuals(gam)) final_data %&gt;% mutate(resid = pred - total_amount) %&gt;% as_tibble() %&gt;% select(resid) %&gt;% pull() %&gt;% jarque.bera.test() shapiro.test(residuals(model)) shapiro.test(residuals(gam)) final_data %&gt;% mutate(resid = pred - total_amount) %&gt;% as_tibble() %&gt;% select(resid) %&gt;% pull() %&gt;% shapiro.test() final_data %&gt;% ggplot(aes(pickup_longitude, pickup_latitude)) + geom_point(aes(color=exp(pred), alpha = pred), show.legend = TRUE) + coord_equal() + scale_color_gradient_tableau() + theme_par() + labs(fill = &quot;total earnings&quot;, alpha = &quot;log(total earnings)&quot;, subtitle = &quot;Manhattan&quot;, x = &quot;longitude&quot;, y = &quot;latitude&quot;) ggplot(final_data, aes(pickup_longitude, pickup_latitude, color = exp(total_amount), alpha = total_amount))+ geom_point(size = 2, show.legend = T) + scale_color_gradient_tableau() + coord_equal() + theme_par() + labs(fill = &quot;total earnings&quot;, alpha = &quot;log(total earnings)&quot;, subtitle = &quot;Manhattan&quot;, x = &quot;longitude&quot;, y = &quot;latitude&quot;) ggplot(final_data, aes(pickup_longitude, pickup_latitude, color = exp(variance)))+ geom_point(size = 2, show.legend = T) + scale_color_gradient_tableau() + coord_equal() + theme_map() + labs(fill = &quot;fare&quot;) + theme(legend.position = &quot;top&quot;) ggplot(final_data) + geom_histogram(aes(exp(total_amount)), bins = 100, color = &quot;royalblue&quot;, fill = &quot;royalblue&quot;) + geom_histogram(aes(exp(pred)), bins = 100, alpha = 0.6, color = &quot;red&quot;, fill = &quot;red&quot;) + theme_minimal() + labs(y = &quot;&quot;, x = &quot;total earnings&quot;) . | . Works Cited . Whong, C. (2014, March 18). FOILing NYC’s Taxi Trip Data. https://chriswhong.com/open-data/foil_nyc_taxi/ &#8617; . | Gámez, M., Montero, J., &amp; Rubio, N. (2000). Kriging methodology for regional economic analysis: Estimating the housing price in Albacete. International Advances in Economic Research, 6, 438–450. https://doi.org/10.1007/BF02294963 &#8617; . | Krige, D. G. (1951). A statistical approach to some mine valuation and allied problems on the Witwatersrand [Thesis]. http://wiredspace.wits.ac.za/handle/10539/17975 &#8617; . | Matheron, G. (1973). The Intrinsic Random Functions and Their Applications. Advances in Applied Probability, 5(3), 439–468. https://doi.org/10.2307/1425829 &#8617; . | Matheron, Georges. (1963). Principles of geostatistics. Economic Geology, 58(8), 1246–1266. https://doi.org/10.2113/gsecongeo.58.8.1246 &#8617; . | Breiman, L. (2001). Random Forests. Machine Learning, 45(1), 5–32. https://doi.org/10.1023/A:1010933404324 &#8617; . | Tin Kam Ho. (1998). The random subspace method for constructing decision forests. IEEE Transactions on Pattern Analysis and Machine Intelligence, 20(8), 832–844. https://doi.org/10.1109/34.709601 &#8617; . |",
            "url": "https://antoniojurlina.github.io/portfolio/projects/r/2020/12/04/kriging-and-machine-learning.html",
            "relUrl": "/projects/r/2020/12/04/kriging-and-machine-learning.html",
            "date": " • Dec 4, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "COVID-19 and Maine Gas Tax",
            "content": ". Summary . This report covers the attempt to estimate the effect of COVID-19 pandemic on gasoline tax revenue in Maine. Around the world, most economies have responded with lockdowns of varying degrees, in order to curtail the spread of the virus. Following this pattern, the state of Maine has imposed mandatory closures of non-essential businesses and reduced access to public areas. An inevitable outcome of this approach has been a significant reduction in traffic levels. Given that most activities require a certain amount of motor vehicle travel, these closures resulted in less distance being traveled and therefore less gasoline being purchased. For the past decade, the state has relied on a steady revenue stream from the tax on gasoline. This has accounted for nearly 70% percent of the Highway Fund and around 5% of the total tax revenue for a given fiscal1 year. . In order to predict the expected losses of decreased traffic volume this reported is broken up into three steps. First, a seasonal ARIMA model is used in order to forecast the general revenue trend the state would have expected to see, had there been no major shock to the economy. Second, the relationship between traffic volume and gasoline tax revenue is estimated through a linear regression model and the expected revenue is predicted using known traffic data. Finally, the entire model is tested within the last fiscal year with available data (which is fiscal year 2019). This allows for comparison of values calculated within steps one and two with actual data. . Note: This project was done at the request of Maine government with an expectation of certain data privacy regarding tax revenues. Therefore, I have removed some values from the figures and omitted the entire part of the project where predictions are made for the fiscal year 2020 expected tax revenue. What’s shown is the process of determining the expected trend using seasonal ARIMA and an OLS model that determines the relationship between gasoline and tax revenue. These are used later on for future projections. I tested the model against known (2019) data and it holds up really well. . Step 1: Forecasting . Looking at Figure 1, which shows monthly gasoline tax revenue between 2007 and 2018, several issues need to be addressed before any modelling is to occur. Within the red boxes are monthly values that drastically diverge from the expected trend and the observed mean. This series should be considered a proxy for the real variable of interest. The actual information we could say the state is interested in is the amount of revenue generated each month. However, the state is aware only of the taxes it actually collects, when it collects them. This means that collection processes and lags between the point in time when revenue is generated and when it is collected, need to be taken into account. With this in mind, these points are deemed outliers and reverted to the mean of the remainder of the series. . . Figure 1 - Gas Tax Revenue Jan 2007 – Jun 2018 . . Furthermore, there is clear seasonality in the data. Each fiscal year resembles the one before. Also, there is a slight upward trend in the data: although barely discernable, it is still worth noting. An augmented Dickey-Fuller (ADF) test, one that accounts for seasonality with a trend, failed to reject the presence of unit roots with a statistic of -1.04. To detrend the data, remove the seasonal pattern and lose the unit root issue, the series was differenced monthly and seasonally. That is, each month’s revenue was subtracted from the one a month away and a year away. Running an ADF test on this series, gives the statistic of -6.79 which strongly indicates no more unit root issues. The final series, which now appears to be stationary, can be seen in Figure 2. . . Figure 2 - Cleaned-up Gas Tax Revenue Jan 2007 – Jun 2018 . . Figure 3 shows what the autocorrelations (both partial and complete) look like for the series. Lines colored red represent the seasonal lags (at 12, 24, etc. to account for the monthly nature of the series). Looking at the complete autocorrelation plot (ACF), there is no significant tailing off within a season. However, two red lines above the line of significance suggest that it would be wise to include a seasonal autoregressive term. Looking at the partial autocorrelation plot (PACF), there is some tailing off within a season and no seasonal pattern of significance, suggesting that a moving average term should be added. . . Figure 3 - ACF (top) and PACF (bottom), seasonal lags in red . . With this in mind (and a few minor alterations around each term), the model with the lowest reported AIC and BIC is SARIMA(0,1,1)(1,1,0)12(0,1,1)(1,1,0)_{12}(0,1,1)(1,1,0)12​ or . (1−ϕ1B12)(1−B)(1−B12)Yt=(1−θ1B)εt(1- phi_1B^{12})(1-B)(1-B^{12})Y_t=(1- theta_1B) varepsilon_t(1−ϕ1​B12)(1−B)(1−B12)Yt​=(1−θ1​B)εt​ . where the moving average is 1, seasonal autoregressive lag is 1 and the data has been differenced once monthly and once seasonally. As can be noted by Figure 4, residuals seem to be distributed randomly and independently. Table 1 reports the estimates of the model. Finally, Figure 5 shows what the expected forecast 12 months ahead is. This forecast covers fiscal year 2019, for which data is available. . . Figure 4 - Analysis of model residuals . Table 1. SARIMA estimation results . estimate std. error t.statistic p.value . ma1 | -0.8469 | 0.0557 | -15.218 | 0.0000 | . sar1 | -0.3528 | 0.0903 | -3.9064 | 0.0002 | . Figure 5 - Forecast for 2019 fiscal year . . Step 2: Revenue vs. Traffic . Plotting traffic and revenue over time, on the same plot (as can be seen in Figure 7), reveals a few interesting things. They both follow the same seasonal pattern over time (with a month lag between the two) and both have quarterly trends that can be individually considered. First of all, the points are separated into quarterly sets and then shifted over by a month so that the highest and lowest points of each series sync up. This lag is assumed to exist because it must take the state some time (presumably a month) to collect the tax after the traffic levels have been observed. . . Figure 6 - OLS estimates . Table 2. OLS estimates . Quarter 1 Quarter 2 Quarter 3 Quarter 4 . intercept | 8.168*** (0.667) | 10.616*** (2.63) | 2.559* (2.93) | 7.744* (3.689) | . log(traffic) | 0.539*** (0.042) | 0.383** (0.167) | 0.899*** (0.188) | 0.561** (0.234) | . * - 90 % significance / ** - 95 % significance / *** - 99 % significance . . Now, as they are synced up, quarterly slope signs of each series are identical and therefore a straightforward linear regression should be all that is necessary. However, to further capture the possible quarterly idiosyncrasy of the data and further reduce variation, regressions are performed within each quarter. Additionally, future data for state traffic levels and tax revenue used for economic impact analysis of COVID-19 closures will concern the last quarter of fiscal year 2020, starting with April. Therefore, it seemed prudent to isolate quarterly effects in order to obtain the most accurate estimates. . Figure 6 and Table 2 show the results of the linear regression. As can be noted, the relationships are fairly straightforward and statistically significant. For each 1% increase in traffic volume, there is 54, 38, 90, and 56 percent increase in revenue for quarters 1 through 4, respectively. This data can now be used to estimate revenue from traffic. In this case, revenue is simply . . revenue = intercept + coefficient * traffic . . and traffic volume is obtained from monthly turnpike data across Maine. . . Figure 7 - Organizing traffic (bottom) and revenue (top) data . . Step 3: Putting it all together . This one is straightforward, as Figures 8 and 9 illustrate. Fiscal year 2019 contains three lines: actual revenue (yellow), OLS predicted revenue (green) and time series forecast of expected revenue (red). Furthermore, Table 3 shows the difference between expected and predicted revenue. For fiscal year 2019, this doesn’t provide new information, because there were no unexpected shocks, and all the models arrive at the same conclusion (mostly). However, the purpose of running the model across 2019 was only to determine its validity. Matching results were expected. Fiscal year 2020 is where it is actually going to be used to make predictions. . . Table 3. Tax revenue comparison for fiscal year 2019, in US$ . OLS OLS OLS SARIMA SARIMA SARIMA . | Actual Data | Min | Average | Max | Min | Average | Max | . Quarter 1 | 58,311,032 | 56,724,305 | 59,275,087 | 61,825,870 | 55,836,036 | 58,001,488 | 60,166,940 | . Quarter 2 | 51,606,718 | 44,014,749 | 52,863,879 | 61,713,010 | 49,853,379 | 52,084,273 | 54,315,167 | . Quarter 3 | 47,028,837 | 40,009,428 | 49,259,003 | 58,508,579 | 45,395,921 | 47,652,095 | 49,908,270 | . Quarter 4 | 50,225,926 | 36,792,179 | 48,887,326 | 60,982,473 | 47,774,515 | 50,034,520 | 52,294,526 | . Figure 8 - Tax revenue data (yellow), OLS prediction (green), SARIMA forecast (red); shaded region represents 99% CI . Figure 9 - Tax revenue data (yellow), OLS prediction (green), SARIMA forecast (red); shaded region represents 99% CI, (close up of 2019 fiscal year) . . GitHub repository . For data, code, and similar projects, visit https://github.com/antoniojurlina/time_series_econometrics. . Appendix: Data Analysis Code in R . 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 . | #-- packages -- library(tidyverse) library(ggplot2) library(lubridate) library(directlabels) library(ggrepel) library(ggthemes) library(scales) library(xts) library(astsa) library(forecast) library(tseries) library(broom) #-- data and directory -- directory &lt;- paste0(here::here(), &quot;/data&quot;) setwd(directory) load(&quot;traffic_breakdown.RData&quot;) load(&quot;traffic_summary.RData&quot;) load(&quot;class.RData&quot;) load(&quot;comparison1.RData&quot;) load(&quot;comparison2.RData&quot;) load(&quot;interchange.RData&quot;) load(&quot;tax_revenues.RData&quot;) load(&quot;unemployment.RData&quot;) directory &lt;- paste0(here::here(), &quot;/output&quot;) setwd(directory) options(scipen = &quot;100&quot;) #-- functions -- linear_reg &lt;- function(data, formula){ lm(formula, data) } #-- analysis -- gas &lt;- tax_revenues %&gt;% filter(revenue != 0, !is.na(revenue)) %&gt;% filter(code == &quot;0321&quot;) %&gt;% filter(date &lt; &quot;2020-01-01&quot;, date &gt;= &quot;2007-01-01&quot;) %&gt;% filter(fiscal_year &lt; 2019) %&gt;% group_by(date) %&gt;% summarise(revenue = sum(revenue, na.rm = TRUE)) %&gt;% ungroup() gas &lt;- gas %&gt;% mutate(revenue = ifelse(revenue &gt; 22000000, NA, revenue), revenue = ifelse(revenue &lt; 13000000, NA, revenue), revenue = ifelse(is.na(revenue), mean(revenue, na.rm = T), revenue)) gas &lt;- xts(gas$revenue, order.by = gas$date) lgas = log(gas) dgas &lt;- na.locf(diff(gas), fromLast = T) ddgas &lt;- na.locf(diff(dgas, lag = 12), fromLast = T) data &lt;- ddgas length(data) adf.test(data) plot(data) acf2(data, max.lag = 100, main = &#39;&#39;) sarima(gas, 0, 1, 1, 1, 1, 0, 12) sarima.for(gas, n.ahead = 12, 0, 1, 1, 1, 1, 0, 12) auto.arima(gas, trace = TRUE, ic = &quot;bic&quot;, seasonal = TRUE, seasonal.test = &quot;ocsb&quot;) ols &lt;- tax_revenues %&gt;% filter(code == &quot;0321&quot;, fiscal_year %in% c(2016:2019)) %&gt;% filter(revenue != 0, !is.na(revenue)) %&gt;% filter(date &lt; &quot;2020-04-01&quot;) %&gt;% left_join(comparison_2 %&gt;% group_by(date) %&gt;% summarise(traffic = sum(transactions, na.rm = TRUE)) %&gt;% mutate(month = month(date) + 1, year = year(date), year = if_else(month &gt; 12, year + 1, year), month = if_else(month &gt; 12, month - 12, month), date_shifted = ymd(paste0(year, &quot;-&quot;, month, &quot;-01&quot;))), by = &quot;date&quot;) plot &lt;- ols %&gt;% ggplot(aes(x = date, y = revenue, color = quarter)) + geom_line() + geom_point() + geom_line(aes(y = lag(traffic, 1))) + geom_point(aes(y = lag(traffic, 1))) + facet_wrap(. ~ fiscal_year, scales = &quot;free_x&quot;, nrow = 1) + scale_color_ptol() + theme_linedraw() + scale_y_continuous(label = comma) + theme(axis.text.x = element_blank(), axis.title.x = element_blank(), axis.text.y = element_blank(), axis.title.y = element_blank(), legend.position = &quot;none&quot;) ggsave(&quot;plot.png&quot;, plot, height = 3, width = 7) ols %&gt;% ggplot(aes(x = log(lag(traffic,1)), y = log(revenue), colour = quarter)) + geom_point() + stat_smooth(method = &quot;lm&quot;) + facet_wrap(. ~ quarter, scales = &quot;free&quot;) + scale_color_ptol() + theme_par() + xlab(&quot;log(traffic)&quot;) + ylab(&quot;log(revenue)&quot;) + theme(legend.position = &quot;none&quot;) ols %&gt;% ggplot(aes(x = lag(traffic,1), y = revenue, colour = quarter)) + geom_point() + stat_smooth(method = &quot;lm&quot;) + scale_y_continuous(label = comma) + scale_x_continuous(label = comma) + facet_wrap(. ~ quarter, scales = &quot;free&quot;) + scale_color_ptol() + theme_par() + xlab(&quot;traffic&quot;) + ylab(&quot;revenue&quot;) + theme(legend.position = &quot;none&quot;) ols_output &lt;- map(c(&quot;Quarter 1&quot;, &quot;Quarter 2&quot;, &quot;Quarter 3&quot;, &quot;Quarter 4&quot;), function(x) { bind_rows( ols%&gt;% filter(quarter == x) %&gt;% linear_reg(log(revenue) ~ log(lag(traffic, 1))) %&gt;% tidy() %&gt;% mutate(type = &quot;log&quot;, quarter = x), ols %&gt;% filter(quarter == x) %&gt;% linear_reg(revenue ~ lag(traffic, 1)) %&gt;% tidy() %&gt;% mutate(type = &quot;normal&quot;, quarter = x) ) }) %&gt;% data.table::rbindlist() ols_output_log &lt;- ols_output %&gt;% filter(type == &quot;log&quot;) ols_output &lt;- ols_output %&gt;% filter(type == &quot;normal&quot;) ols_output &lt;- left_join( ols_output %&gt;% filter(term == &quot;lag(traffic, 1)&quot;) %&gt;% rename(slope = estimate, slope_se = std.error) %&gt;% select(slope, slope_se, quarter), ols_output %&gt;% filter(term != &quot;lag(traffic, 1)&quot;) %&gt;% rename(intercept = estimate, intercept_se = std.error) %&gt;% select(intercept, intercept_se, quarter), by = &quot;quarter&quot;) expectation &lt;- sarima.for(gas, n.ahead = 12, 6, 0, 1, 1, 0, 0, 12) expectation &lt;- tibble(expectation$pred, expectation$se, tax_revenues %&gt;% filter(fiscal_year == 2019) %&gt;% select(date) %&gt;% unique() ) colnames(expectation) &lt;- c(&quot;expectation&quot;, &quot;error&quot;, &quot;date&quot;) prediction &lt;- tax_revenues %&gt;% filter(code == &quot;0321&quot;) %&gt;% filter(revenue != 0, !is.na(revenue)) %&gt;% filter(date &lt; &quot;2020-04-01&quot;) %&gt;% left_join(comparison_2 %&gt;% group_by(date) %&gt;% summarise(traffic = sum(transactions, na.rm = TRUE)) %&gt;% mutate(month = month(date) + 1, year = year(date), year = if_else(month &gt; 12, year + 1, year), month = if_else(month &gt; 12, month - 12, month), date_shifted = ymd(paste0(year, &quot;-&quot;, month, &quot;-01&quot;))), by = &quot;date&quot;) %&gt;% left_join(ols_output, by = &quot;quarter&quot;) %&gt;% left_join(expectation, by = &quot;date&quot;) %&gt;% mutate(traffic = lag(traffic, 1), prediction = ifelse(fiscal_year == 2019, intercept + slope*traffic, NA), prediction_upper = ifelse(fiscal_year == 2019, (intercept + intercept_se) + (slope + slope_se) * traffic, NA), prediction_lower = ifelse(fiscal_year == 2019, (intercept- intercept_se) + (slope - slope_se) * traffic, NA), prediction_error = revenue - prediction, expectation_upper = expectation + error, expectation_lower = expectation - error) prediction %&gt;% filter(fiscal_year == 2019) %&gt;% ggplot(aes(x = date)) + geom_point(aes(y = revenue), colour = &quot;#DDAA33&quot;) + geom_line(aes(y = revenue), colour = &quot;#DDAA33&quot;, size=1.5) + geom_point(aes(y = expectation), colour = &quot;#BB5566&quot;) + geom_line(aes(y = expectation), colour = &quot;#BB5566&quot;, size=1.5) + geom_point(aes(y = prediction), colour = &quot;#228833&quot;) + geom_line(aes(y = prediction), colour = &quot;#228833&quot;, size=1.5) + geom_ribbon(aes(ymin = prediction_lower, ymax =prediction_upper), fill = &quot;#228833&quot;, colour = &quot;transparent&quot;, alpha = 0.1) + geom_ribbon(aes(ymin = expectation_lower, ymax = expectation_upper), fill = &quot;#BB5566&quot;, colour = &quot;transparent&quot;, alpha = 0.3) + scale_y_continuous(label = comma) + theme_par() + theme(axis.title.x = element_blank()) prediction %&gt;% filter(fiscal_year == 2019) %&gt;% group_by(quarter) %&gt;% summarize(revenue = sum(revenue, na.rm = T), prediction = sum(prediction, na.rm = T), prediction_upper = sum(prediction_upper, na.rm = T), prediciton_lower = sum(prediction_lower, na.rm = T), expectation = sum(expectation, na.rm = T), expectation_upper = sum(expectation_upper, na.rm = T), expectation_lower = sum(expectation_lower, na.rm = T)) %&gt;% View() . | . in Maine, fiscal year spans July of last year to June of current year. That is, fiscal year 2018 starts in July 2017 and ends in June 2018. &#8617; . |",
            "url": "https://antoniojurlina.github.io/portfolio/projects/r/2020/05/01/covid-and-gas-tax.html",
            "relUrl": "/projects/r/2020/05/01/covid-and-gas-tax.html",
            "date": " • May 1, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Economic Freedom and Growth",
            "content": ". Summary . The Economic Freedom Index is published yearly by the Fraser Institute. Latest edition is comprised of five areas used to construct a scale of economic freedom, with each area rated on a scale of 1 to 10. In this project, I use this index along with GDP data for 52 countries, ranging between 1970 and 2015, to estimate the relationship between index components and GDP growth. I begin by running a preliminary OLS panel model, followed by several statistical tests designed to probe for the presence of multicollinearity, autocorrelation, heteroskedastisicty and effectiveness of my proposed approach. Eventually, I settled on a feasible generalized least squares model for panel data. The project concludes with a strong indication that the freedom to trade interantionally and sound money are important predictors of GDP growth. Included, also, is a link to the GitHub repository containing the data I worked with and the EViews files. I chose to work in EViews in order to learn how it works, while I was still builidng my econometrics toolset. After having dealt with everything it has to offer, it is clear to me that there is much better software and statistical languages out there. I would recommend R and Stata for this (and any other similar) project, over EViews. . 1. Review . Adam Smith1, with the publication of the Wealth of the Nations, instigated a debate around the causes of economic growth. Although mercantilism had dominated the period of late Renaissance in Europe and powerful merchants had built routes based on the belief that trade would benefit them greatly2, it wasn’t until Smith’s work had been published that attention turned towards free trade, production advantage, economies of scale and institutions (or lack thereof) intended to orchestrate this in unison. This work was further expanded by Ricardo3, who ushered the realization that benefits can be acquired even by those countries that are not the most efficient suppliers around. Eventually, economic growth was taken up by the likes of Solow4, who had expanded the Harrod-Domar model5 6 so that growth is represented as function of capital, labor and technology (with more optimistic limitations). . Following Solow’s work, Kuznets7 argued that, while necessary, technology itself wouldn’t suffice in producing measurable growth. He claimed that growth would induce change along the way (something along the lines of Schumpeter8), and all the conflict would have to be resolved cost-effectively through institutions designed to do so. Only then, with conflict resolution costs smaller than benefits of growth, would long-term economic progress occur. Finally, there is Milton Friedman9 , crafting institutional approach along more libertarian lines, arguing for a government that is there to promote safety, monopolize violence and influence the economy through the money supply. He also argued for the removal of major trade barriers as the only way to introduce stable equilibria. . More recent work had found that property rights, monetary stability, and freedom to trade internationally all have visible impact on growth 10 11 12 13 14. Additionally, previously underdeveloped, closed-off, and/or countries with centralized economies had all undergone drastic economic changes (in the positive direction) upon loosening institutional grips, opening towards the world and openly stifling hierarchical corruption. This can be observed in the economies of Taiwan, Singapore and Hong Kong, with China following closely upon realizing its neighbors had adopted a slightly more laissez-faire approach and experienced significant growth15. . Market equilibria inside closed economies adjust themselves according to supply and demand interactions and institutional involvement. With constraints effectively placed to incorporate the costs of most inefficiencies, effective resource allocation is determined on aggregate, through interactions of all the individual participants. With current levels of globalization and economic interconnectedness, eliminating quotas and barriers results in a world- wide market place with freer price points. This, much like on a single-country level, is an amalgamation of countless interactions producing an inherent equilibrium. Depending on the institutional restrictions imposed by all the individual players (and their size), this equilibrium will inch towards comparative efficiency, fostering more growth. With the readjustment of the production possibility frontiers to accommodate the new demand and supply pressures, Mundell16 and Fleming17 identify certain factors affecting GDP levels of open-economies: fiscal policy, monetary policy, and foreign trade shifts. Even if Leonteif’s18 observations (the failure of H-O theorem) hold across countries, there is still an adjustment shift according to the world market. . Finally, Gwartney, Lawson, &amp; Block19, have created an index consisting of all these factors influencing growth. The index rates the economic freedom of countries on a scale of 1 to 10, with 10 indicating a country that is completely free economically. The Economic Freedom Index (from here on referred to as EFI), is comprised of separate indices for the size of the government, legal system and property rights, freedom to trade internationally, stability of the monetary policy, and the number of regulatory obstacles. This index, and similar ones (such as the one produced by the Heritage Foundation) have been used in several ways in order to determine a possible causal link between economic freedom and economic growth20 21 22 23 24. . 2. Econometric Model . The EFI is published yearly by the Fraser Institute. Latest edition 25 is comprised of five areas used to construct a scale of economic freedom, with each area rated on a scale of 1 to 10. Size of Government focuses on individual choice-making through market interactions, as opposed to relying on policy making. Countries with low levels of government spending, a smaller government enterprise sector, and lower tax rates earn the highest ratings in this area. Legal System and Property Rights focuses on unbiased judiciary systems, effective protection of private property and impartial enforcement of the law. Countries that satisfy these categories the best, score the highest in this area. Sound Money refers to money with a stable purchasing power over time. Countries that score high in this area, must follow policies and adopt institutions that lead to low rates of inflation and avoid regulations that limit the ability to use alternative currencies. Freedom to Trade Internationally focuses on the level and ease of interactions across the borders. To score high in this area, a country must have “low tariffs, easy clearance and efficient administration of customs, a freely convertible currency, and few controls on the movement of physical and human capital”25. Regulation measures the access into markets and restrictions around economic interactions. To score high in this area, countries need to relax regulatory constraints around labor, product and credit markets. For more detail on the construction of each of these areas, see Appendix 1. . The model under consideration uses these areas as explanatory variables. This should help in determining the causal link, or at least, the sign of the relationship, between economic growth and factors determining classical assumptions around economic freedom. The model is as follows . GROWTHi,(t−5,t)=β0+β1GOVi,t−5+β2LEGALi,t−5+β3MONEYi,t−5+β4TRADEi,t−5+β5REGULATIONi,t−5+β6log(GDP)i,t−5+εi,tGROWTH_{i,(t-5,t)} = beta_0 + beta_1GOV_{i,t-5} + beta_2LEGAL_{i,t-5} + beta_3MONEY_{i,t-5} + beta_4TRADE_{i,t-5} + beta_5REGULATION_{i,t-5} + beta_6log(GDP)_{i,t-5} + varepsilon_{i,t}GROWTHi,(t−5,t)​=β0​+β1​GOVi,t−5​+β2​LEGALi,t−5​+β3​MONEYi,t−5​+β4​TRADEi,t−5​+β5​REGULATIONi,t−5​+β6​log(GDP)i,t−5​+εi,t​ . where the dependent variable represents growth of log GDP per capita over a 5-year period, β1 beta_1β1​ through β5 beta_5β5​ represent the five areas of EFI at the beginning of each 5-year period, β6 beta_6β6​ represents the log of GDP per capita at the beginning of each 5-year period and ε varepsilonε represents the error term. Each of these variables is defined across countries (i) and time (t, t-5), making this a fixed effects model. Country-level heterogeneity carries a lot of unobservable variables, so with fixed effects, the remaining variation can be used to causally identify the relationships of interest. The null hypotheses are that there is no significant effect between the five areas of EFI and growth of GDP per capita. The alternative hypotheses are that the higher a country scores in all five areas of EFI, the higher the log growth of GDP per capita, in accordance with classical assumptions. . 3. Data . The data set used for this project was created using three different sources. The EFI was obtained from the Fraser Institute25, data on GDP per capita was obtained from the World Bank Group26, recorded in 2018 US dollars. GDP was chosen to be per capita specifically to avoid any issues with population size differences among countries. Finally, a dummy variable on whether nations are members of the OECD was created using the list of member nations from the OECD website27. These variables are organized as panel data with 52 countries, ranging from 1970 to 2015. Countries were selected based on data availability, to avoid missing values that would result in omitted observations during estimation. Summary statistics (minimum, maximum, mean, and standard deviation) for all variables can be seen in Appendix 2 and graphs representing these variables, faceted by country, can be seen in Appendix 3. . . Figure 1 - Economic Freedom of the World 2015 Report (The Fraser Institute) . . 4. Preliminary Estimates . The first step of the analysis revolved around estimating an OLS version of the model in Equation 1. Results were estimated in three different ways – by grouping all countries together, by using only OECD member countries and by using only non-member countries. This dummy variable was introduced due to the special economic relationships fostered by OECD member nations, to stabilize unobservable heterogeneity between members and non-members. The use of the dummy variable was also inspired by previous research, especially that of Mankiw, Romer and Weil28. Detailed results are presented in Figure 2. . . Figure 2 - Panel Least Squares Regressions Dependent variable: log growth of GDP per capita (1975 - 2015) . Pooled non-OECD OECD . Intercept | 0.441 (0.034) | 0.372 (0.054) | 0.526 (0.046) | . log GDP per capita | -0.071*** (0.005) | -0.072*** (0.009) | -0.072*** (0.007) | . Size of Government | 0.005 (0.004) | 0.006 (0.006) | 0.002 (0.005) | . Legal System &amp; Property Rights | -0.003 (0.003) | -0.005 (0.005) | -0.002 (0.004) | . Sound Money | -0.004 (0.002) | -0.006** (0.003) | -0.000 (0.004) | . Regulation | 0.024*** (0.005) | 0.024*** (0.008) | 0.025*** (0.007) | . Freedom to Trade Internationally | 0.013*** (0.003) | 0.017*** (0.004) | 0.008*** (0.004) | . Observations | 430 | 189 | 241 | . Cross-sections (periods) | 52 (9) | 24 (9) | 28 (9) | . R2R^2R2 | 0.44 | 0.41 | 0.46 | . F-statistic (p-value) | 5.03 (0.000) | 3.88 (0.000) | 5.4 (0.000) | . Durbin-Watson statistic | 2.67 | 2.41 | 2.92 | . * - 90 % significance / ** - 95 % significance / *** - 99 % significance . . As Figure 2 shows, Freedom to Trade Internationally and Regulation have a significant effect across three model runs, and Sound Money has a significant effect for non-OECD countries only. Any one-point increase in the Freedom to Trade Internationally index is correlated with a 1.3 % (pooled), 1.7 % (non-OECD) and 0.8% (OECD) increase in the growth rate of GDP per capita, on average. Any one-point increase in the Regulation index is correlated with a 2.4 % (pooled), 2.4 % (non-OECD) and 2.5% (OECD) increase in the growth rate of GDP per capita, on average. Finally, any one-point increase in the Sound Money index for non-OECD countries is correlated with a 0.6 % decrease in the growth rate of GDP per capita, on average. To reiterate, an increase in index score across all three of the variables mentioned indicates an increase in economic freedom, as per EFI design. . 5. Testing . Although the results seem significant at first glance, there are many causes for concern regarding the validity of parameter identification in a simple OLS approach to this data set. This section is dedicated to discovering possible violations of Gauss-Markovian assumptions and checking the validity of model design. . Multicollinearity . One of the primary issues with deconstructing an index is the causal relationships between some of the subcomponents. It seems reasonable to assume that size of the government, amount of regulation and property rights are correlated with one another. This can result in multicollinearity among explanatory variables, affecting the robustness of the estimates. The correlation matrix in Figure 3 indicates strong correlation (over 0.5) between the five areas of the EFI. This serves as a rough estimate of multicollinearity present in the model, indicating that estimates need to be interpreted conservatively. For future reference, multicollinearity should be further confirmed by estimating the model and changing the data slightly, many times over, seeing how the estimates react. Also, dependent variables should be dropped, and model estimated without some, to see the effect on estimates. Significant estimate changes in both these approaches would indicate a presence of multicollinearity. Finally, a variance inflation factor should be calculated, as it gives an exact numeric value for evaluation. Since multicollinearity doesn’t change the BLUE properties of the model, and due to time limitations, the model will be left as is. . . Figure 3. Correlation Matrix . log growth of GDP per capita GDP per capita Size of Government Legal System &amp; Property Rights Sound Money Regulation Freedom to Trade Internationally . log growth of GDP per capita | 1.000 | | | | | | | . GDP per capita | -0.089 | 1.000 | | | | | | . Size of Government | 0.042 | -0.162 | 1.000 | | | | | . Legal System &amp; Property Rights | -0.007 | 0.686 | -0.185 | 1.000 | | | | . Sound Money | -0.029 | 0.533 | 0.043 | 0.583 | 1.000 | | | . Regulation | -0.035 | 0.590 | 0.239 | 0.626 | 0.649 | 1.000 | | . Freedom to Trade Internationally | 0.030 | 0.495 | 0.079 | 0.722 | 0.644 | 0.668 | 1.000 | . . Autocorrelation . Since the primary statistical software used in estimation doesn’t allow for direct autocorrelation testing, several indirect ways shall be explored, to detect any potential autocorrelation. First, simply observing graphs of residuals from the three OLS approaches, can be very indicative of any potential autocorrelation. Indeed, by looking at the attached graphs (see Appendix 4), it seems highly possible that autocorrelation is present. The order is harder to discern. Furthermore, Durbin-Watson statistic can be used for identification of first-order autocorrelation. The autocorrelation coefficient, ρ rhoρ, (with stable errors) is located on the interval −1 &lt; ρ rhoρ &lt; 1, and the Durbin-Watson test statistic is approximately equal to 4, 2 and 0, for the ρ rhoρ values of -1, 0, and 1, respectively. Therefore, the d-statistic serves as a rough guide of first order autocorrelation. In the first (pooled) OLS estimate, d is 2.7. Being further from 2 (in the positive direction), indicates that autocorrelation is more likely. The same can be said for the d values of the second (non- OECD) and third (OECD) OLS estimates, for which d values are 2.4 and 2.9, respectively (see Figure 2). Finally, a feasible GLS model is estimated, with the addition of autocorrelation parameters. These are added individually, starting with a parameter for first order autocorrelation, up to the point where their p-values start to become insignificant at conventional confidence levels (see Figure 4). The results indicate a presence of first and second order autocorrelation, with any subsequent level added failing to pass as significant or reducing the number of observations past the optimal point. . . Figure 4. FGLS Regression - Sensitivity Testing for Autocorrelation Dependent variable: log growth of GDP per capita (1975 - 2015) . Pooled non-OECD OECD Pooled non-OECD OECD Pooled non-OECD OECD . Intercept | 0.365 (0.032) | 0.320 (0.059) | 0.430 (0.038) | 0.239 (0.024) | 0.225 (0.049) | 0.286 (0.022) | 0.164 (0.062) | 0.112 (0.092) | 0.240 (0.106) | . log GDP per capita | -0.063*** (0.006) | -0.065*** (0.011) | -0.064*** (0.007) | -0.037*** (0.004) | -0.048*** (0.010) | -0.031*** (0.004) | -0.023*** (0.009) | 0.016 (0.017) | -0.032*** (0.009) | . Size of Government | 0.008 (0.003) | 0.006 (0.006) | 0.009 (0.004) | 0.005* (0.003) | 0.006 (0.005) | 0.005** (0.002) | 0.000 (0.005) | -0.002 (0.008) | 0.003 (0.005) | . Legal System &amp; Property Rights | 0.001 (0.003) | -0.004 (0.006) | 0.006 (0.004) | -0.001 (0.003) | -0.004 (0.005) | 0.000 (0.003) | 0.000 (0.005) | 0.000* (0.007) | 0.000 (0.008) | . Sound Money | -0.004** (0.002) | -0.007** (0.003) | -0.001 (0.003) | -0.009*** (0.002) | -0.010*** (0.003) | -0.006*** (0.002) | -0.008*** (0.003) | -0.006 (0.004) | -0.008* (0.004) | . Regulation | 0.019*** (0.005) | 0.024*** (0.009) | 0.014** (0.006) | 0.009** (0.004) | 0.019** (0.009) | 0.003 (0.003) | 0.003 (0.007) | 0.012 (0.016) | 0.005 (0.006) | . Freedom to Trade Internationally | 0.013*** (0.003) | 0.016*** (0.005) | 0.008*** (0.003) | 0.017*** (0.002) | 0.018*** (0.004) | 0.009*** (0.002) | 0.022*** (0.005) | 0.017* (0.009) | 0.009 (0.006) | . AR(1) | -0.351*** (0.053) | -0.226*** (0.089) | -0.478*** (0.065) | -0.590*** (0.050) | -0.406*** (0.089) | -0.821*** (0.054) | -0.517*** (0.073) | -0.498*** (0.121) | -0.586*** (0.009) | . AR(2) | | | | -0.469*** (0.049) | -0.447*** (0.089) | -0.607*** (0.051) | -0.513*** (0.064) | -0.585*** (0.095) | -0.552*** (0.079) | . AR(5) | | | | | | | -0.089*** (0.049) | -0.262*** (0.100) | 0.008 (0.055) | . Observations | 377 | 165 | 212 | 325 | 141 | 184 | 168 | 69 | 100 | . Cross-section (periods) | 52 (8) | 24 (8) | 28 (8) | 52 (7) | 24 (7) | 28 (7) | 50 (4) | 22 (4) | 28 (4) | . R2R^2R2 | 0.44 | 0.37 | 0.54 | 0.53 | 0.38 | 0.74 | 0.71 | 0.73 | 0.78 | . F-statistic (p-value) | 4.3 (0.000) | 2.66 (0.000) | 6.03 (0.000) | 5.03 (0.000) | 2.17 (0.002) | 11.79 (0.000) | 4.72 (0.000) | 3.49 (0.000) | 6.18 (0.000) | . Durbin-Watson statistic | 2.32 | 2.22 | 2.56 | 1.80 | 2.06 | 1.60 | 2.41 | 2.61 | 2.24 | . Note: This approach assumes there might be first, second and fifth order autocorrelation. The fifth order autocorrelation is assumed because growth is studied through 5-year periods. Sensitivity analysis is looking for AR terms that are significant at standard confidence levels and that produce d-statistics closest to 2. Significance levels are as follows: * - 90 % significance / ** - 95 % significance / *** - 99 % significance. . . Heteroskedasticity . After scouring EViews help pages and related forums and blogposts, I have concluded that the current version of the statistical software just doesn’t provide support when it comes to testing for heteroskedasticity in panel data through direct tests. With that in mind, there were two options left for attempting to detect possible heteroskedasticity in the data. First approach is a common-sense (backed by econometrics textbooks29 30) approach, that assumes there is high probability of heteroskedastic errors occurring in cross-sectional data. This seems intuitively reasonable as well – countries vary greatly in GDP per capita and economic freedom measures, indicating a strong possibility of errors having varying degrees of statistical dispersion. Furthermore, Figure 5 plots residuals against fitted values, for the three OLS estimates (pooled, non-OECD, and OECD). These graphs indicate that errors are indeed not uniformly dispersed and that heteroskedasticity is likely present between cross-sections (i.e. countries). Finally, plots showing within-country fitted values and residuals aren’t feasible since there are only 10 periods under consideration meaning that there aren’t enough points to visually estimate the shape of error dispersion. . . Figure 5 - Residuals versus Fitted Values plots . . Redundant Fixed Effects . Figure 6 shows the results of redundant fixed effects tests, performed on the three OLS models. With future revised estimation in mind, the tests were completed for fixed cross-sectional effects, fixed period effects, and both. In each case, across all the model version, tests confirm that all model specifications are supported, with significant p-values across multiple tests. . . Figure 6 - Redundant Fixed Effects Test . Pooled non-OECD OECD . Cross-Section F | 3.039*** (0.000) | 3.057*** (0.000) | 2.782*** (0.000) | . Cross-Section Chi-Square | 152.544*** (0.000) | 72.255*** (0.000) | 77.188*** (0.000) | . Period F | 38.593*** (0.000) | 13.747*** (0.000) | 54.386*** (0.000) | . Period Chi-Square | 264.110*** (0.000) | 103.413*** (0.000) | 279.292*** (0.000) | . Cross-Section/Period F | 10.400*** (0.000) | 6.794*** (0.000) | 16.360*** (0.000) | . Cross-Section Chi-Square | 424.806*** (0.000) | 165.049*** (0.000) | 326.592*** (0.000) | . Note: * - 90 % significance / ** - 95 % significance / *** - 99 % significance Null: Cross-Section/Period/Both Effects specifications are redundant, Alternative: Cross-Section/Period/Both Effects specifications are valid . . Hausman Test . Hausman Test null hypothesis states that there is no correlation between unique errors and regressors (meaning that the random effects model is preferred) against an alternative that there is correlation between unique errors and regressors (meaning that the fixed effects model is preferred). This test could only be performed on cross-sectional fixed effects and not two-way fixed effects since EViews doesn’t estimate two-way random effects tests on unbalanced data for the purposes of further testing. Hausman test results (see Figure 7) reject the null hypothesis and the alternative is accepted – fixed effects model is appropriate. . . Figure 7 - Random versus Fixed Effects Test . Pooled non-OECD OECD . Chi-Square statistic | 146.276*** (0.000) | 52.771*** (0.000) | 37.882*** (0.000) | . degrees of freedom | 6 | 6 | 6 | . Note: * - 90 % significance / ** - 95 % significance / *** - 99 % significance . . 6. Revised Estimates . Following all the tests performed, I have decided to reformulate the previous cross-section fixed effects OLS model. For more precise parameter identification and robust standard errors, cross-sectional heteroskedasticity and first (and possibly second) order autocorrelation need to be addressed. Therefore, the revised model is structured as a cross-section fixed effects GLS model, with two terms for autocorrelation and cross-section weights which assume the presence of heteroskedasticity in the relevant dimension. Estimates for this model are presented in Figure 8, split up between estimates for all countries, only non-OECD countries, and only OECD countries. . . Figure 8. Panel EGLS Regression (Cross-Section Weights) Dependent variable: log growth of GDP per capita (1975 - 2015) . Pooled Pooled non-OECD non-OECD OECD OECD . | EGLS | OLS | EGLS | OLS | EGLS | OLS | . Intercept | 0.228*** (0.014) | 0.441*** (0.034) | 0.217*** (0.028) | 0.372*** (0.054) | 0.280*** (0.016) | 0.526*** (0.046) | . log GDP per capita | -0.032*** (0.003) | -0.071*** (0.005) | -0.044*** (0.007) | -0.072*** (0.009) | -0.027*** (0.003) | -0.072*** (0.007) | . Size of Government | 0.005*** (0.003) | 0.005 (0.004) | 0.005 (0.003) | 0.006 (0.006) | 0.002 (0.001) | 0.002 (0.005) | . Legal System &amp; Property Rights | 0.001 (0.002) | -0.003 (0.003) | 0.001 (0.003) | -0.005 (0.005) | -0.002 (0.002) | -0.002 (0.004) | . Sound Money | -0.007*** (0.001) | -0.004 (0.002) | -0.009*** (0.002) | -0.006** (0.003) | -0.006*** (0.001) | 0.000 (0.004) | . Regulation | 0.003 (0.003) | 0.024*** (0.005) | 0.013** (0.006) | 0.024*** (0.008) | 0.001 (0.002) | 0.025*** (0.004) | . Freedom to Trade Internationally | 0.014*** (0.002) | 0.013*** (0.003) | 0.017*** (0.006) | 0.017*** (0.004) | 0.010*** (0.002) | 0.008* (0.004) | . AR(1) | -0.669*** (0.038) | | -0.498*** (0.072) | | -0.862*** (0.039) | | . AR(2) | -0.538*** (0.033) | | -0.530*** (0.071) | | -0.663*** (0.034) | | . Observations | 325 | 430 | 141 | 189 | 184 | 241 | . Cross-section (periods) | 52 (7) | 52 (9) | 24 (7) | 24 (9) | 28 (7) | 28 (9) | . R2R^2R2 | 0.71 | 0.44 | 0.56 | 0.41 | 0.85 | 0.46 | . F-statistic (p-value) | 10.83 (0.000) | 5.03 (0.000) | 4.42 (0.000) | 3.88 (0.000) | 24.64 (0.002) | 5.4 (0.000) | . Durbin-Watson statistic | 1.84 | 2.67 | 2.08 | 2.41 | 1.74 | 2.92 | . Note: * - 90 % significance / ** - 95 % significance / *** - 99 % significance . . As Figure 8 shows, Freedom to Trade Internationally and Sound Money have a significant effect across three model runs, and Regulation has a significant effect for non-OECD countries only. Any one-point increase in the Freedom to Trade Internationally index is correlated with a 1.3 % (pooled), 1.7 % (non-OECD) and 1% (OECD) increase in the growth rate of GDP per capita, on average. Any one-point increase in the Sound Money index is correlated with a 0.7 % (pooled), 0.9 % (non-OECD) and 0.6% (OECD) increase in the growth rate of GDP per capita, on average. Any one-point increase in the Regulation index for non-OECD countries is correlated with a 1.3 % increase in the growth rate of GDP per capita, on average. Finally, any one-point increase in the Size of Government index for pooled countries is correlated with a 0.5 % increase in the growth rate of GDP per capita, on average. To reiterate, an increase in index scores across all three of the variables mentioned indicates an increase in economic freedom, as per EFI design. . . Figure 9 - Residuals versus Fitted Values plots . . Moreover, to confirm that the revised model significantly diminishes heteroskedasticity and autocorrelation detected previously, Figure 8 reports the Durbin-Watson statistic and Figure 9 shows the residual plots. The Durbin-Watson (d) statistic went from 2.7 to 1.8 (pooled), 2.4 to 2.1 (non-OECD), and 2.9 to 1.7 (OECD). Since a d value of 2 indicates that ρ rhoρ is 0, this indicates a reduction in residual trend correlation. Additionally, Appendix 5 reports the trend of residuals across periods (compared with the OLS estimated ones), indicating a smoothening. Finally, Figure 9 reports the error dispersion (compared with the OLS estimated ones), indicating more uniform dispersion (homoskedasticity). . 7. Conclusion . This project finds a significant relationship between freedom to trade internationally and economic growth. This freedom is reflected in lower tariffs, few regulations on movement of human and physical capital, easily convertible currency and simple customs clearance operations. This finding mirrors that of Gwartney 23 and Torstensson 14, and contradicts the findings of Ayal and Karras10, who find a negative relationship between freedom to trade and economic growth. This finding holds for all countries pooled together, only non-OECD countries, as well as OECD member nations. Furthermore, this research finds that pursuing low inflation and allowing free access to alternative currency use has a small negative impact on economic growth (between 0.6% and 0.9% for each EFI area unit increase). This result directly contradicts that of Ayal and Karras10 and Barro31. There is also significant evidence that less stringent regulation positively affects economic growth for non-OECD countries only. This reflects the findings of Barro31, Torstensson14 and Knack &amp; Keefer13. Finally, there is significant evidence that smaller governments, that intervene economically less often, are positively correlated with economic growth. This has only been observed for the pooled data set and the effect was only 0.05%. This finding is mirrored in a more robust way in other research11 23 13. . Across all model formulations, freedom to trade internationally remained very robust. These results support classical economic theory3 1, as well as more modern assumptions9. Monetary stability through low inflation and the freedom to use alternate currencies seemed likely to be correlated with economic growth. However, research did not support this hypothesis. There are a few reasons that might explain this finding. Subcomponents of this area of the index might be constructed out of elements with opposite effects. Also, most countries in the data set do not wield the economic power of the United States and are economically tied to the fluctuations of more influential currencies. Therefore, they are unable to produce sound monetary policy and often attempt to restrict the power of foreign currencies in the domestic marketplace. If these countries experience increased growth rates, this area of the index does not predict development as assumed. Free movement into markets, as pictured through the regulation variable, is only significant for non-OECD countries, most of which are underdeveloped. This could indicate that lax regulation fosters growth on the way to the status of a first-world country. However, countries that had already reached these levels of development are not experiencing such growth rates and often begin introducing new regulation when they become appropriately placed on the Kuznets curve to do so. This is often regulation that deals with various market inefficiencies (externalities, informational asymmetry, etc.). . Limitations and future research . This data set was somewhat unbalanced (missing periods for a few cross-sections), resulting in the omission of those observations. Along with balanced panel data, the set needs to extend over a longer time frame, given that in the process of correcting for autocorrelation, the number of observations got further reduced. It would also be useful to detect breakpoints in the time series, centered around significant economic events (like the Great Recession), and perform the estimation around them. Much like a longer time frame, more countries included in the set would be useful. The issue lies in procuring the necessary data, especially further into the past, given that some countries do not provide any data or provide data that is highly questionable. . 8. GitHub repository . For data, code, and similar projects, visit github.com/antoniojurlina/economic_freedom_and_growth. . Appendix 1 - Index Area Components . Size of Government: (a) Government Consumption (b) Transfers and subsidies (c) Government enterprises and investment (d) Top marginal tax rate | Legal System and Property Rights: (a) Judicial independence (b) Impartial courts (c) Protection of property rights (d) Military interference in rule of law and politics (e) Integrity of the legal system (f) Legal enforcement of contracts (g) Regulatory costs of the sale of real property (h) Reliability of police (i) Business costs of crime | Sound Money: (a) Money growth (b) Standard deviation of inflation (c) Inflation: most recent year (d) Freedom to own foreign currency bank | Freedom to Trade Internationally: (a) Tariffs (b) Regulatory trade barriers (c) Black-market exchange rates (d) Controls of the movement of capital and people | Regulation: (a) Credit market regulations (b) Labor market regulations (c) Business regulations | . Appendix 2 - Summary Statistics . Appendix 3 - Summary Graphs . Appendix 4 - Residuals Plots (before) . Appendix 5 - Residuals Plots (after) . Appendix 6 - EViews Code . spool results output(s) results &#39;creating variables necessary to introduce growth to the model series pp = log(gdp) series growth = (pp - pp(-1)) / 5 &#39;&#39;&#39;&#39;&#39;&#39; Creating summary statistics, graphs and a covariance matrix&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; alpha country = countries group dataset country year gdp gov growth*100 legal money oecd pp regulation trade results.insert dataset delete country gdp.statby(max, min, nov, noa, p) countries close gdp graph ggdp.line(m, ab = histogram, panel = individual) gdp ggdp.options connect results.insert ggdp growth.statby(max, min, nov, noa, p) countries close growth graph ggrowth.line(m, ab = histogram, panel = individual) growth ggrowth.options connect results.insert ggrowth gov.statby(max, min, nov, noa, p) countries close gov graph ggov.line(m, ab = histogram, panel = individual) gov ggov.options connect results.insert ggov legal.statby(max, min, nov, noa, p) countries close legal graph glegal.line(m, ab = histogram, panel = individual) legal glegal.options connect results.insert glegal money.statby(max, min, nov, noa, p) countries close money graph gmoney.line(m, ab = histogram, panel = individual) money gmoney.options connect results.insert gmoney trade.statby(max, min, nov, noa, p) countries close trade graph gtrade.line(m, ab = histogram, panel = individual) trade gtrade.options connect results.insert gtrade regulation.statby(max, min, nov, noa, p) countries close regulation graph gregulation.line(m, ab = histogram, panel = individual) regulation gregulation.options connect results.insert gregulation group variables growth gdp gov legal money regulation trade matrix x = @cor(variables) x.setcollabels growth gdp gov legal money regulation trade x.setrowlabels growth gdp gov legal money regulation trade x.displayname Correlation Matrix x.label results.insert x delete ggdp ggov glegal gtrade gmoney gregulation ggrowth world_gdp usd delete variables rank economic_freedom_summary_index x results.displayname untitled01 &quot;Data Set&quot; results.displayname untitled02 &quot;GDP per capita&quot; results.displayname untitled03 &quot;GDP per capita&quot; results.displayname untitled04 &quot;Growth rate&quot; results.displayname untitled05 &quot;Growth rate&quot; results.displayname untitled06 &quot;Size of Government&quot; results.displayname untitled07 &quot;Size of Government&quot; results.displayname untitled08 &quot;Legal System &amp; Property Rights&quot; results.displayname untitled09 &quot;Legal System &amp; Property Rights&quot; results.displayname untitled10 &quot;Sound Money&quot; results.displayname untitled11 &quot;Sound Money&quot; results.displayname untitled12 &quot;Freedom to trade internationally&quot; results.displayname untitled13 &quot;Freedom to trade internationally&quot; results.displayname untitled14 &quot;Regulation&quot; results.displayname untitled15 &quot;Regulation&quot; results.displayname untitled16 &quot;Correlation Matrix&quot; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39; end of summary statistics &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39; Fixed effects OLS models &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; smpl @all equation eq_a.ls(cx=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) results.insert eq_a results.displayname untitled17 &quot;OLS (pooled)&quot; smpl if oecd = 0 equation eq_b.ls(cx=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) results.insert eq_b results.displayname untitled18 &quot;OLS (non-OECD)&quot; smplifoecd=1 equation eq_c.ls(cx=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) results.insert eq_c results.displayname untitled19 &quot;OLS (OECD)&quot; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; End of OLS estimation &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39; &#39;&#39;&#39;&#39;&#39;&#39; Tests &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; smpl @all equation eq_d.ls(cx=f, per=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) eq_d.fixedtest(p) results.displayname untitled20 &quot;Redundancy Test a&quot; smpl if oecd = 0 equation eq_e.ls(cx=f, per=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) eq_e.fixedtest(p) results.displayname untitled21 &quot;Redundancy Test b&quot; smpl if oecd = 1 equation eq_f.ls(cx=f, per=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) eq_f.fixedtest(p) results.displayname untitled22 &quot;Redundancy Test c&quot; close eq_d close eq_e close eq_f smpl @all equation eq_d.ls(cx=r) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) eq_d.ranhaus(p) close eq_d results.displayname untitled23 &quot;RE vs FE Test a&quot; smpl if oecd = 0 equation eq_e.ls(cx=r) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) eq_e.ranhaus(p) close eq_e results.displayname untitled24 &quot;RE vs FE Test b&quot; smpl if oecd = 1 equation eq_f.ls(cx=r) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) eq_f.ranhaus(p) close eq_f results.displayname untitled25 &quot;RE vs FE Test c&quot; smpl @all EQ_A.makeresids resid_a graph res_a.spike(m, panel = combine) resid_a res_a.options connect results.insert res_a results.displayname untitled26 &quot;OLS (pooled) Residuals&quot; smpl if oecd = 0 EQ_B.makeresids resid_b graph res_b.spike(m, panel = combine) resid_b res_b.options connect results.insert res_b results.displayname untitled27 &quot;OLS (non-OECD) Residuals&quot; smpl if oecd = 1 EQ_C.makeresids resid_c graph res_c.spike(m, panel = combine) resid_c res_c.options connect results.insert res_c results.displayname untitled28 &quot;OLS (OECD) Residuals&quot; smpl @all eq_a.forecast(e, g) growthf graph hetero1.scat(panel = stack) growthf resid_a hetero1.axis(b) angle(auto) hetero1.legend position(LEFT) hetero1.setelem(1) symbol(CIRCLE) linepattern(none) linecolor(@rgb(57,106,177)) hetero1.setelem(1) legend(Fitted Values) hetero1.setelem(2) legend(Residuals) hetero1.setelem(1) axis(b) results.insert hetero1 results.displayname untitled29 &quot;OLS (pooled) Residuals Plot&quot; smpl if oecd = 0 eq_b.forecast(e, g) growthf graph hetero2.scat(panel = stack) growthf resid_b hetero2.axis(b) angle(auto) hetero2.legend position(LEFT) hetero2.setelem(1) symbol(CIRCLE) linepattern(none) linecolor(@rgb(57,106,177)) hetero2.setelem(1) legend(Fitted Values) hetero2.setelem(2) legend(Residuals) hetero2.setelem(1) axis(b) results.insert hetero2 results.displayname untitled30 &quot;OLS (non-OECD) Residuals Plot&quot; smpl if oecd = 1 eq_c.forecast(e, g) growthf graph hetero3.scat(panel = stack) growthf resid_c hetero3.axis(b) angle(auto) hetero3.legend position(LEFT) hetero3.setelem(1) symbol(CIRCLE) linepattern(none) linecolor(@rgb(57,106,177)) hetero3.setelem(1) legend(Fitted Values) hetero3.setelem(2) legend(Residuals) hetero3.setelem(1) axis(b) results.insert hetero3 results.displayname untitled31 &quot;OLS (OECD) Residuals Plot&quot; delete res_a res_b res_c hetero1 hetero2 hetero3 resid_a resid_b resid_c growthf smpl @all equation eq_g.ls(cx=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) ar(1) ar(2) results.insert eq_g results.displayname untitled32 &quot;OLS (pooled)&quot; smpl if oecd = 0 equation eq_h.ls(cx=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) ar(1) ar(2) results.insert eq_h results.displayname untitled33 &quot;OLS (non-OECD)&quot; smpl if oecd = 1 equation eq_i.ls(cx=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) ar(1) ar(2) results.insert eq_i results.displayname untitled34 &quot;OLS (OECD)&quot; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; &#39;&#39;&#39;&#39; end of tests &#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39;&#39; smpl @all equation eq_j.ls(cx=f, wgt=cxdiag, deriv=aa) growth c gov(- 1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) ar(1) ar(2) results.insert eq_j results.displayname untitled35 &quot;OLS (pooled)&quot; smpl if oecd = 0 equation eq_k.ls(cx=f, wgt=cxdiag, deriv=aa) growth c gov(- 1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) ar(1) ar(2) results.insert eq_k results.displayname untitled36 &quot;OLS (non-OECD)&quot; smpl if oecd = 1 equation eq_l.ls(cx=f, wgt=cxdiag, deriv=aa) growth c gov(- 1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) ar(1) ar(2) results.insert eq_l results.displayname untitled37 &quot;OLS (OECD)&quot; smpl @all equation eq_m.ls(cx=f, per=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) results.insert eq_m results.displayname untitled38 &quot;OLS (pooled)&quot; smpl if oecd = 0 equation eq_n.ls(cx=f, per=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) results.insert eq_n results.displayname untitled39 &quot;OLS (non-OECD)&quot; smpl if oecd = 1 equation eq_o.ls(cx=f, per=f) growth c gov(-1) legal(-1) money(-1) trade(-1) regulation(-1) pp(-1) results.insert eq_o results.displayname untitled40 &quot;OLS (OECD)&quot; smpl @all eq_j.makeresids resid_j graph res_j.spike(m, panel = combine) resid_j res_j.options connect results.insert res_j results.displayname untitled41 &quot;OLS (pooled) Residuals&quot; smpl if oecd = 0 eq_k.makeresids resid_k graph res_k.spike(m, panel = combine) resid_k res_k.options connect results.insert res_k results.displayname untitled42 &quot;OLS (non-OECD) Residuals&quot; smpl if oecd = 1 eq_l.makeresids resid_l graph res_l.spike(m, panel = combine) resid_l res_l.options connect results.insert res_l results.displayname untitled43 &quot;OLS (OECD) Residuals&quot; smpl @all eq_j.forecast(e, g) growthf graph hetero1.scat(panel = stack) growthf resid_j hetero1.axis(b) angle(auto) hetero1.legend position(LEFT) hetero1.setelem(1) symbol(CIRCLE) linepattern(none) linecolor(@rgb(57,106,177)) hetero1.setelem(1) legend(Fitted Values) hetero1.setelem(2) legend(Residuals) hetero1.setelem(1) axis(b) results.insert hetero1 results.displayname untitled44 &quot;OLS (pooled) Residuals Plot&quot; smpl if oecd = 0 eq_k.forecast(e, g) growthf graph hetero2.scat(panel = stack) growthf resid_k hetero2.axis(b) angle(auto) hetero2.legend position(LEFT) hetero2.setelem(1) symbol(CIRCLE) linepattern(none) linecolor(@rgb(57,106,177)) hetero2.setelem(1) legend(Fitted Values) hetero2.setelem(2) legend(Residuals) hetero2.setelem(1) axis(b) results.insert hetero2 results.displayname untitled45 &quot;OLS (non-OECD) Residuals Plot&quot; smpl if oecd = 1 eq_l.forecast(e, g) growthf graph hetero3.scat(panel = stack) growthf resid_l hetero3.axis(b) angle(auto) hetero3.legend position(LEFT) hetero3.setelem(1) symbol(CIRCLE) linepattern(none) linecolor(@rgb(57,106,177)) hetero3.setelem(1) legend(Fitted Values) hetero3.setelem(2) legend(Residuals) hetero3.setelem(1) axis(b) results.insert hetero3 results.displayname untitled46 &quot;OLS (OECD) Residuals Plot&quot; delete res_j res_k res_l hetero1 hetero2 hetero3 resid_j resid_k resid_l growthf results.options displaynames . Works Cited . Smith, A. (2003). An Inquiry into the Nature and Causes of the Wealth of Nations. Bantam Classics. &#8617; &#8617;2 . | McCusker, J., &amp; Morgan, K. (Eds.). (2001). The Early Modern Atlantic Economy. Cambridge: Cambridge University Press. doi:10.1017/CBO9780511523878 &#8617; . | Ricardo, D. (1817). On the Principles of Political Economy and Taxation. London: Dover Publications. &#8617; &#8617;2 . | Solow, R. M. (1956). A Contribution to the Theory of Economic Growth. The Quarterly Journal of Economics, 70(1), 65. https://doi.org/10.2307/1884513 &#8617; . | Domar, E. D. (1946). Capital Expansion, Rate of Growth, and Employment. Econometrica, 14(2), 137–147. https://doi.org/10.2307/1905364 &#8617; . | Harrod, R. F. (1939). An Essay in Dynamic Theory. The Economic Journal, 49(193), 14–33. https://doi.org/10.2307/2225181 &#8617; . | Kuznets, S. (1973). Modern Economic Growth: Findings and Reflections. The American Economic Review, 63(3), 247–258. &#8617; . | Schumpeter, J. (1942). Capitalism, Socialism and Democracy (Vol. 1). Routledge. Retrieved from https://www.goodreads.com/work/best_book/129884-capitalism-socialism-and-democracy &#8617; . | Friedman, M. (1962). Capitalism and Freedom (1st ed.). University of Chicago Press. Retrieved from https://www.goodreads.com/work/best_book/1534488-capitalism-and-freedom &#8617; &#8617;2 . | Ayal, E. B., &amp; Karras, G. (1998). Components of Economic Freedom and Growth: An Empirical Study. The Journal of Developing Areas, 32(3), 327–338. &#8617; &#8617;2 &#8617;3 . | Barro, R. J. (1991). Economic Growth in a Cross Section of Countries. The Quarterly Journal of Economics, 106(2), 407–443. https://doi.org/10.2307/2937943 &#8617; &#8617;2 . | Easterly, W. (1992). Marginal income tax rates and economic growth in developing countries (No. WPS1050) (p. 1). The World Bank. Retrieved from http://documents.worldbank.org/curated/en/432391468766196026/Marginal-income-tax-rates-and-economic-growth-in-developing-countries &#8617; . | Knack, S., &amp; Keefer, P. (1995). Institutions and Economic Performance: Cross-Country Tests Using Alternative Institutional Measures. Economics &amp; Politics, 7(3), 207–227. https://doi.org/10.1111/j.1468-0343.1995.tb00111.x &#8617; &#8617;2 &#8617;3 . | Torstensson, J. (1994). Property Rights and Economic Growth: An Empirical Study. Kyklos, 47(2), 231–247. &#8617; &#8617;2 &#8617;3 . | Naughton, B. (2007). The Chinese economy: transitions and growth. Cambridge, Mass: MIT Press. &#8617; . | Mundell, R. A. (1963). Capital Mobility and Stabilization Policy under Fixed and Flexible Exchange Rates. The Canadian Journal of Economics and Political Science / Revue Canadienne d’Economique et de Science Politique, 29(4), 475–485. https://doi.org/10.2307/139336 &#8617; . | J. M. Fleming, “Domestic Financial Policies under Fixed and Floating Exchange Rates,” IMF Staff Papers, Vol. 9, 1962, pp. 369-379. doi:10.2307/3866091 &#8617; . | Leontief, W. (1953). Domestic Production and Foreign Trade; The American Capital Position Re-Examined. Proceedings of the American Philosophical Society, 97(4), 332– 349. &#8617; . | Gwartney, J. D., Lawson, R. A., &amp; Block, W. (1996). Economic Freedom of the World:1975-1995 (p. 342). The Fraser Institute. Retrieved from http://bit.ly/2jUkBGR &#8617; . | Berggren, N. (2003). The Benefits of Economic Freedom: A Survey. The Independent Review, 8(2), 193–211. &#8617; . | Carlsson, F., &amp; Lundström, S. (2002). Economic freedom and growth: Decomposing the effects. Public Choice, 112, 335–344. https://doi.org/10.1023/a:1019968525415 &#8617; . | de Haan, J., &amp; Sturm, J.-E. (2000). On the relationship between economic freedom and economic growth. European Journal of Political Economy, 16(2), 215–241. https://doi.org/10.1016/S0176-2680(99)00065-8 &#8617; . | Gwartney, J. D., Lawson, R. A., &amp; Holcombe, R. G. (1999). Economic Freedom and the Environment for Economic Growth. Journal of Institutional and Theoretical Economics, 155(4), 643–663. &#8617; &#8617;2 &#8617;3 . | Nelson, M. A., &amp; Singh, R. D. (1998). Democracy, Economic Freedom, Fiscal Policy, and Growth in LDCs: A Fresh Look. Economic Development and Cultural Change, 46(4), 677– 696. https://doi.org/10.1086/452369 &#8617; . | Economic Freedom of the World. (2015, December 22). Retrieved October 9, 2018, from http://bit.ly/2h5xBVI &#8617; &#8617;2 &#8617;3 . | GDP per capita (current US$) Data. (2018). Retrieved October 9, 2018, from https://data.worldbank.org/indicator/NY.GDP.PCAP.CD &#8617; . | OECD - Members and partners. (2018, July). Retrieved December 12, 2018, from http://www.oecd.org/about/membersandpartners/ &#8617; . | Mankiw, N. G., Romer, D., &amp; Weil, D. N. (1992). A Contribution to the Empirics of Economic Growth. The Quarterly Journal of Economics, 407–437. https://doi.org/10.3386/w3541 &#8617; . | Guajarati, D. N. (1987). Basic Econometrics (4th ed.). &#8617; . | Wooldridge, J. M. (2016). Introductory Econometrics: A Modern Approach (6th ed.). Thomson South-Western. &#8617; . | Barro, R. J. (1996). Democracy and Growth. Journal of Economic Growth, 1(1), 1–27. &#8617; &#8617;2 . |",
            "url": "https://antoniojurlina.github.io/portfolio/projects/eviews/2019/12/07/economic-freedom-and-growth.html",
            "relUrl": "/projects/eviews/2019/12/07/economic-freedom-and-growth.html",
            "date": " • Dec 7, 2019"
        }
        
    
  
    
        ,"post10": {
            "title": "How does traffic congestion impact life satisfaction?",
            "content": ". Summary . In an attempt to venture solo, while an undergraduate student in economics, I worked on a probit model designed to study the effect traffic congestion has on satisfaction with work and place of residence. The question seemed straightforward, and the dependent variables were already either presented as binary or Likert scales that could be easily turned into one. Data came from the city of Cardiff, UK, in the form of an online questionnaire. I find that an increase in the perceived level of congestion makes individuals less happy with working and living in Cardiff. After adjusting for all the potential issues (like hetersokedasticity) this data might have, the results seem quite robust across two probit models. Included, also, is a link to the GitHub repository containing the data I worked with and the Stata do-file. . Part I . 1. Data . Council authority of Cardiff has, in cooperation with Cardiff Business Partnership, conducted an online survey, yielding 2,094 responses. After data clean-up process, 2,045 observations remained. While most variables were dropped, 17 were deemed significant enough to include (see Table 1). Missing values were filled with unconditional means, except in the case of the variables describing the primary method of transportation and proximity of a train station, where 19 and 32 observations with missing values were dropped, respectively. Variables regarding age, income and employment duration were acquired via drop-down answer menus on the survey, with each answer constituting a specific bracket. For the sake of continuity and interpretability, these answers were encoded as the midpoint value of each bracket. Since each set of brackets concluded with an open ended one (e.g. more than 10, 65+, etc.), and no middle point could be determined, each of those categories was encoded as the sum of the size of the largest bracket (for the relevant variable) and the bracket’s lower bound. . . Table 1 - Data Summary . Average Std. Dev. Min Max Explanation . Quality of public transportation | 3.26 | 1.09 | 1 | 5 | 1 is very bad and 5 is very good | . Being able to get from place to place with little traffic (i.e. congestion) | 2.72 | 1.04 | 1 | 5 | 1 is very bad and 5 is very good | . Work proximity | 2.98 | 1.28 | 1 | 5 | 1 you like the most and 5 you dislike | . Travel within the city | 3.86 | 0.96 | 1 | 5 | 1 is not important and 5 is very important | . Ease of getting to work | 3.65 | 1.02 | 1 | 5 | 1 is not important and 5 is very important | . Overall satisfaction with life | 2.95 | 0.98 | 1 | 5 | 1 being highly satisfied and 5 being unsatisfied | . Number of years employed | 9.02 | 5.46 | 0.5 | 15 | How long have you worked in your present employment | . Income | 25,748.18 | 10,318.80 | 5,720 | 55,241 | How much do you get paid, before taxes | . Age | 41.50 | 11.05 | 21 | 74 | How old are you | . Sex | 0.42 | 0.49 | 0 | 1 | 1 for male, 0 for female | . Relationship | 0.69 | 0.46 | 0 | 1 | 1 for in a relationship, 0 for single | . Children | 0.56 | 0.49 | 0 | 1 | 1 for having any number of children, 0 for none | . Overall satisfaction with place of residence | 0.66 | 0.47 | 0 | 1 | 1 for satisfied, 0 for unsatisfied | . Train station proximity | 2.72 | 0.50 | 1 | 3 | Do you have a train station within 2 miles of your residence? 1 - Don’t Know, 2 - No, 3 - Yes | . Education | 2.53 | 1.10 | 1 | 6 | 1 - High School, 2 - Associates degree, 3 – Bachelor’s Degree BA, BSc, 4 – Master’s Degree, 5 - Professional degree, 6 - PhD | . Primary mode of transportation | 1.36 | 0.83 | 0 | 2 | 0 - Hippie, 1 - Public, 2 - Drive | . Job satisfaction | 0.52 | 0.50 | 0 | 1 | 1 for satisfied, 0 for unsatisfied | . . 2. Methodology . Literature review, focusing on measuring different sorts of satisfaction1 2 3, exemplifies various probit models and their appropriate usage. Binary nature of the dependent variables, together with the literature, was the main reason for choosing a probit-model approach. Given that the report attempts to clarify the effect congestion might have on satisfaction with place of residence (R) and working (J) in Cardiff, following two models represent its cornerstone: . J=β0+β1congestion+xδ+ϵ1J = beta_0 + beta_1 congestion + x delta + epsilon_1J=β0​+β1​congestion+xδ+ϵ1​ . R=β0+β1congestion+xδ+ϵ2R = beta_0 + beta_1 congestion + x delta + epsilon_2R=β0​+β1​congestion+xδ+ϵ2​ . Model 2 dependent variable was binary in the form the data was presented. However, Model 1 dependent variable had to be created from a set of Likert scale variables focusing on job satisfaction. Mean value for the set was created across individuals and compared to “3”, which is the neutral option on the answer sheet. Individuals above “3” were categorized as satisfied and the rest were categorized as unsatisfied. This way, a binary variable was generated, appropriate for use in a probit model. Cronbach’s alpha was used to determine the scale reliability coefficient (0.8471), indicating that the set of Likert-scale variables used are closely related as a group and representative of the shared concept. . Furthermore, for both Models 1 and 2, β betaβ and δ deltaδ represent coefficients to be estimated (see Table 1). Control variables were chosen based on two criteria: 1) those causally linked with the dependent variable through congestion and 2) those with a direct, causal link to satisfaction that were deemed as likely sources of severe endogeneity. Explanatory variables of interest consist of demographic characteristics4 5, congestion6 7 8 and those capturing satisfaction spillover9 10. Stochastic terms are noted as ϵ1 epsilon_1ϵ1​ and ϵ2 epsilon_2ϵ2​. Also, since the probit models are being used, robust standard errors are reported and corrected for underlying (inherent) heteroskedasticity. . 3. Hypotheses . . H01H_{01}H01​: Congestion has no significant impact on job satisfaction for residents of Cardiff. . HA1H_{A1}HA1​: Congestion has a negative impact on job satisfaction for residents of Cardiff. . . H02H_{02}H02​: Congestion has no significant impact on satisfaction with place of residence. . HA2H_{A2}HA2​: Congestion has a negative impact on satisfaction with place of residence. . . Models are demonstrating the conditional probability of a specific outcome occurring (Yi=1Y_i = 1Yi​=1 meaning satisfied with job for Model 1 and satisfied with place of residence for Model 2), for 3 which the marginal effects show how a unit change for congestion increases (or decreases) the probability of the given outcome occurring (et ceteris paribus). . . Part II . 1. Satisfaction with working and living in Cardiff . . I am quite confident that an increase in perceived level of congestion makes individuals 4% less likely to be satisfied with working in Cardiff. . . I am quite confident that an increase in perceived level of congestion makes individuals 5% less likely to be satisfied with living in Cardiff. . . Tables 2 and 3 present more detailed results for these claims and state the possibility of a mistake being made. Numbers showing the impact of congestion on satisfaction with working and living in Cardiff are positive. This is a consequence of the way the survey was designed (1 is very bad and 5 is very good) and should be interpreted with an opposite sign. Literature review supports this finding. High levels of traffic congestion are associated with mental and physical stress11 12, which are further aggravated with the inability to complete daily routines 13. Furthermore, long work commutes cause residual stress in the workplace 7 14 15. Kahneman et al.8 found that work commutes were most frequently associated with negative feelings, out of all daily habits. . . Table 2 - Model 1 marginal effects . Marginal effect Standard error . Quality of public transportation | | 0.049*** | 0.011 | . Being able to get from place to place with little traffic (i.e. congestion) | | 0.041*** | 0.011 | . Work proximity | | 0.023*** | 0.008 | . Travel within the city | | 0.007 | 0.014 | . Ease of getting to work | | 0.015 | 0.013 | . Overall satisfaction with life | | -0.027** | 0.011 | . Number of years employed | | -0.001 | 0.002 | . Income | | 0.000*** | 0.000 | . Age | | -0.008 | 0.007 | . Sex | | -0.072*** | 0.022 | . Relationship | | -0.015 | 0.025 | . Children | | 0.039 | 0.026 | . Overall satisfaction with place of residence | | 0.094*** | 0.023 | . Train station proximity | Yes No | -0.204*** -0.219*** | 0.058 0.027 | . Education | | 0.024** | 0.010 | . Primary mode of transportation | Public Drive | 0.010 0.076*** | 0.033 0.027 | . * - 90 % significance / ** - 95 % significance / *** - 99 % significance . . Public transportation plays a major role in mitigating the effects of congestion on satisfaction16. However, while it is the most accepted solution to congestion17, it only works if it is appropriately scaled with the needs of the public18. In Cardiff, an increase in perceived quality of public transportation makes individuals 4% (on average) more likely to feel satisfied with their work and 6% (on average) more likely to feel satisfied with where they live, holding everything else the same. Moreover, utilizing public transportation systems decreases the likelihood of being satisfied with place of residence by 6% (on average), holding everything else the same. Those most likely to utilize public transportation live inside the urban area while those living in the suburbs are more likely to drive to work. Therefore, this decrease in likelihood of being satisfied with the place of residence, based on public transport utilization, indicates a preference for living outside of the city. Furthermore, driving decreases the likelihood of being satisfied with working in Cardiff by 8% (on average), holding everything else the same (see Tables 2 and 3 for more detailed results and the likelihood of mistakes being reported). Finally, living close to work increases the likelihood of being satisfied with working in Cardiff by 2% (on average), holding everything else the same. . . Table 3 - Model 2 marginal effects . Marginal effect Standard error . Quality of public transportation | | 0.062*** | 0.010 | . Being able to get from place to place with little traffic (i.e. congestion) | | 0.052*** | 0.010 | . Work proximity | | 0.003 | 0.008 | . Travel within the city | | 0.016 | 0.013 | . Ease of getting to work | | -0.021* | 0.012 | . Overall satisfaction with workplace | | 0.086*** | 0.020 | . Number of years employed | | 0.002 | 0.002 | . Income | | 0.000*** | 0.000 | . Age | | -0.006 | 0.006 | . Sex | | -0.035 | 0.020 | . Relationship | | 0.106*** | 0.023 | . Children | | 0.034 | 0.024 | . Overall satisfaction with life | | 0.002 | 0.010 | . Train station proximity | Yes No | 0.212*** 0.121 | 0.075 0.077 | . Education | | 0.019*** | 0.009 | . Primary mode of transportation | Public Drive | -0.055* -0.030 | 0.032 0.026 | . * - 90 % significance / ** - 95 % significance / *** - 99 % significance . . While there are negative effects of congestion on satisfaction with working and living in Cardiff, those effects are smaller than anticipated. This could be because working during a period of recession or post-recession19 recovery causes enough satisfaction that lessens the burdens of daily commute13. Additionally, as noted by Stokols et al. 11, congestion is only relevant when it significantly differs from the expected traffic levels. If people are exposed to similar congestion levels every day, negative effects are diminished. . 2. Policy relevance . Those that live within 2 miles of a train station and those that live further away, are about 20% (on average) less likely to feel satisfied with working in Cardiff, holding everything else the same. Moreover, those that live within 2 miles of a train station are about 20% (on average) more likely to be satisfied with their place of residence, holding everything else the same (see Tables 2 and 3). Since most of survey respondents (74%) live within 2 miles of a train station and more than half (59%) drive to work (not necessarily overlapping groups), it could be assumed that there is a negative perception of public transportation and a strong tendency to avoid it. This, in addition to reported congestion and public transportation effects, indicates that policy under consideration by the council authority of Cardiff needs to address both public transportation and congestion. Building a new metro system as well as introducing a congestion charge for those driving into the center of Cardiff should be packaged together. Literature review supports this claim, as well20. . 3. Limitations . Variability of answers provided was reduced when most missing values were filled with average values for the category. Further adding to this issue was the design of the survey itself. Answers that were selected from a drop-down menu were presented as brackets, chosen by the surveyor. This eliminated most of the effect that individuals with answers far away from the average would have. Moreover, for privacy reasons, answers stating the specific place of residence were excluded, introducing additional lack of clarity. . Due to model choice, differences between observed and predicted and expected and predicted values for variables of interest were not uniform across the data. This means that I was more likely to falsely perceive an observation as insignificant and reject its validity and explanatory power. Finally, I wish to point out the possibility that any variable not considered stands a chance of being correlated with variables I did consider (e.g. place of residence might be linked with public transport utilization, commute satisfaction and satisfaction with living and working in Cardiff). By omitting any such variable, I introduced the likelihood of overstating the effects considered variables have on overall satisfaction. . 4. GitHub repository . For data, code, and similar projects, visit https://github.com/antoniojurlina/econometrics. . . Stata code . cls clear //clear previous data use CBP_survey.dta //choose data // following commands represent my data clean-up process //////////////////////////////////////////////////////////////////// encode Q3, generate(yrs_employed) replace yrs_employed = 0.5 if Q3 == &quot;Less than 1 year&quot; replace yrs_employed = 2 if Q3 == &quot;1 to 3 years&quot; replace yrs_employed = 4 if Q3 == &quot;3 to 5 years&quot; replace yrs_employed = 7.5 if Q3 == &quot;5 to 10 years&quot; replace yrs_employed = 15 if Q3 == &quot;More than 10 years&quot; encode Q4, generate(income) replace income = 5720 if Q4 == &quot;£0 - £11,440 per year&quot; replace income = 12480.5 if Q4 == &quot;£11,441 - £13,520 per year&quot; replace income = 14820.5 if Q4 == &quot;£13,521 - £16,120 per year&quot; replace income = 17420.5 if Q4 == &quot;£16,121 - £18,720 per year&quot; replace income = 20540.5 if Q4 == &quot;£18,721 - £22,360 per year&quot; replace income = 25220.5 if Q4 == &quot;£22,361 - £28,080 per year&quot; replace income = 31720.5 if Q4 == &quot;£28,081 - £35,360 per year&quot; replace income = 40300.5 if Q4 == &quot;£35,361 - £45,240 per year&quot; replace income = 55241 if Q4 == &quot;£45,241 or more per year&quot; drop if Q5 == &quot;25- 40&quot; | Q5 == &quot;25- 41&quot; | Q5 == &quot;25- 42&quot; | Q5 == &quot;25- 43&quot; drop if Q5 == &quot;25- 44&quot; | Q5 == &quot;25- 45&quot; | Q5 == &quot;25- 46&quot; | Q5 == &quot;25- 47&quot; | Q5 == &quot;25- 48&quot; | Q5 == &quot;25- 49&quot; encode Q5, generate(age) replace age = 21 if Q5 == &quot;18- 24&quot; replace age = 32 if Q5 == &quot;25- 39&quot; replace age = 47 if Q5 == &quot;40- 54&quot; replace age = 59.5 if Q5 == &quot;55- 64&quot; replace age = 74 if Q5 == &quot;65&quot; generate age_sq = age * age label define sex 1 &quot;Male&quot; 0 &quot;Female&quot; encode Q6, generate(sex) generate relationshipy = Q7 replace relationshipy = &quot;Yes&quot; if Q7 == &quot;Co-habiting&quot; replace relationshipy = &quot;Yes&quot; if Q7 == &quot;Married&quot; replace relationshipy = &quot;No&quot; if Q7 == &quot;Single&quot; label define relationship 1 &quot;Yes&quot; 0 &quot;No&quot; encode relationshipy, generate(relationship) drop relationshipy Q7 label define children 1 &quot;Yes&quot; 0 &quot;No&quot; encode Q8, generate(children) rename Q14h pub_trans_quality rename Q14i congestion generate satisfactiony = Q15 label define satisfaction 1 &quot;Satisfied&quot; 0 &quot;Unsatisfied&quot; encode satisfactiony, generate(satisfaction) drop satisfactiony Q15 encode Q19, generate(train) label define education 1 &quot;High School&quot; 2 &quot;Associates degree&quot; 3 &quot;Bachelors Degree BA, BSc&quot; 4 &quot;Masters Degree&quot; 5 &quot;Professional degree&quot; 6 &quot;PhD&quot; encode Q20, generate(education) rename Q30c travel_importance rename Q30f work_travel_ease rename Q31 life_satisfaction summarize pub_trans_quality replace pub_trans_quality = r(mean) if pub_trans_quality == . summarize congestion replace congestion = r(mean) if congestion == . summarize yrs_employed replace yrs_employed = r(mean) if yrs_employed == . summarize income replace income = r(mean) if income == . summarize age replace age = r(mean) if age == . summarize sex replace sex = r(mean) if sex == . summarize relationship replace relationship = r(mean) if relationship == . summarize satisfaction replace satisfaction = r(mean) if satisfaction == . summarize train drop if train == . summarize education replace education = r(mean) if education == . summarize life_satisfaction replace life_satisfaction = r(mean) if life_satisfaction == . summarize children replace children = r(mean) if children == . summarize work_travel_ease replace work_travel_ease = r(mean) if work_travel_ease == . summarize travel_importance replace travel_importance = r(mean) if travel_importance == . generate transport1 = Q16 replace transport1 = &quot;Hippie&quot; if Q16 == &quot;Walk&quot; | Q16 == &quot;Cycle&quot; replace transport1 = &quot;Public&quot; if Q16 == &quot;Train&quot; | Q16 == &quot;Bus&quot; replace transport1 = &quot;Drive&quot; if Q16 == &quot;Car /Motorcycle&quot; label define trans1 0 &quot;Hippie&quot; 1 &quot;Public&quot; 2 &quot;Drive&quot; encode transport1, generate(trans1) drop if trans1 == . summarize Q28a replace Q28a = r(mean) if Q28a == . summarize Q28b replace Q28b = r(mean) if Q28b == . summarize Q28c replace Q28c = r(mean) if Q28c == . summarize Q28d replace Q28d = r(mean) if Q28d == . summarize Q28e replace Q28e = r(mean) if Q28e == . summarize Q28f replace Q28f = r(mean) if Q28f == . summarize Q28g replace Q28g = r(mean) if Q28g == . summarize Q29a replace Q29a = r(mean) if Q29a == . summarize Q29b replace Q29b = r(mean) if Q29b == . summarize Q29c replace Q29c = r(mean) if Q29c == . summarize Q29d replace Q29d = r(mean) if Q29d == . summarize Q29e replace Q29e = r(mean) if Q29e == . summarize Q29f replace Q29f = r(mean) if Q29f == . summarize Q29g replace Q29g = r(mean) if Q29g == . summarize Q29h replace Q29h = r(mean) if Q29h == . rename Q29c work_proximity generate job = (Q28a + Q28b + Q28c + Q28d + Q28e + Q28f + Q28g)/7 generate job_satisfaction = job summarize job replace job_satisfaction = 1 if job &gt; 3 replace job_satisfaction = 0 if job &lt;= 3 alpha Q28a-Q28g drop Q3 Q4 Q5 Q6 Q8 Q19 Q20 drop Q1 Q2 Q11 Q13 Q12 Q14a Q14b Q14c Q14d Q14e Q14f Q14g Q14j Q14k Q14l Q14m Q14n Q14o drop Q17 Q21 Q22 Q23a Q23b Q24 Q25 Q26 Q27 Q28a Q28b Q28c Q28d Q28e Q28f Q28g drop Q29a Q29b Q29d Q29e Q29f Q29g Q29h Q30a Q30b Q30d Q30e Q30g Q30h Q30i Q30j Q30k drop Q32 Q33 drop job Q16 Q18 transport1 //////////////////////////////////////////////////////////////////// // end of data clean-up process summarize probit job_satisfaction satisfaction pub_trans_quality children congestion life_satisfaction yrs_employed travel_importance work_travel_ease work_proximity income age age_sq sex relationship i.train education i.trans1,robust margins, dydx(*) probit satisfaction job_satisfaction pub_trans_quality children congestion life_satisfaction yrs_employed travel_importance work_travel_ease work_proximity income age age_sq sex relationship i.train education i.trans1,robust margins, dydx(*) save CBP_survey_clean.dta, replace . . Works Cited . Blanchflower, D. G., &amp; Oswald, A. J. (2004). Well-being over time in Britain and the USA. Journal of Public Economics, 88, 1359-1386. doi:10.1016/S0047-2727(02)00168-8 &#8617; . | Fetai, B., Abduli, S., &amp; Qirici, S. (2015). An Ordered Probit Model of Job Satisfaction in the Former Yugoslav Republic of Macedonia. Procedia Economics and Finance, 33, 350- 357. doi:10.1016/S2212-5671(15)01719-0 &#8617; . | Rayton, B. A. (2006). Examining the interconnection of job satisfaction and organizational commitment: An application of the bivariate probit model. Int. J. of Human Resource Management, 17(1), 139-154. doi:10.1080/09585190500366649 &#8617; . | Richey, S. (2012). Determinants of Community Satisfaction and its Relative Importance for Life Satisfaction. &#8617; . | Auh, S., &amp; Cook, C. (2009). Quality of Community Life Among Rural Residents: An Integrated Model. Social Indicators Research, 94(3), 377-389. &#8617; . | Lipsetz, D. A. (2000). Residential Satisfaction: Identifying the differences between suburban-ites and urbanites. Columbus: Ohio State University. &#8617; . | Novaco, R. W., Stokols, D., &amp; Milanesi, L. (1990). Objective and subjective dimensions of travel impedance as determinants of commuting stress. American Journal of Community Psychology, 18, 231–257. &#8617; &#8617;2 . | Kahneman, D., Krueger, A. B., Schkade, D., Schwarz, N., &amp; Stone, A. (2004). A survey method for characterizing daily life experience: The day reconstruction method (DRM). Science, 306, 1776–1780. &#8617; &#8617;2 . | Shimon L. Dolan &amp; Eric Gosselin, 2000. “Job satisfaction and life satisfaction: Analysis of a reciprocal model with social demographic moderators,” Economics Working Papers 484, Department of Economics and Business, Universitat Pompeu Fabra. &#8617; . | Ilies, R., Wilson, K. S., &amp; Wagner, D. T. (2009). The Spillover of Daily Job Satisfaction Onto Employees’ Family Lives: The Facilitating Role of Work-Family Integration. Academy of Management Journal, 52(1), 87-102. doi:10.5465/AMJ.2009.36461938 &#8617; . | Stokols, D., Novaco, R. W., Stokols, J., &amp; Campbell, J. (1978). Traffic Congestion, Type A Behavior, and Stress. Journal of Applied Psychology, 63(4), 467-480. doi:10.1037/0021- 9010.63.4.467 &#8617; &#8617;2 . | Novaco, R. W., &amp; Gonzales, O. I. (2009). Commuting and well-being. In Y. Amichai- Hamburger (Ed.), Technology and well-being (pp. 174–205). New York: Cambridge University Press. &#8617; . | Olsson, L. E., Garling, T., Ettema, D., Friman, M., &amp; Fujii, S. (2013). Happiness and Satisfaction with Work Commute. Soc Indic Res, 111, 255-263. doi:10.1007/s11205-012- 0003-2 &#8617; &#8617;2 . | Glass, D. C., Singer, J., &amp; Pennebaker, J. Behavioral and physiological effects of uncontrollable environmental events. In D. Stokols (Ed.), Perspectives on environment and behavior. New York: Plenum, 1977. &#8617; . | Sherrod, D. R. Crowding, perceived control, and behavioral aftereffects. Journal of Applied Social Psychology, 1974, 4, 171-186. &#8617; . | Kottenhoff, K., &amp; Freij, K. B. (2009). The role of public transport for feasibility and acceptability of congestion charging – The case of Stockholm. Transportation Research Part A, 43, 297-305. doi:10.1016/j.tra.2008.09.004 &#8617; . | Schlag, B., Teubel, U., 1997. Public acceptability of transport pricing. IATSS Research, 21(2). &#8617; . | Transport for London (TfL), 2005. Central London Congestion Charging. Impacts monitoring, Third Annual Report, April 2005 &#8617; . | Gross Domestic Product Preliminary Estimate: Q4 2014 (pp. 1-20, Rep.). (2015). Office for National Statistics. &#8617; . | Jaensirisak, S., Wardman, M., May, A.D., 2005. Explaining variations in public acceptability of road pricing schemes. Journal of Transport Economics and Policy 39, 127–154. &#8617; . |",
            "url": "https://antoniojurlina.github.io/portfolio/projects/stata/2019/04/30/congestion-and-satisfaction.html",
            "relUrl": "/projects/stata/2019/04/30/congestion-and-satisfaction.html",
            "date": " • Apr 30, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Home",
          "content": "antoniojurlina.github.io .",
          "url": "https://antoniojurlina.github.io/portfolio/return/",
          "relUrl": "/return/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://antoniojurlina.github.io/portfolio/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}